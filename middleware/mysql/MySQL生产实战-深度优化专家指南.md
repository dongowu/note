# MySQLç”Ÿäº§å®æˆ˜ - æ·±åº¦ä¼˜åŒ–ä¸“å®¶æŒ‡å—

## ç›®å½•
1. [æ…¢æŸ¥è¯¢ä¼˜åŒ–å®æˆ˜](#1-æ…¢æŸ¥è¯¢ä¼˜åŒ–å®æˆ˜)
2. [ä¸»ä»åŒæ­¥æ·±åº¦é…ç½®](#2-ä¸»ä»åŒæ­¥æ·±åº¦é…ç½®)
3. [åˆ†åº“åˆ†è¡¨ä¸­é—´ä»¶å®ç°](#3-åˆ†åº“åˆ†è¡¨ä¸­é—´ä»¶å®ç°)
4. [å¤‡ä»½æ¢å¤è‡ªåŠ¨åŒ–è¿ç»´](#4-å¤‡ä»½æ¢å¤è‡ªåŠ¨åŒ–è¿ç»´)

---

## 1. æ…¢æŸ¥è¯¢ä¼˜åŒ–å®æˆ˜

### 1.1 EXPLAINæ‰§è¡Œè®¡åˆ’æ·±åº¦åˆ†æ

#### æ ¸å¿ƒå­—æ®µè§£è¯»ä¸ä¼˜åŒ–ç­–ç•¥

**idå­—æ®µåˆ†æ**ï¼š
```sql
-- å¤æ‚æŸ¥è¯¢ç¤ºä¾‹
EXPLAIN SELECT u.name, o.total, p.title 
FROM users u 
JOIN orders o ON u.id = o.user_id 
JOIN order_items oi ON o.id = oi.order_id
JOIN products p ON oi.product_id = p.id
WHERE u.created_at > '2024-01-01' 
AND o.status = 'completed';
```

**æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–å¯¹ç…§è¡¨**ï¼š
| å­—æ®µ | ä¼˜åŒ–ç›®æ ‡ | é—®é¢˜è¯†åˆ« | è§£å†³æ–¹æ¡ˆ |
|------|----------|----------|----------|
| `type` | const > eq_ref > ref > range > index > ALL | ALL/indexæ‰«æ | æ·»åŠ åˆé€‚ç´¢å¼• |
| `key` | ä½¿ç”¨é¢„æœŸç´¢å¼• | NULLæˆ–é”™è¯¯ç´¢å¼• | å¼ºåˆ¶ç´¢å¼•æˆ–é‡å»ºç´¢å¼• |
| `rows` | æ‰«æè¡Œæ•°æœ€å°åŒ– | å¤§é‡è¡Œæ‰«æ | ä¼˜åŒ–WHEREæ¡ä»¶ |
| `Extra` | é¿å…Using filesort/temporary | æ–‡ä»¶æ’åº/ä¸´æ—¶è¡¨ | è°ƒæ•´ORDER BY/GROUP BY |

#### ç”Ÿäº§ç¯å¢ƒæ…¢æŸ¥è¯¢åˆ†æå·¥å…·

**Goè¯­è¨€å®ç°çš„EXPLAINåˆ†æå™¨**ï¼š
```go
package main

import (
    "database/sql"
    "encoding/json"
    "fmt"
    "log"
    "strings"
    "time"
    
    _ "github.com/go-sql-driver/mysql"
)

type ExplainResult struct {
    ID           int     `json:"id"`
    SelectType   string  `json:"select_type"`
    Table        string  `json:"table"`
    Type         string  `json:"type"`
    PossibleKeys *string `json:"possible_keys"`
    Key          *string `json:"key"`
    KeyLen       *string `json:"key_len"`
    Ref          *string `json:"ref"`
    Rows         int     `json:"rows"`
    Extra        *string `json:"extra"`
    Score        int     `json:"performance_score"`
}

type QueryAnalyzer struct {
    db *sql.DB
}

func NewQueryAnalyzer(dsn string) (*QueryAnalyzer, error) {
    db, err := sql.Open("mysql", dsn)
    if err != nil {
        return nil, err
    }
    return &QueryAnalyzer{db: db}, nil
}

// åˆ†ææŸ¥è¯¢æ€§èƒ½å¹¶ç»™å‡ºä¼˜åŒ–å»ºè®®
func (qa *QueryAnalyzer) AnalyzeQuery(query string) (*QueryAnalysis, error) {
    // 1. æ‰§è¡ŒEXPLAIN
    explainSQL := "EXPLAIN FORMAT=JSON " + query
    var jsonResult string
    err := qa.db.QueryRow(explainSQL).Scan(&jsonResult)
    if err != nil {
        return nil, fmt.Errorf("explain failed: %w", err)
    }
    
    // 2. è§£æJSONç»“æœ
    var explainData map[string]interface{}
    if err := json.Unmarshal([]byte(jsonResult), &explainData); err != nil {
        return nil, err
    }
    
    // 3. æ€§èƒ½è¯„åˆ†
    analysis := &QueryAnalysis{
        Query:        query,
        ExplainData:  explainData,
        Timestamp:    time.Now(),
    }
    
    analysis.Score = qa.calculatePerformanceScore(explainData)
    analysis.Suggestions = qa.generateOptimizationSuggestions(explainData)
    
    return analysis, nil
}

type QueryAnalysis struct {
    Query        string                 `json:"query"`
    ExplainData  map[string]interface{} `json:"explain_data"`
    Score        int                    `json:"score"`
    Suggestions  []string               `json:"suggestions"`
    Timestamp    time.Time              `json:"timestamp"`
}

// æ€§èƒ½è¯„åˆ†ç®—æ³•ï¼ˆ0-100åˆ†ï¼‰
func (qa *QueryAnalyzer) calculatePerformanceScore(explainData map[string]interface{}) int {
    score := 100
    
    queryBlock := explainData["query_block"].(map[string]interface{})
    
    // æ£€æŸ¥è¡¨æ‰«æç±»å‹
    if table, ok := queryBlock["table"]; ok {
        tableInfo := table.(map[string]interface{})
        accessType := tableInfo["access_type"].(string)
        
        switch accessType {
        case "ALL":
            score -= 50 // å…¨è¡¨æ‰«æä¸¥é‡æ‰£åˆ†
        case "index":
            score -= 30 // ç´¢å¼•æ‰«ææ‰£åˆ†
        case "range":
            score -= 10 // èŒƒå›´æ‰«æè½»å¾®æ‰£åˆ†
        case "ref", "eq_ref", "const":
            // ç†æƒ³çš„è®¿é—®ç±»å‹ï¼Œä¸æ‰£åˆ†
        }
        
        // æ£€æŸ¥æ‰«æè¡Œæ•°
        if rows, ok := tableInfo["rows_examined_per_scan"]; ok {
            rowCount := int(rows.(float64))
            if rowCount > 10000 {
                score -= 30
            } else if rowCount > 1000 {
                score -= 15
            }
        }
        
        // æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ–‡ä»¶æ’åº
        if usingFilesort := strings.Contains(fmt.Sprintf("%v", tableInfo), "filesort"); usingFilesort {
            score -= 20
        }
        
        // æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ä¸´æ—¶è¡¨
        if usingTempTable := strings.Contains(fmt.Sprintf("%v", tableInfo), "temporary"); usingTempTable {
            score -= 25
        }
    }
    
    if score < 0 {
        score = 0
    }
    
    return score
}

// ç”Ÿæˆä¼˜åŒ–å»ºè®®
func (qa *QueryAnalyzer) generateOptimizationSuggestions(explainData map[string]interface{}) []string {
    var suggestions []string
    
    queryBlock := explainData["query_block"].(map[string]interface{})
    
    if table, ok := queryBlock["table"]; ok {
        tableInfo := table.(map[string]interface{})
        accessType := tableInfo["access_type"].(string)
        
        switch accessType {
        case "ALL":
            suggestions = append(suggestions, "âŒ å‘ç°å…¨è¡¨æ‰«æï¼Œå»ºè®®æ·»åŠ WHEREæ¡ä»¶å¯¹åº”çš„ç´¢å¼•")
            suggestions = append(suggestions, "ğŸ’¡ åˆ†ææŸ¥è¯¢æ¡ä»¶ï¼Œåˆ›å»ºå¤åˆç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢")
        case "index":
            suggestions = append(suggestions, "âš ï¸ ä½¿ç”¨äº†ç´¢å¼•æ‰«æï¼Œè€ƒè™‘ä¼˜åŒ–WHEREæ¡ä»¶")
        }
        
        // æ£€æŸ¥JOINä¼˜åŒ–
        if joinTables, ok := queryBlock["nested_loop"]; ok {
            suggestions = append(suggestions, "ğŸ”— æ£€æµ‹åˆ°JOINæ“ä½œï¼Œç¡®ä¿å…³è”å­—æ®µéƒ½æœ‰ç´¢å¼•")
            suggestions = append(suggestions, "ğŸ“Š è€ƒè™‘è°ƒæ•´JOINé¡ºåºï¼Œå°è¡¨é©±åŠ¨å¤§è¡¨")
            
            // åˆ†æJOINç±»å‹
            joinInfo := joinTables.([]interface{})
            for _, join := range joinInfo {
                if joinMap, ok := join.(map[string]interface{}); ok {
                    if table, ok := joinMap["table"]; ok {
                        tableData := table.(map[string]interface{})
                        if accessType, ok := tableData["access_type"]; ok && accessType == "ALL" {
                            tableName := tableData["table_name"].(string)
                            suggestions = append(suggestions, 
                                fmt.Sprintf("ğŸš¨ è¡¨ %s åœ¨JOINä¸­ä½¿ç”¨å…¨è¡¨æ‰«æï¼Œæ€¥éœ€ä¼˜åŒ–", tableName))
                        }
                    }
                }
            }
        }
        
        // æ£€æŸ¥æ’åºä¼˜åŒ–
        if strings.Contains(fmt.Sprintf("%v", tableInfo), "filesort") {
            suggestions = append(suggestions, "ğŸ“ˆ ä½¿ç”¨äº†æ–‡ä»¶æ’åºï¼Œè€ƒè™‘åˆ›å»ºORDER BYå­—æ®µçš„ç´¢å¼•")
            suggestions = append(suggestions, "ğŸ¯ å¦‚æœæ˜¯å¤åˆæ’åºï¼Œåˆ›å»ºå¤šå­—æ®µå¤åˆç´¢å¼•")
        }
        
        // æ£€æŸ¥ä¸´æ—¶è¡¨
        if strings.Contains(fmt.Sprintf("%v", tableInfo), "temporary") {
            suggestions = append(suggestions, "ğŸ’¾ ä½¿ç”¨äº†ä¸´æ—¶è¡¨ï¼Œæ£€æŸ¥GROUP BYå’ŒDISTINCTæ“ä½œ")
            suggestions = append(suggestions, "âš¡ è€ƒè™‘é‡å†™æŸ¥è¯¢é€»è¾‘é¿å…ä¸´æ—¶è¡¨")
        }
    }
    
    return suggestions
}

// æ…¢æŸ¥è¯¢ç›‘æ§å’Œè‡ªåŠ¨åˆ†æ
func (qa *QueryAnalyzer) MonitorSlowQueries() {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for range ticker.C {
        slowQueries, err := qa.getSlowQueries()
        if err != nil {
            log.Printf("è·å–æ…¢æŸ¥è¯¢å¤±è´¥: %v", err)
            continue
        }
        
        for _, query := range slowQueries {
            if analysis, err := qa.AnalyzeQuery(query.SQL); err == nil {
                if analysis.Score < 60 {
                    qa.alertSlowQuery(query, analysis)
                }
            }
        }
    }
}

type SlowQuery struct {
    SQL           string        `json:"sql"`
    QueryTime     time.Duration `json:"query_time"`
    LockTime      time.Duration `json:"lock_time"`
    RowsSent      int           `json:"rows_sent"`
    RowsExamined  int           `json:"rows_examined"`
    Timestamp     time.Time     `json:"timestamp"`
}

func (qa *QueryAnalyzer) getSlowQueries() ([]SlowQuery, error) {
    // ä»performance_schemaè·å–æ…¢æŸ¥è¯¢
    query := `
        SELECT sql_text, timer_wait/1000000000 as query_time_ms,
               lock_time/1000000000 as lock_time_ms,
               rows_sent, rows_examined
        FROM performance_schema.events_statements_history_long 
        WHERE timer_wait > 1000000000  -- è¶…è¿‡1ç§’çš„æŸ¥è¯¢
        ORDER BY timer_wait DESC 
        LIMIT 10
    `
    
    rows, err := qa.db.Query(query)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var slowQueries []SlowQuery
    for rows.Next() {
        var sq SlowQuery
        var queryTimeMs, lockTimeMs float64
        
        err := rows.Scan(&sq.SQL, &queryTimeMs, &lockTimeMs, 
                        &sq.RowsSent, &sq.RowsExamined)
        if err != nil {
            continue
        }
        
        sq.QueryTime = time.Duration(queryTimeMs) * time.Millisecond
        sq.LockTime = time.Duration(lockTimeMs) * time.Millisecond
        sq.Timestamp = time.Now()
        
        slowQueries = append(slowQueries, sq)
    }
    
    return slowQueries, nil
}

func (qa *QueryAnalyzer) alertSlowQuery(query SlowQuery, analysis *QueryAnalysis) {
    alert := map[string]interface{}{
        "type":        "slow_query_alert",
        "query":       query.SQL,
        "query_time":  query.QueryTime.String(),
        "score":       analysis.Score,
        "suggestions": analysis.Suggestions,
        "timestamp":   time.Now(),
    }
    
    // å‘é€åˆ°ç›‘æ§ç³»ç»Ÿï¼ˆå¦‚Prometheus AlertManagerï¼‰
    alertJSON, _ := json.MarshalIndent(alert, "", "  ")
    log.Printf("ğŸš¨ æ…¢æŸ¥è¯¢å‘Šè­¦:\n%s", string(alertJSON))
    
    // è¿™é‡Œå¯ä»¥é›†æˆé’‰é’‰ã€å¾®ä¿¡ç­‰å‘Šè­¦é€šé“
    // qa.sendDingTalkAlert(alert)
}
```

### 1.2 ç´¢å¼•è®¾è®¡æœ€ä½³å®è·µ

#### å¤åˆç´¢å¼•è®¾è®¡åŸåˆ™

**æœ€å·¦å‰ç¼€åŒ¹é…åŸåˆ™å®æˆ˜**ï¼š
```sql
-- ç”¨æˆ·è®¢å•æŸ¥è¯¢åœºæ™¯
CREATE TABLE orders (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id BIGINT NOT NULL,
    status TINYINT NOT NULL,
    created_at DATETIME NOT NULL,
    updated_at DATETIME NOT NULL,
    total_amount DECIMAL(10,2) NOT NULL,
    
    -- å¤åˆç´¢å¼•è®¾è®¡ï¼ˆæŒ‰æŸ¥è¯¢é¢‘ç‡å’Œé€‰æ‹©æ€§æ’åºï¼‰
    INDEX idx_user_status_time (user_id, status, created_at),
    INDEX idx_status_time (status, created_at),
    INDEX idx_created_at (created_at)
) ENGINE=InnoDB;

-- ç´¢å¼•ä½¿ç”¨æ•ˆæœå¯¹æ¯”
-- âœ… èƒ½ä½¿ç”¨idx_user_status_timeç´¢å¼•
SELECT * FROM orders WHERE user_id = 123 AND status = 1;
SELECT * FROM orders WHERE user_id = 123 AND status = 1 AND created_at > '2024-01-01';

-- âŒ ä¸èƒ½ä½¿ç”¨idx_user_status_timeç´¢å¼•ï¼ˆè·³è¿‡äº†user_idï¼‰
SELECT * FROM orders WHERE status = 1 AND created_at > '2024-01-01';
-- è¿™ç§æƒ…å†µä¼šä½¿ç”¨idx_status_timeç´¢å¼•
```

#### ç´¢å¼•ä¼˜åŒ–å·¥å…·å®ç°

**Goè¯­è¨€ç´¢å¼•åˆ†æå™¨**ï¼š
```go
type IndexAnalyzer struct {
    db *sql.DB
}

type IndexUsage struct {
    TableName    string  `json:"table_name"`
    IndexName    string  `json:"index_name"`
    Cardinality  int64   `json:"cardinality"`
    UsageCount   int64   `json:"usage_count"`
    LastUsed     *time.Time `json:"last_used"`
    Selectivity  float64 `json:"selectivity"`
    Redundant    bool    `json:"redundant"`
}

// åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ
func (ia *IndexAnalyzer) AnalyzeIndexUsage(tableName string) ([]IndexUsage, error) {
    query := `
        SELECT 
            s.table_name,
            s.index_name,
            s.cardinality,
            IFNULL(u.count_read, 0) as usage_count,
            u.last_read
        FROM information_schema.statistics s
        LEFT JOIN performance_schema.table_io_waits_summary_by_index_usage u 
            ON s.table_schema = u.object_schema 
            AND s.table_name = u.object_name 
            AND s.index_name = u.index_name
        WHERE s.table_schema = DATABASE()
        AND s.table_name = ?
        AND s.seq_in_index = 1  -- åªçœ‹å¤åˆç´¢å¼•çš„ç¬¬ä¸€åˆ—
        ORDER BY s.table_name, s.index_name
    `
    
    rows, err := ia.db.Query(query, tableName)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var indexes []IndexUsage
    for rows.Next() {
        var idx IndexUsage
        var lastUsed sql.NullTime
        
        err := rows.Scan(&idx.TableName, &idx.IndexName, &idx.Cardinality,
                        &idx.UsageCount, &lastUsed)
        if err != nil {
            continue
        }
        
        if lastUsed.Valid {
            idx.LastUsed = &lastUsed.Time
        }
        
        // è®¡ç®—é€‰æ‹©æ€§
        idx.Selectivity = ia.calculateSelectivity(tableName, idx.IndexName)
        
        // æ£€æŸ¥æ˜¯å¦å†—ä½™
        idx.Redundant = ia.isRedundantIndex(tableName, idx.IndexName)
        
        indexes = append(indexes, idx)
    }
    
    return indexes, nil
}

// è®¡ç®—ç´¢å¼•é€‰æ‹©æ€§
func (ia *IndexAnalyzer) calculateSelectivity(tableName, indexName string) float64 {
    // è·å–è¡¨æ€»è¡Œæ•°
    var totalRows int64
    ia.db.QueryRow("SELECT COUNT(*) FROM "+tableName).Scan(&totalRows)
    
    if totalRows == 0 {
        return 0
    }
    
    // è·å–ç´¢å¼•åˆ—ä¿¡æ¯
    columns := ia.getIndexColumns(tableName, indexName)
    if len(columns) == 0 {
        return 0
    }
    
    // è®¡ç®—ç¬¬ä¸€åˆ—çš„å”¯ä¸€å€¼æ•°é‡
    var distinctCount int64
    query := fmt.Sprintf("SELECT COUNT(DISTINCT %s) FROM %s", columns[0], tableName)
    ia.db.QueryRow(query).Scan(&distinctCount)
    
    return float64(distinctCount) / float64(totalRows)
}

// è·å–ç´¢å¼•åŒ…å«çš„åˆ—
func (ia *IndexAnalyzer) getIndexColumns(tableName, indexName string) []string {
    query := `
        SELECT column_name 
        FROM information_schema.statistics 
        WHERE table_schema = DATABASE() 
        AND table_name = ? 
        AND index_name = ?
        ORDER BY seq_in_index
    `
    
    rows, err := ia.db.Query(query, tableName, indexName)
    if err != nil {
        return nil
    }
    defer rows.Close()
    
    var columns []string
    for rows.Next() {
        var column string
        if rows.Scan(&column) == nil {
            columns = append(columns, column)
        }
    }
    
    return columns
}

// æ£€æŸ¥å†—ä½™ç´¢å¼•
func (ia *IndexAnalyzer) isRedundantIndex(tableName, indexName string) bool {
    currentColumns := ia.getIndexColumns(tableName, indexName)
    if len(currentColumns) == 0 {
        return false
    }
    
    // è·å–è¡¨ä¸Šæ‰€æœ‰ç´¢å¼•
    allIndexes := ia.getAllIndexes(tableName)
    
    for _, otherIndex := range allIndexes {
        if otherIndex == indexName {
            continue
        }
        
        otherColumns := ia.getIndexColumns(tableName, otherIndex)
        
        // æ£€æŸ¥å½“å‰ç´¢å¼•æ˜¯å¦æ˜¯å…¶ä»–ç´¢å¼•çš„å‰ç¼€
        if ia.isPrefix(currentColumns, otherColumns) {
            return true
        }
    }
    
    return false
}

func (ia *IndexAnalyzer) isPrefix(prefix, full []string) bool {
    if len(prefix) >= len(full) {
        return false
    }
    
    for i, col := range prefix {
        if i >= len(full) || col != full[i] {
            return false
        }
    }
    
    return true
}

func (ia *IndexAnalyzer) getAllIndexes(tableName string) []string {
    query := `
        SELECT DISTINCT index_name 
        FROM information_schema.statistics 
        WHERE table_schema = DATABASE() 
        AND table_name = ?
        AND index_name != 'PRIMARY'
    `
    
    rows, err := ia.db.Query(query, tableName)
    if err != nil {
        return nil
    }
    defer rows.Close()
    
    var indexes []string
    for rows.Next() {
        var indexName string
        if rows.Scan(&indexName) == nil {
            indexes = append(indexes, indexName)
        }
    }
    
    return indexes
}

// ç”Ÿæˆç´¢å¼•ä¼˜åŒ–å»ºè®®
func (ia *IndexAnalyzer) GenerateIndexOptimizationReport(tableName string) (*IndexOptimizationReport, error) {
    indexes, err := ia.AnalyzeIndexUsage(tableName)
    if err != nil {
        return nil, err
    }
    
    report := &IndexOptimizationReport{
        TableName: tableName,
        Timestamp: time.Now(),
    }
    
    for _, idx := range indexes {
        if idx.UsageCount == 0 && idx.LastUsed == nil {
            report.UnusedIndexes = append(report.UnusedIndexes, idx.IndexName)
        }
        
        if idx.Redundant {
            report.RedundantIndexes = append(report.RedundantIndexes, idx.IndexName)
        }
        
        if idx.Selectivity < 0.01 { // é€‰æ‹©æ€§ä½äº1%
            report.LowSelectivityIndexes = append(report.LowSelectivityIndexes, 
                fmt.Sprintf("%s (é€‰æ‹©æ€§: %.2f%%)", idx.IndexName, idx.Selectivity*100))
        }
    }
    
    // ç”Ÿæˆä¼˜åŒ–å»ºè®®
    report.Recommendations = ia.generateRecommendations(report)
    
    return report, nil
}

type IndexOptimizationReport struct {
    TableName              string    `json:"table_name"`
    UnusedIndexes          []string  `json:"unused_indexes"`
    RedundantIndexes       []string  `json:"redundant_indexes"`
    LowSelectivityIndexes  []string  `json:"low_selectivity_indexes"`
    Recommendations        []string  `json:"recommendations"`
    Timestamp              time.Time `json:"timestamp"`
}

func (ia *IndexAnalyzer) generateRecommendations(report *IndexOptimizationReport) []string {
    var recommendations []string
    
    if len(report.UnusedIndexes) > 0 {
        recommendations = append(recommendations, 
            fmt.Sprintf("ğŸ—‘ï¸ å‘ç° %d ä¸ªæœªä½¿ç”¨çš„ç´¢å¼•ï¼Œå»ºè®®åˆ é™¤ä»¥èŠ‚çœå­˜å‚¨ç©ºé—´å’Œæå‡å†™å…¥æ€§èƒ½", 
                       len(report.UnusedIndexes)))
        for _, idx := range report.UnusedIndexes {
            recommendations = append(recommendations, 
                fmt.Sprintf("   DROP INDEX %s ON %s;", idx, report.TableName))
        }
    }
    
    if len(report.RedundantIndexes) > 0 {
        recommendations = append(recommendations, 
            fmt.Sprintf("ğŸ”„ å‘ç° %d ä¸ªå†—ä½™ç´¢å¼•ï¼Œå»ºè®®åˆ é™¤é‡å¤çš„ç´¢å¼•", 
                       len(report.RedundantIndexes)))
    }
    
    if len(report.LowSelectivityIndexes) > 0 {
        recommendations = append(recommendations, 
            "ğŸ“Š å‘ç°ä½é€‰æ‹©æ€§ç´¢å¼•ï¼Œè€ƒè™‘é‡æ–°è®¾è®¡æˆ–åˆ é™¤")
    }
    
    return recommendations
}
```

---

## 2. ä¸»ä»åŒæ­¥æ·±åº¦é…ç½®

### 2.1 ç”Ÿäº§çº§ä¸»ä»å¤åˆ¶é…ç½®

#### ä¸»åº“é…ç½®ï¼ˆmy.cnfï¼‰
```ini
[mysqld]
# åŸºç¡€é…ç½®
server-id = 1
log-bin = mysql-bin
binlog-format = ROW
binlog-do-db = production_db

# æ€§èƒ½ä¼˜åŒ–
sync_binlog = 1
innodb_flush_log_at_trx_commit = 1

# ä¸»ä»åŒæ­¥ä¼˜åŒ–
max_binlog_size = 1G
binlog_cache_size = 32M
binlog_stmt_cache_size = 32M

# åŠåŒæ­¥å¤åˆ¶ï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰
plugin-load = "rpl_semi_sync_master=semisync_master.so"
rpl_semi_sync_master_enabled = 1
rpl_semi_sync_master_timeout = 1000  # 1ç§’è¶…æ—¶
rpl_semi_sync_master_wait_for_slave_count = 1

# GTIDæ¨¡å¼ï¼ˆæ¨èï¼‰
gtid_mode = ON
enforce_gtid_consistency = ON
log_slave_updates = ON
```

#### ä»åº“é…ç½®ï¼ˆmy.cnfï¼‰
```ini
[mysqld]
# åŸºç¡€é…ç½®
server-id = 2
log-bin = mysql-bin
binlog-format = ROW
read_only = 1
super_read_only = 1

# ä¸­ç»§æ—¥å¿—é…ç½®
relay-log = relay-bin
relay_log_purge = 1
relay_log_recovery = 1

# åŠåŒæ­¥å¤åˆ¶
plugin-load = "rpl_semi_sync_slave=semisync_slave.so"
rpl_semi_sync_slave_enabled = 1

# GTIDæ¨¡å¼
gtid_mode = ON
enforce_gtid_consistency = ON
log_slave_updates = ON

# å¹¶è¡Œå¤åˆ¶ä¼˜åŒ–
slave_parallel_type = LOGICAL_CLOCK
slave_parallel_workers = 4
slave_preserve_commit_order = 1

# å¤åˆ¶è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
replicate-do-db = production_db
```

### 2.2 å»¶è¿Ÿç›‘æ§ä¸è‡ªåŠ¨åŒ–å¤„ç†

#### Goè¯­è¨€å®ç°çš„ä¸»ä»å»¶è¿Ÿç›‘æ§ç³»ç»Ÿ

```go
package main

import (
    "context"
    "database/sql"
    "encoding/json"
    "fmt"
    "log"
    "sync"
    "time"
    
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
    _ "github.com/go-sql-driver/mysql"
)

// ä¸»ä»å»¶è¿Ÿç›‘æ§å™¨
type ReplicationMonitor struct {
    masterDB    *sql.DB
    slaveDBs    []*sql.DB
    config      *MonitorConfig
    metrics     *ReplicationMetrics
    alertChan   chan *ReplicationAlert
    mu          sync.RWMutex
}

type MonitorConfig struct {
    CheckInterval     time.Duration `json:"check_interval"`
    MaxLagThreshold   time.Duration `json:"max_lag_threshold"`
    CriticalThreshold time.Duration `json:"critical_threshold"`
    AlertWebhook      string        `json:"alert_webhook"`
    AutoFailover      bool          `json:"auto_failover"`
}

type ReplicationMetrics struct {
    LagSeconds     prometheus.Gauge
    SlaveIORunning prometheus.Gauge
    SlaveSQLRunning prometheus.Gauge
    MasterLogPos   prometheus.Gauge
    SlaveLogPos    prometheus.Gauge
}

type ReplicationStatus struct {
    SlaveIOState       string        `json:"slave_io_state"`
    MasterLogFile      string        `json:"master_log_file"`
    ReadMasterLogPos   int64         `json:"read_master_log_pos"`
    RelayLogFile       string        `json:"relay_log_file"`
    RelayLogPos        int64         `json:"relay_log_pos"`
    RelayMasterLogFile string        `json:"relay_master_log_file"`
    SlaveIORunning     string        `json:"slave_io_running"`
    SlaveSQLRunning    string        `json:"slave_sql_running"`
    SecondsBehindMaster *int64       `json:"seconds_behind_master"`
    LastIOError        string        `json:"last_io_error"`
    LastSQLError       string        `json:"last_sql_error"`
    GTIDSet            string        `json:"executed_gtid_set"`
    AutoPosition       bool          `json:"auto_position"`
    Timestamp          time.Time     `json:"timestamp"`
}

type ReplicationAlert struct {
    Type        string            `json:"type"`
    Severity    string            `json:"severity"`
    Message     string            `json:"message"`
    SlaveHost   string            `json:"slave_host"`
    LagSeconds  *int64            `json:"lag_seconds,omitempty"`
    Details     map[string]interface{} `json:"details"`
    Timestamp   time.Time         `json:"timestamp"`
}

func NewReplicationMonitor(masterDSN string, slaveDSNs []string, config *MonitorConfig) (*ReplicationMonitor, error) {
    // è¿æ¥ä¸»åº“
    masterDB, err := sql.Open("mysql", masterDSN)
    if err != nil {
        return nil, fmt.Errorf("è¿æ¥ä¸»åº“å¤±è´¥: %w", err)
    }
    
    // è¿æ¥ä»åº“
    var slaveDBs []*sql.DB
    for _, dsn := range slaveDSNs {
        slaveDB, err := sql.Open("mysql", dsn)
        if err != nil {
            return nil, fmt.Errorf("è¿æ¥ä»åº“å¤±è´¥: %w", err)
        }
        slaveDBs = append(slaveDBs, slaveDB)
    }
    
    // åˆå§‹åŒ–PrometheusæŒ‡æ ‡
    metrics := &ReplicationMetrics{
        LagSeconds: promauto.NewGauge(prometheus.GaugeOpts{
            Name: "mysql_replication_lag_seconds",
            Help: "MySQLä¸»ä»å¤åˆ¶å»¶è¿Ÿç§’æ•°",
        }),
        SlaveIORunning: promauto.NewGauge(prometheus.GaugeOpts{
            Name: "mysql_slave_io_running",
            Help: "MySQLä»åº“IOçº¿ç¨‹è¿è¡ŒçŠ¶æ€",
        }),
        SlaveSQLRunning: promauto.NewGauge(prometheus.GaugeOpts{
            Name: "mysql_slave_sql_running",
            Help: "MySQLä»åº“SQLçº¿ç¨‹è¿è¡ŒçŠ¶æ€",
        }),
    }
    
    return &ReplicationMonitor{
        masterDB:  masterDB,
        slaveDBs:  slaveDBs,
        config:    config,
        metrics:   metrics,
        alertChan: make(chan *ReplicationAlert, 100),
    }, nil
}

// å¯åŠ¨ç›‘æ§
func (rm *ReplicationMonitor) Start(ctx context.Context) {
    // å¯åŠ¨å‘Šè­¦å¤„ç†åç¨‹
    go rm.handleAlerts(ctx)
    
    ticker := time.NewTicker(rm.config.CheckInterval)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            rm.checkReplicationStatus()
        }
    }
}

// æ£€æŸ¥å¤åˆ¶çŠ¶æ€
func (rm *ReplicationMonitor) checkReplicationStatus() {
    for i, slaveDB := range rm.slaveDBs {
        status, err := rm.getSlaveStatus(slaveDB)
        if err != nil {
            log.Printf("è·å–ä»åº“%dçŠ¶æ€å¤±è´¥: %v", i, err)
            continue
        }
        
        rm.updateMetrics(status)
        rm.checkForAlerts(status, fmt.Sprintf("slave-%d", i))
    }
}

// è·å–ä»åº“çŠ¶æ€
func (rm *ReplicationMonitor) getSlaveStatus(db *sql.DB) (*ReplicationStatus, error) {
    query := "SHOW SLAVE STATUS"
    rows, err := db.Query(query)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    if !rows.Next() {
        return nil, fmt.Errorf("æœªæ‰¾åˆ°ä»åº“çŠ¶æ€ä¿¡æ¯")
    }
    
    // è·å–åˆ—å
    columns, err := rows.Columns()
    if err != nil {
        return nil, err
    }
    
    // åˆ›å»ºæ‰«æç›®æ ‡
    values := make([]interface{}, len(columns))
    valuePtrs := make([]interface{}, len(columns))
    for i := range values {
        valuePtrs[i] = &values[i]
    }
    
    // æ‰«ææ•°æ®
    if err := rows.Scan(valuePtrs...); err != nil {
        return nil, err
    }
    
    // è§£æç»“æœ
    status := &ReplicationStatus{
        Timestamp: time.Now(),
    }
    
    for i, col := range columns {
        val := values[i]
        if val == nil {
            continue
        }
        
        switch col {
        case "Slave_IO_State":
            status.SlaveIOState = string(val.([]byte))
        case "Master_Log_File":
            status.MasterLogFile = string(val.([]byte))
        case "Read_Master_Log_Pos":
            status.ReadMasterLogPos = val.(int64)
        case "Slave_IO_Running":
            status.SlaveIORunning = string(val.([]byte))
        case "Slave_SQL_Running":
            status.SlaveSQLRunning = string(val.([]byte))
        case "Seconds_Behind_Master":
            if val != nil {
                seconds := val.(int64)
                status.SecondsBehindMaster = &seconds
            }
        case "Last_IO_Error":
            status.LastIOError = string(val.([]byte))
        case "Last_SQL_Error":
            status.LastSQLError = string(val.([]byte))
        case "Executed_Gtid_Set":
            status.GTIDSet = string(val.([]byte))
        case "Auto_Position":
            status.AutoPosition = val.(int64) == 1
        }
    }
    
    return status, nil
}

// æ›´æ–°PrometheusæŒ‡æ ‡
func (rm *ReplicationMonitor) updateMetrics(status *ReplicationStatus) {
    if status.SecondsBehindMaster != nil {
        rm.metrics.LagSeconds.Set(float64(*status.SecondsBehindMaster))
    }
    
    if status.SlaveIORunning == "Yes" {
        rm.metrics.SlaveIORunning.Set(1)
    } else {
        rm.metrics.SlaveIORunning.Set(0)
    }
    
    if status.SlaveSQLRunning == "Yes" {
        rm.metrics.SlaveSQLRunning.Set(1)
    } else {
        rm.metrics.SlaveSQLRunning.Set(0)
    }
}

// æ£€æŸ¥å‘Šè­¦æ¡ä»¶
func (rm *ReplicationMonitor) checkForAlerts(status *ReplicationStatus, slaveHost string) {
    // æ£€æŸ¥IOçº¿ç¨‹çŠ¶æ€
    if status.SlaveIORunning != "Yes" {
        alert := &ReplicationAlert{
            Type:      "io_thread_stopped",
            Severity:  "critical",
            Message:   "ä»åº“IOçº¿ç¨‹å·²åœæ­¢",
            SlaveHost: slaveHost,
            Details: map[string]interface{}{
                "last_io_error": status.LastIOError,
                "io_state":      status.SlaveIOState,
            },
            Timestamp: time.Now(),
        }
        rm.sendAlert(alert)
    }
    
    // æ£€æŸ¥SQLçº¿ç¨‹çŠ¶æ€
    if status.SlaveSQLRunning != "Yes" {
        alert := &ReplicationAlert{
            Type:      "sql_thread_stopped",
            Severity:  "critical",
            Message:   "ä»åº“SQLçº¿ç¨‹å·²åœæ­¢",
            SlaveHost: slaveHost,
            Details: map[string]interface{}{
                "last_sql_error": status.LastSQLError,
            },
            Timestamp: time.Now(),
        }
        rm.sendAlert(alert)
    }
    
    // æ£€æŸ¥å¤åˆ¶å»¶è¿Ÿ
    if status.SecondsBehindMaster != nil {
        lagSeconds := *status.SecondsBehindMaster
        
        if time.Duration(lagSeconds)*time.Second > rm.config.CriticalThreshold {
            alert := &ReplicationAlert{
                Type:       "high_replication_lag",
                Severity:   "critical",
                Message:    fmt.Sprintf("ä¸»ä»å¤åˆ¶å»¶è¿Ÿè¿‡é«˜: %dç§’", lagSeconds),
                SlaveHost:  slaveHost,
                LagSeconds: &lagSeconds,
                Timestamp:  time.Now(),
            }
            rm.sendAlert(alert)
        } else if time.Duration(lagSeconds)*time.Second > rm.config.MaxLagThreshold {
            alert := &ReplicationAlert{
                Type:       "moderate_replication_lag",
                Severity:   "warning",
                Message:    fmt.Sprintf("ä¸»ä»å¤åˆ¶å»¶è¿Ÿè¾ƒé«˜: %dç§’", lagSeconds),
                SlaveHost:  slaveHost,
                LagSeconds: &lagSeconds,
                Timestamp:  time.Now(),
            }
            rm.sendAlert(alert)
        }
    }
}

// å‘é€å‘Šè­¦
func (rm *ReplicationMonitor) sendAlert(alert *ReplicationAlert) {
    select {
    case rm.alertChan <- alert:
    default:
        log.Printf("å‘Šè­¦é€šé“å·²æ»¡ï¼Œä¸¢å¼ƒå‘Šè­¦: %+v", alert)
    }
}

// å¤„ç†å‘Šè­¦
func (rm *ReplicationMonitor) handleAlerts(ctx context.Context) {
    for {
        select {
        case <-ctx.Done():
            return
        case alert := <-rm.alertChan:
            rm.processAlert(alert)
        }
    }
}

func (rm *ReplicationMonitor) processAlert(alert *ReplicationAlert) {
    // è®°å½•æ—¥å¿—
    alertJSON, _ := json.MarshalIndent(alert, "", "  ")
    log.Printf("ğŸš¨ ä¸»ä»å¤åˆ¶å‘Šè­¦:\n%s", string(alertJSON))
    
    // å‘é€åˆ°å¤–éƒ¨å‘Šè­¦ç³»ç»Ÿ
    if rm.config.AlertWebhook != "" {
        rm.sendWebhookAlert(alert)
    }
    
    // è‡ªåŠ¨æ•…éšœå¤„ç†
    if rm.config.AutoFailover && alert.Severity == "critical" {
        rm.handleCriticalAlert(alert)
    }
}

func (rm *ReplicationMonitor) sendWebhookAlert(alert *ReplicationAlert) {
    // å®ç°Webhookå‘Šè­¦å‘é€é€»è¾‘
    // å¯ä»¥é›†æˆé’‰é’‰ã€å¾®ä¿¡ã€Slackç­‰
}

func (rm *ReplicationMonitor) handleCriticalAlert(alert *ReplicationAlert) {
    switch alert.Type {
    case "io_thread_stopped":
        rm.restartIOThread(alert.SlaveHost)
    case "sql_thread_stopped":
        rm.restartSQLThread(alert.SlaveHost)
    case "high_replication_lag":
        if rm.config.AutoFailover {
            rm.considerFailover(alert)
        }
    }
}

func (rm *ReplicationMonitor) restartIOThread(slaveHost string) {
    // å®ç°IOçº¿ç¨‹é‡å¯é€»è¾‘
    log.Printf("å°è¯•é‡å¯ä»åº“ %s çš„IOçº¿ç¨‹", slaveHost)
}

func (rm *ReplicationMonitor) restartSQLThread(slaveHost string) {
    // å®ç°SQLçº¿ç¨‹é‡å¯é€»è¾‘
    log.Printf("å°è¯•é‡å¯ä»åº“ %s çš„SQLçº¿ç¨‹", slaveHost)
}

func (rm *ReplicationMonitor) considerFailover(alert *ReplicationAlert) {
    // å®ç°æ•…éšœè½¬ç§»å†³ç­–é€»è¾‘
    log.Printf("è€ƒè™‘å¯¹ä»åº“ %s æ‰§è¡Œæ•…éšœè½¬ç§»", alert.SlaveHost)
}
```

### 2.3 æ•°æ®ä¸€è‡´æ€§ä¿è¯æœºåˆ¶

#### åŠåŒæ­¥å¤åˆ¶é…ç½®è„šæœ¬

```bash
#!/bin/bash
# åŠåŒæ­¥å¤åˆ¶é…ç½®è„šæœ¬

set -e

# é…ç½®å‚æ•°
MASTER_HOST="192.168.1.10"
SLAVE_HOSTS=("192.168.1.11" "192.168.1.12")
MYSQL_USER="root"
MYSQL_PASS="your_password"
REPL_USER="repl_user"
REPL_PASS="repl_password"

echo "ğŸš€ å¼€å§‹é…ç½®MySQLåŠåŒæ­¥å¤åˆ¶..."

# 1. åœ¨ä¸»åº“ä¸Šå¯ç”¨åŠåŒæ­¥å¤åˆ¶
echo "ğŸ“ é…ç½®ä¸»åº“åŠåŒæ­¥å¤åˆ¶..."
mysql -h$MASTER_HOST -u$MYSQL_USER -p$MYSQL_PASS <<EOF
-- å®‰è£…åŠåŒæ­¥å¤åˆ¶æ’ä»¶
INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so';

-- å¯ç”¨åŠåŒæ­¥å¤åˆ¶
SET GLOBAL rpl_semi_sync_master_enabled = 1;
SET GLOBAL rpl_semi_sync_master_timeout = 1000;
SET GLOBAL rpl_semi_sync_master_wait_for_slave_count = 1;

-- åˆ›å»ºå¤åˆ¶ç”¨æˆ·
CREATE USER IF NOT EXISTS '$REPL_USER'@'%' IDENTIFIED BY '$REPL_PASS';
GRANT REPLICATION SLAVE ON *.* TO '$REPL_USER'@'%';
FLUSH PRIVILEGES;

-- æŸ¥çœ‹ä¸»åº“çŠ¶æ€
SHOW MASTER STATUS;
EOF

# 2. é…ç½®ä»åº“
for SLAVE_HOST in "${SLAVE_HOSTS[@]}"; do
    echo "ğŸ“ é…ç½®ä»åº“ $SLAVE_HOST..."
    
    mysql -h$SLAVE_HOST -u$MYSQL_USER -p$MYSQL_PASS <<EOF
-- å®‰è£…åŠåŒæ­¥å¤åˆ¶æ’ä»¶
INSTALL PLUGIN rpl_semi_sync_slave SONAME 'semisync_slave.so';

-- å¯ç”¨åŠåŒæ­¥å¤åˆ¶
SET GLOBAL rpl_semi_sync_slave_enabled = 1;

-- åœæ­¢å¤åˆ¶
STOP SLAVE;

-- é…ç½®ä¸»åº“è¿æ¥
CHANGE MASTER TO
    MASTER_HOST='$MASTER_HOST',
    MASTER_USER='$REPL_USER',
    MASTER_PASSWORD='$REPL_PASS',
    MASTER_AUTO_POSITION=1;

-- å¯åŠ¨å¤åˆ¶
START SLAVE;

-- æŸ¥çœ‹ä»åº“çŠ¶æ€
SHOW SLAVE STATUS\G
EOF

done

# 3. éªŒè¯åŠåŒæ­¥å¤åˆ¶çŠ¶æ€
echo "ğŸ” éªŒè¯åŠåŒæ­¥å¤åˆ¶çŠ¶æ€..."
mysql -h$MASTER_HOST -u$MYSQL_USER -p$MYSQL_PASS <<EOF
SHOW STATUS LIKE 'Rpl_semi_sync_master%';
EOF

for SLAVE_HOST in "${SLAVE_HOSTS[@]}"; do
    echo "æ£€æŸ¥ä»åº“ $SLAVE_HOST çŠ¶æ€:"
    mysql -h$SLAVE_HOST -u$MYSQL_USER -p$MYSQL_PASS <<EOF
SHOW STATUS LIKE 'Rpl_semi_sync_slave%';
EOF
done

echo "âœ… åŠåŒæ­¥å¤åˆ¶é…ç½®å®Œæˆï¼"
```

#### æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å·¥å…·

```go
// æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å™¨
type ConsistencyChecker struct {
    masterDB *sql.DB
    slaveDB  *sql.DB
    config   *CheckConfig
}

type CheckConfig struct {
    Tables          []string      `json:"tables"`
    CheckInterval   time.Duration `json:"check_interval"`
    SampleSize      int           `json:"sample_size"`
    ToleranceWindow time.Duration `json:"tolerance_window"`
}

type ConsistencyResult struct {
    TableName       string    `json:"table_name"`
    MasterChecksum  string    `json:"master_checksum"`
    SlaveChecksum   string    `json:"slave_checksum"`
    IsConsistent    bool      `json:"is_consistent"`
    RowCount        int64     `json:"row_count"`
    CheckTime       time.Time `json:"check_time"`
    Inconsistencies []RowDiff `json:"inconsistencies,omitempty"`
}

type RowDiff struct {
    PrimaryKey   interface{} `json:"primary_key"`
    MasterData   map[string]interface{} `json:"master_data"`
    SlaveData    map[string]interface{} `json:"slave_data"`
    DiffFields   []string    `json:"diff_fields"`
}

// æ‰§è¡Œä¸€è‡´æ€§æ£€æŸ¥
func (cc *ConsistencyChecker) CheckTableConsistency(tableName string) (*ConsistencyResult, error) {
    result := &ConsistencyResult{
        TableName: tableName,
        CheckTime: time.Now(),
    }
    
    // 1. è·å–è¡¨ç»“æ„ä¿¡æ¯
    primaryKey, err := cc.getPrimaryKey(tableName)
    if err != nil {
        return nil, fmt.Errorf("è·å–ä¸»é”®å¤±è´¥: %w", err)
    }
    
    // 2. è®¡ç®—æ ¡éªŒå’Œ
    masterChecksum, err := cc.calculateChecksum(cc.masterDB, tableName)
    if err != nil {
        return nil, fmt.Errorf("è®¡ç®—ä¸»åº“æ ¡éªŒå’Œå¤±è´¥: %w", err)
    }
    
    slaveChecksum, err := cc.calculateChecksum(cc.slaveDB, tableName)
    if err != nil {
        return nil, fmt.Errorf("è®¡ç®—ä»åº“æ ¡éªŒå’Œå¤±è´¥: %w", err)
    }
    
    result.MasterChecksum = masterChecksum
    result.SlaveChecksum = slaveChecksum
    result.IsConsistent = masterChecksum == slaveChecksum
    
    // 3. å¦‚æœä¸ä¸€è‡´ï¼Œè¿›è¡Œè¯¦ç»†æ¯”è¾ƒ
    if !result.IsConsistent {
        inconsistencies, err := cc.findInconsistencies(tableName, primaryKey)
        if err != nil {
            log.Printf("æŸ¥æ‰¾ä¸ä¸€è‡´æ•°æ®å¤±è´¥: %v", err)
        } else {
            result.Inconsistencies = inconsistencies
        }
    }
    
    return result, nil
}

// è®¡ç®—è¡¨æ ¡éªŒå’Œ
func (cc *ConsistencyChecker) calculateChecksum(db *sql.DB, tableName string) (string, error) {
    query := fmt.Sprintf(`
        SELECT BIT_XOR(CAST(CRC32(CONCAT_WS(',', 
            %s
        )) AS UNSIGNED)) as checksum
        FROM %s
    `, cc.getAllColumns(tableName), tableName)
    
    var checksum sql.NullString
    err := db.QueryRow(query).Scan(&checksum)
    if err != nil {
        return "", err
    }
    
    if checksum.Valid {
        return checksum.String, nil
    }
    return "0", nil
}

// è·å–è¡¨çš„æ‰€æœ‰åˆ—
func (cc *ConsistencyChecker) getAllColumns(tableName string) string {
    query := `
        SELECT GROUP_CONCAT(column_name ORDER BY ordinal_position)
        FROM information_schema.columns 
        WHERE table_schema = DATABASE() 
        AND table_name = ?
    `
    
    var columns string
    cc.masterDB.QueryRow(query, tableName).Scan(&columns)
    return columns
}

// è·å–ä¸»é”®åˆ—
func (cc *ConsistencyChecker) getPrimaryKey(tableName string) (string, error) {
    query := `
        SELECT column_name
        FROM information_schema.key_column_usage
        WHERE table_schema = DATABASE()
        AND table_name = ?
        AND constraint_name = 'PRIMARY'
        ORDER BY ordinal_position
        LIMIT 1
    `
    
    var primaryKey string
    err := cc.masterDB.QueryRow(query, tableName).Scan(&primaryKey)
    return primaryKey, err
}

// æŸ¥æ‰¾ä¸ä¸€è‡´çš„æ•°æ®
func (cc *ConsistencyChecker) findInconsistencies(tableName, primaryKey string) ([]RowDiff, error) {
    // è·å–æ ·æœ¬æ•°æ®è¿›è¡Œæ¯”è¾ƒ
    query := fmt.Sprintf("SELECT * FROM %s ORDER BY %s LIMIT %d", 
                        tableName, primaryKey, cc.config.SampleSize)
    
    masterRows, err := cc.queryRows(cc.masterDB, query)
    if err != nil {
        return nil, err
    }
    
    slaveRows, err := cc.queryRows(cc.slaveDB, query)
    if err != nil {
        return nil, err
    }
    
    return cc.compareRows(masterRows, slaveRows, primaryKey), nil
}

func (cc *ConsistencyChecker) queryRows(db *sql.DB, query string) ([]map[string]interface{}, error) {
    rows, err := db.Query(query)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    columns, err := rows.Columns()
    if err != nil {
        return nil, err
    }
    
    var result []map[string]interface{}
    for rows.Next() {
        values := make([]interface{}, len(columns))
        valuePtrs := make([]interface{}, len(columns))
        for i := range values {
            valuePtrs[i] = &values[i]
        }
        
        if err := rows.Scan(valuePtrs...); err != nil {
            continue
        }
        
        row := make(map[string]interface{})
        for i, col := range columns {
            row[col] = values[i]
        }
        result = append(result, row)
    }
    
    return result, nil
}

func (cc *ConsistencyChecker) compareRows(masterRows, slaveRows []map[string]interface{}, primaryKey string) []RowDiff {
    var diffs []RowDiff
    
    // åˆ›å»ºä»åº“æ•°æ®çš„æ˜ å°„
    slaveMap := make(map[interface{}]map[string]interface{})
    for _, row := range slaveRows {
        if pk, ok := row[primaryKey]; ok {
            slaveMap[pk] = row
        }
    }
    
    // æ¯”è¾ƒä¸»åº“æ•°æ®
    for _, masterRow := range masterRows {
        pk := masterRow[primaryKey]
        slaveRow, exists := slaveMap[pk]
        
        if !exists {
            // ä»åº“ç¼ºå°‘è¯¥è¡Œ
            diffs = append(diffs, RowDiff{
                PrimaryKey: pk,
                MasterData: masterRow,
                SlaveData:  nil,
                DiffFields: []string{"missing_in_slave"},
            })
            continue
        }
        
        // æ¯”è¾ƒå­—æ®µå€¼
        var diffFields []string
        for field, masterValue := range masterRow {
            slaveValue := slaveRow[field]
            if !cc.valuesEqual(masterValue, slaveValue) {
                diffFields = append(diffFields, field)
            }
        }
        
        if len(diffFields) > 0 {
            diffs = append(diffs, RowDiff{
                PrimaryKey: pk,
                MasterData: masterRow,
                SlaveData:  slaveRow,
                DiffFields: diffFields,
            })
        }
    }
    
    return diffs
}

func (cc *ConsistencyChecker) valuesEqual(v1, v2 interface{}) bool {
    // å¤„ç†NULLå€¼
    if v1 == nil && v2 == nil {
        return true
    }
    if v1 == nil || v2 == nil {
        return false
    }
    
    // è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ¯”è¾ƒï¼ˆç®€åŒ–å¤„ç†ï¼‰
    return fmt.Sprintf("%v", v1) == fmt.Sprintf("%v", v2)
}
```

---

## 3. åˆ†åº“åˆ†è¡¨ä¸­é—´ä»¶å®ç°

### 3.1 Goè¯­è¨€åˆ†åº“åˆ†è¡¨ä¸­é—´ä»¶æ¶æ„

#### æ ¸å¿ƒæ¶æ„è®¾è®¡

```go
package sharding

import (
    "context"
    "database/sql"
    "fmt"
    "hash/crc32"
    "regexp"
    "strconv"
    "strings"
    "sync"
    "time"
)

// åˆ†ç‰‡ä¸­é—´ä»¶æ ¸å¿ƒç»“æ„
type ShardingMiddleware struct {
    config      *ShardingConfig
    dataSources map[string]*sql.DB
    router      *ShardingRouter
    parser      *SQLParser
    merger      *ResultMerger
    monitor     *ShardingMonitor
    mu          sync.RWMutex
}

// åˆ†ç‰‡é…ç½®
type ShardingConfig struct {
    Databases []DatabaseConfig `json:"databases"`
    Tables    []TableConfig    `json:"tables"`
    Rules     []ShardingRule   `json:"rules"`
}

type DatabaseConfig struct {
    Name        string `json:"name"`
    DSN         string `json:"dsn"`
    Weight      int    `json:"weight"`
    MaxConn     int    `json:"max_conn"`
    MaxIdle     int    `json:"max_idle"`
    MaxLifetime string `json:"max_lifetime"`
}

type TableConfig struct {
    LogicTable    string   `json:"logic_table"`
    ActualTables  []string `json:"actual_tables"`
    ShardingKey   string   `json:"sharding_key"`
    ShardingAlgo  string   `json:"sharding_algo"`
    DatabaseShards []string `json:"database_shards"`
}

type ShardingRule struct {
    TableName     string `json:"table_name"`
    ShardingKey   string `json:"sharding_key"`
    Algorithm     string `json:"algorithm"`
    ShardCount    int    `json:"shard_count"`
    DatabaseRule  string `json:"database_rule"`
    TableRule     string `json:"table_rule"`
}

// SQLè§£æå™¨
type SQLParser struct {
    selectRegex *regexp.Regexp
    insertRegex *regexp.Regexp
    updateRegex *regexp.Regexp
    deleteRegex *regexp.Regexp
}

type ParsedSQL struct {
    Type        SQLType                `json:"type"`
    TableName   string                 `json:"table_name"`
    Columns     []string               `json:"columns"`
    Values      []interface{}          `json:"values"`
    Conditions  map[string]interface{} `json:"conditions"`
    OrderBy     []string               `json:"order_by"`
    GroupBy     []string               `json:"group_by"`
    Limit       *LimitClause           `json:"limit"`
    OriginalSQL string                 `json:"original_sql"`
}

type SQLType int

const (
    SQLTypeSelect SQLType = iota
    SQLTypeInsert
    SQLTypeUpdate
    SQLTypeDelete
)

type LimitClause struct {
    Offset int `json:"offset"`
    Count  int `json:"count"`
}

// åˆ†ç‰‡è·¯ç”±å™¨
type ShardingRouter struct {
    config *ShardingConfig
    algos  map[string]ShardingAlgorithm
}

type ShardingAlgorithm interface {
    Shard(key interface{}, shardCount int) int
}

// å“ˆå¸Œåˆ†ç‰‡ç®—æ³•
type HashShardingAlgorithm struct{}

func (h *HashShardingAlgorithm) Shard(key interface{}, shardCount int) int {
    keyStr := fmt.Sprintf("%v", key)
    hash := crc32.ChecksumIEEE([]byte(keyStr))
    return int(hash) % shardCount
}

// èŒƒå›´åˆ†ç‰‡ç®—æ³•
type RangeShardingAlgorithm struct {
    Ranges []RangeConfig `json:"ranges"`
}

type RangeConfig struct {
    Min   int64 `json:"min"`
    Max   int64 `json:"max"`
    Shard int   `json:"shard"`
}

func (r *RangeShardingAlgorithm) Shard(key interface{}, shardCount int) int {
    keyInt, _ := strconv.ParseInt(fmt.Sprintf("%v", key), 10, 64)
    
    for _, rangeConfig := range r.Ranges {
        if keyInt >= rangeConfig.Min && keyInt <= rangeConfig.Max {
            return rangeConfig.Shard
        }
    }
    
    // é»˜è®¤ä½¿ç”¨å“ˆå¸Œ
    return int(keyInt) % shardCount
}

// æ—¶é—´åˆ†ç‰‡ç®—æ³•
type TimeShardingAlgorithm struct {
    TimeFormat string `json:"time_format"`
    ShardUnit  string `json:"shard_unit"` // month, day, year
}

func (t *TimeShardingAlgorithm) Shard(key interface{}, shardCount int) int {
    timeStr := fmt.Sprintf("%v", key)
    parsedTime, err := time.Parse(t.TimeFormat, timeStr)
    if err != nil {
        // è§£æå¤±è´¥ï¼Œä½¿ç”¨å“ˆå¸Œ
        return int(crc32.ChecksumIEEE([]byte(timeStr))) % shardCount
    }
    
    switch t.ShardUnit {
    case "month":
        return int(parsedTime.Month()) % shardCount
    case "day":
        return parsedTime.YearDay() % shardCount
    case "year":
        return parsedTime.Year() % shardCount
    default:
        return int(parsedTime.Unix()) % shardCount
    }
}

// åˆ›å»ºåˆ†ç‰‡ä¸­é—´ä»¶
func NewShardingMiddleware(config *ShardingConfig) (*ShardingMiddleware, error) {
    sm := &ShardingMiddleware{
        config:      config,
        dataSources: make(map[string]*sql.DB),
        router:      NewShardingRouter(config),
        parser:      NewSQLParser(),
        merger:      NewResultMerger(),
        monitor:     NewShardingMonitor(),
    }
    
    // åˆå§‹åŒ–æ•°æ®æºè¿æ¥
    if err := sm.initDataSources(); err != nil {
        return nil, err
    }
    
    return sm, nil
}

func (sm *ShardingMiddleware) initDataSources() error {
    for _, dbConfig := range sm.config.Databases {
        db, err := sql.Open("mysql", dbConfig.DSN)
        if err != nil {
            return fmt.Errorf("è¿æ¥æ•°æ®åº“ %s å¤±è´¥: %w", dbConfig.Name, err)
        }
        
        // è®¾ç½®è¿æ¥æ± å‚æ•°
        db.SetMaxOpenConns(dbConfig.MaxConn)
        db.SetMaxIdleConns(dbConfig.MaxIdle)
        
        if dbConfig.MaxLifetime != "" {
            if duration, err := time.ParseDuration(dbConfig.MaxLifetime); err == nil {
                db.SetConnMaxLifetime(duration)
            }
        }
        
        sm.dataSources[dbConfig.Name] = db
    }
    
    return nil
}

func NewShardingRouter(config *ShardingConfig) *ShardingRouter {
    router := &ShardingRouter{
        config: config,
        algos:  make(map[string]ShardingAlgorithm),
    }
    
    // æ³¨å†Œåˆ†ç‰‡ç®—æ³•
    router.algos["hash"] = &HashShardingAlgorithm{}
    router.algos["range"] = &RangeShardingAlgorithm{}
    router.algos["time"] = &TimeShardingAlgorithm{}
    
    return router
}

// è·¯ç”±æŸ¥è¯¢åˆ°å…·ä½“åˆ†ç‰‡
func (sr *ShardingRouter) Route(parsedSQL *ParsedSQL) (*RoutingResult, error) {
    tableConfig := sr.getTableConfig(parsedSQL.TableName)
    if tableConfig == nil {
        return &RoutingResult{
            DatabaseName: sr.config.Databases[0].Name,
            TableName:    parsedSQL.TableName,
        }, nil
    }
    
    shardingKey := tableConfig.ShardingKey
    shardingValue := parsedSQL.Conditions[shardingKey]
    
    if shardingValue == nil {
        // æ²¡æœ‰åˆ†ç‰‡é”®ï¼Œéœ€è¦æŸ¥è¯¢æ‰€æœ‰åˆ†ç‰‡
        return sr.routeToAllShards(tableConfig), nil
    }
    
    // è®¡ç®—åˆ†ç‰‡
    algo := sr.algos[tableConfig.ShardingAlgo]
    if algo == nil {
        algo = sr.algos["hash"] // é»˜è®¤ä½¿ç”¨å“ˆå¸Œ
    }
    
    shardIndex := algo.Shard(shardingValue, len(tableConfig.ActualTables))
    
    return &RoutingResult{
        DatabaseName: tableConfig.DatabaseShards[shardIndex%len(tableConfig.DatabaseShards)],
        TableName:    tableConfig.ActualTables[shardIndex],
        ShardIndex:   shardIndex,
    }, nil
}

type RoutingResult struct {
    DatabaseName string `json:"database_name"`
    TableName    string `json:"table_name"`
    ShardIndex   int    `json:"shard_index"`
    IsMultiShard bool   `json:"is_multi_shard"`
    AllShards    []ShardInfo `json:"all_shards,omitempty"`
}

type ShardInfo struct {
    DatabaseName string `json:"database_name"`
    TableName    string `json:"table_name"`
    ShardIndex   int    `json:"shard_index"`
}

func (sr *ShardingRouter) routeToAllShards(tableConfig *TableConfig) *RoutingResult {
    result := &RoutingResult{
        IsMultiShard: true,
        AllShards:    make([]ShardInfo, 0),
    }
    
    for i, tableName := range tableConfig.ActualTables {
        dbIndex := i % len(tableConfig.DatabaseShards)
        result.AllShards = append(result.AllShards, ShardInfo{
            DatabaseName: tableConfig.DatabaseShards[dbIndex],
            TableName:    tableName,
            ShardIndex:   i,
        })
    }
    
    return result
}

func (sr *ShardingRouter) getTableConfig(tableName string) *TableConfig {
    for _, tableConfig := range sr.config.Tables {
        if tableConfig.LogicTable == tableName {
            return &tableConfig
        }
    }
    return nil
}

// SQLè§£æå™¨å®ç°
func NewSQLParser() *SQLParser {
    return &SQLParser{
        selectRegex: regexp.MustCompile(`(?i)^\s*SELECT\s+(.+?)\s+FROM\s+(\w+)(?:\s+WHERE\s+(.+?))?(?:\s+ORDER\s+BY\s+(.+?))?(?:\s+LIMIT\s+(\d+)(?:\s*,\s*(\d+))?)?\s*$`),
        insertRegex: regexp.MustCompile(`(?i)^\s*INSERT\s+INTO\s+(\w+)\s*\(([^)]+)\)\s*VALUES\s*\(([^)]+)\)\s*$`),
        updateRegex: regexp.MustCompile(`(?i)^\s*UPDATE\s+(\w+)\s+SET\s+(.+?)(?:\s+WHERE\s+(.+?))?\s*$`),
        deleteRegex: regexp.MustCompile(`(?i)^\s*DELETE\s+FROM\s+(\w+)(?:\s+WHERE\s+(.+?))?\s*$`),
    }
}

func (sp *SQLParser) Parse(sql string) (*ParsedSQL, error) {
    sql = strings.TrimSpace(sql)
    
    // åˆ¤æ–­SQLç±»å‹å¹¶è§£æ
    if sp.selectRegex.MatchString(sql) {
        return sp.parseSelect(sql)
    } else if sp.insertRegex.MatchString(sql) {
        return sp.parseInsert(sql)
    } else if sp.updateRegex.MatchString(sql) {
        return sp.parseUpdate(sql)
    } else if sp.deleteRegex.MatchString(sql) {
        return sp.parseDelete(sql)
    }
    
    return nil, fmt.Errorf("ä¸æ”¯æŒçš„SQLç±»å‹: %s", sql)
}

func (sp *SQLParser) parseSelect(sql string) (*ParsedSQL, error) {
    matches := sp.selectRegex.FindStringSubmatch(sql)
    if len(matches) < 3 {
        return nil, fmt.Errorf("è§£æSELECTè¯­å¥å¤±è´¥")
    }
    
    parsed := &ParsedSQL{
        Type:        SQLTypeSelect,
        TableName:   matches[2],
        OriginalSQL: sql,
        Conditions:  make(map[string]interface{}),
    }
    
    // è§£æåˆ—
    if matches[1] != "*" {
        parsed.Columns = strings.Split(strings.ReplaceAll(matches[1], " ", ""), ",")
    }
    
    // è§£æWHEREæ¡ä»¶
    if len(matches) > 3 && matches[3] != "" {
        parsed.Conditions = sp.parseWhereConditions(matches[3])
    }
    
    // è§£æORDER BY
    if len(matches) > 4 && matches[4] != "" {
        parsed.OrderBy = strings.Split(strings.ReplaceAll(matches[4], " ", ""), ",")
    }
    
    // è§£æLIMIT
    if len(matches) > 5 && matches[5] != "" {
        limit := &LimitClause{}
        if len(matches) > 6 && matches[6] != "" {
            limit.Offset, _ = strconv.Atoi(matches[5])
            limit.Count, _ = strconv.Atoi(matches[6])
        } else {
            limit.Count, _ = strconv.Atoi(matches[5])
        }
        parsed.Limit = limit
    }
    
    return parsed, nil
}

func (sp *SQLParser) parseInsert(sql string) (*ParsedSQL, error) {
    matches := sp.insertRegex.FindStringSubmatch(sql)
    if len(matches) < 4 {
        return nil, fmt.Errorf("è§£æINSERTè¯­å¥å¤±è´¥")
    }
    
    parsed := &ParsedSQL{
        Type:        SQLTypeInsert,
        TableName:   matches[1],
        OriginalSQL: sql,
        Conditions:  make(map[string]interface{}),
    }
    
    // è§£æåˆ—å
    columns := strings.Split(strings.ReplaceAll(matches[2], " ", ""), ",")
    parsed.Columns = columns
    
    // è§£æå€¼
    values := strings.Split(matches[3], ",")
    parsed.Values = make([]interface{}, len(values))
    for i, val := range values {
        val = strings.Trim(strings.TrimSpace(val), "'\"")
        parsed.Values[i] = val
        
        // æ„å»ºæ¡ä»¶æ˜ å°„ï¼ˆç”¨äºåˆ†ç‰‡è·¯ç”±ï¼‰
        if i < len(columns) {
            parsed.Conditions[columns[i]] = val
        }
    }
    
    return parsed, nil
}

func (sp *SQLParser) parseUpdate(sql string) (*ParsedSQL, error) {
    matches := sp.updateRegex.FindStringSubmatch(sql)
    if len(matches) < 3 {
        return nil, fmt.Errorf("è§£æUPDATEè¯­å¥å¤±è´¥")
    }
    
    parsed := &ParsedSQL{
        Type:        SQLTypeUpdate,
        TableName:   matches[1],
        OriginalSQL: sql,
        Conditions:  make(map[string]interface{}),
    }
    
    // è§£æSETå­å¥
    setClause := matches[2]
    setPairs := strings.Split(setClause, ",")
    for _, pair := range setPairs {
        parts := strings.Split(strings.TrimSpace(pair), "=")
        if len(parts) == 2 {
            key := strings.TrimSpace(parts[0])
            value := strings.Trim(strings.TrimSpace(parts[1]), "'\"")
            parsed.Conditions[key] = value
        }
    }
    
    // è§£æWHEREæ¡ä»¶
    if len(matches) > 3 && matches[3] != "" {
        whereConditions := sp.parseWhereConditions(matches[3])
        for k, v := range whereConditions {
            parsed.Conditions[k] = v
        }
    }
    
    return parsed, nil
}

func (sp *SQLParser) parseDelete(sql string) (*ParsedSQL, error) {
    matches := sp.deleteRegex.FindStringSubmatch(sql)
    if len(matches) < 2 {
        return nil, fmt.Errorf("è§£æDELETEè¯­å¥å¤±è´¥")
    }
    
    parsed := &ParsedSQL{
        Type:        SQLTypeDelete,
        TableName:   matches[1],
        OriginalSQL: sql,
        Conditions:  make(map[string]interface{}),
    }
    
    // è§£æWHEREæ¡ä»¶
    if len(matches) > 2 && matches[2] != "" {
        parsed.Conditions = sp.parseWhereConditions(matches[2])
    }
    
    return parsed, nil
}

func (sp *SQLParser) parseWhereConditions(whereClause string) map[string]interface{} {
    conditions := make(map[string]interface{})
    
    // ç®€å•çš„æ¡ä»¶è§£æï¼ˆç”Ÿäº§ç¯å¢ƒéœ€è¦æ›´å¤æ‚çš„è§£æå™¨ï¼‰
    parts := strings.Split(whereClause, " AND ")
    for _, part := range parts {
        part = strings.TrimSpace(part)
        if strings.Contains(part, "=") {
            keyValue := strings.Split(part, "=")
            if len(keyValue) == 2 {
                key := strings.TrimSpace(keyValue[0])
                value := strings.Trim(strings.TrimSpace(keyValue[1]), "'\"")
                conditions[key] = value
            }
        }
    }
    
    return conditions
}

// æŸ¥è¯¢æ‰§è¡Œå™¨
func (sm *ShardingMiddleware) Execute(ctx context.Context, sql string, args ...interface{}) (*ShardingResult, error) {
    // 1. è§£æSQL
    parsedSQL, err := sm.parser.Parse(sql)
    if err != nil {
        return nil, fmt.Errorf("SQLè§£æå¤±è´¥: %w", err)
    }
    
    // 2. è·¯ç”±åˆ°åˆ†ç‰‡
    routingResult, err := sm.router.Route(parsedSQL)
    if err != nil {
        return nil, fmt.Errorf("è·¯ç”±å¤±è´¥: %w", err)
    }
    
    // 3. æ‰§è¡ŒæŸ¥è¯¢
    if routingResult.IsMultiShard {
        return sm.executeMultiShard(ctx, parsedSQL, routingResult, args...)
    } else {
        return sm.executeSingleShard(ctx, parsedSQL, routingResult, args...)
    }
}

type ShardingResult struct {
    Rows         []map[string]interface{} `json:"rows"`
    RowsAffected int64                    `json:"rows_affected"`
    LastInsertID int64                    `json:"last_insert_id"`
    ExecutionTime time.Duration           `json:"execution_time"`
    ShardCount   int                      `json:"shard_count"`
    Error        error                    `json:"error,omitempty"`
}

func (sm *ShardingMiddleware) executeSingleShard(ctx context.Context, parsedSQL *ParsedSQL, routing *RoutingResult, args ...interface{}) (*ShardingResult, error) {
    start := time.Now()
    
    db := sm.dataSources[routing.DatabaseName]
    if db == nil {
        return nil, fmt.Errorf("æ•°æ®æº %s ä¸å­˜åœ¨", routing.DatabaseName)
    }
    
    // æ›¿æ¢è¡¨å
    actualSQL := strings.ReplaceAll(parsedSQL.OriginalSQL, parsedSQL.TableName, routing.TableName)
    
    result := &ShardingResult{
        ShardCount: 1,
        ExecutionTime: time.Since(start),
    }
    
    switch parsedSQL.Type {
    case SQLTypeSelect:
        rows, err := db.QueryContext(ctx, actualSQL, args...)
        if err != nil {
            result.Error = err
            return result, err
        }
        defer rows.Close()
        
        result.Rows, err = sm.scanRows(rows)
        if err != nil {
            result.Error = err
            return result, err
        }
        
    default:
        execResult, err := db.ExecContext(ctx, actualSQL, args...)
        if err != nil {
            result.Error = err
            return result, err
        }
        
        result.RowsAffected, _ = execResult.RowsAffected()
        result.LastInsertID, _ = execResult.LastInsertId()
    }
    
    result.ExecutionTime = time.Since(start)
    return result, nil
}

func (sm *ShardingMiddleware) executeMultiShard(ctx context.Context, parsedSQL *ParsedSQL, routing *RoutingResult, args ...interface{}) (*ShardingResult, error) {
    start := time.Now()
    
    var wg sync.WaitGroup
    results := make([]*ShardingResult, len(routing.AllShards))
    errors := make([]error, len(routing.AllShards))
    
    // å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰åˆ†ç‰‡
    for i, shard := range routing.AllShards {
        wg.Add(1)
        go func(index int, shardInfo ShardInfo) {
            defer wg.Done()
            
            shardRouting := &RoutingResult{
                DatabaseName: shardInfo.DatabaseName,
                TableName:    shardInfo.TableName,
                ShardIndex:   shardInfo.ShardIndex,
            }
            
            result, err := sm.executeSingleShard(ctx, parsedSQL, shardRouting, args...)
            results[index] = result
            errors[index] = err
        }(i, shard)
    }
    
    wg.Wait()
    
    // åˆå¹¶ç»“æœ
    mergedResult := &ShardingResult{
        ShardCount:    len(routing.AllShards),
        ExecutionTime: time.Since(start),
    }
    
    return sm.merger.Merge(parsedSQL, results, errors, mergedResult)
}

// æ‰«ææŸ¥è¯¢ç»“æœ
func (sm *ShardingMiddleware) scanRows(rows *sql.Rows) ([]map[string]interface{}, error) {
    columns, err := rows.Columns()
    if err != nil {
        return nil, err
    }
    
    var result []map[string]interface{}
    for rows.Next() {
        values := make([]interface{}, len(columns))
        valuePtrs := make([]interface{}, len(columns))
        for i := range values {
            valuePtrs[i] = &values[i]
        }
        
        if err := rows.Scan(valuePtrs...); err != nil {
            return nil, err
        }
        
        row := make(map[string]interface{})
        for i, col := range columns {
            val := values[i]
            if b, ok := val.([]byte); ok {
                row[col] = string(b)
            } else {
                row[col] = val
            }
        }
        result = append(result, row)
    }
    
    return result, nil
}

// ç»“æœåˆå¹¶å™¨
type ResultMerger struct{}

func NewResultMerger() *ResultMerger {
    return &ResultMerger{}
}

func (rm *ResultMerger) Merge(parsedSQL *ParsedSQL, results []*ShardingResult, errors []error, mergedResult *ShardingResult) (*ShardingResult, error) {
    // æ£€æŸ¥é”™è¯¯
    for i, err := range errors {
        if err != nil {
            return nil, fmt.Errorf("åˆ†ç‰‡ %d æ‰§è¡Œå¤±è´¥: %w", i, err)
        }
    }
    
    switch parsedSQL.Type {
    case SQLTypeSelect:
        return rm.mergeSelectResults(parsedSQL, results, mergedResult)
    default:
        return rm.mergeModifyResults(results, mergedResult)
    }
}

func (rm *ResultMerger) mergeSelectResults(parsedSQL *ParsedSQL, results []*ShardingResult, mergedResult *ShardingResult) (*ShardingResult, error) {
    var allRows []map[string]interface{}
    
    // åˆå¹¶æ‰€æœ‰åˆ†ç‰‡çš„ç»“æœ
    for _, result := range results {
        if result != nil && result.Rows != nil {
            allRows = append(allRows, result.Rows...)
        }
    }
    
    // æ’åº
    if len(parsedSQL.OrderBy) > 0 {
        rm.sortRows(allRows, parsedSQL.OrderBy)
    }
    
    // åº”ç”¨LIMIT
    if parsedSQL.Limit != nil {
        start := parsedSQL.Limit.Offset
        end := start + parsedSQL.Limit.Count
        if start > len(allRows) {
            allRows = []map[string]interface{}{}
        } else if end > len(allRows) {
            allRows = allRows[start:]
        } else {
            allRows = allRows[start:end]
        }
    }
    
    mergedResult.Rows = allRows
    return mergedResult, nil
}

func (rm *ResultMerger) mergeModifyResults(results []*ShardingResult, mergedResult *ShardingResult) (*ShardingResult, error) {
    var totalAffected int64
    var lastInsertID int64
    
    for _, result := range results {
        if result != nil {
            totalAffected += result.RowsAffected
            if result.LastInsertID > 0 {
                lastInsertID = result.LastInsertID
            }
        }
    }
    
    mergedResult.RowsAffected = totalAffected
    mergedResult.LastInsertID = lastInsertID
    return mergedResult, nil
}

func (rm *ResultMerger) sortRows(rows []map[string]interface{}, orderBy []string) {
    // ç®€å•çš„æ’åºå®ç°ï¼ˆç”Ÿäº§ç¯å¢ƒéœ€è¦æ›´å¤æ‚çš„æ’åºé€»è¾‘ï¼‰
    // è¿™é‡Œåªå®ç°å•å­—æ®µæ’åº
    if len(orderBy) == 0 {
        return
    }
    
    sortField := strings.TrimSpace(orderBy[0])
    isDesc := strings.HasSuffix(strings.ToUpper(sortField), " DESC")
    if isDesc {
        sortField = strings.TrimSuffix(sortField, " DESC")
        sortField = strings.TrimSpace(sortField)
    }
    
    // è¿™é‡Œéœ€è¦å®ç°å…·ä½“çš„æ’åºé€»è¾‘
    // ç”±äºGoçš„sortåŒ…éœ€è¦å…·ä½“çš„æ¯”è¾ƒå‡½æ•°ï¼Œè¿™é‡Œçœç•¥å…·ä½“å®ç°
}

// åˆ†ç‰‡ç›‘æ§å™¨
type ShardingMonitor struct {
    metrics map[string]*ShardMetrics
    mu      sync.RWMutex
}

type ShardMetrics struct {
    QueryCount    int64         `json:"query_count"`
    ErrorCount    int64         `json:"error_count"`
    AvgLatency    time.Duration `json:"avg_latency"`
    MaxLatency    time.Duration `json:"max_latency"`
    LastQueryTime time.Time     `json:"last_query_time"`
}

func NewShardingMonitor() *ShardingMonitor {
    return &ShardingMonitor{
        metrics: make(map[string]*ShardMetrics),
    }
}

func (sm *ShardingMonitor) RecordQuery(shardName string, latency time.Duration, err error) {
    sm.mu.Lock()
    defer sm.mu.Unlock()
    
    metrics, exists := sm.metrics[shardName]
    if !exists {
        metrics = &ShardMetrics{}
        sm.metrics[shardName] = metrics
    }
    
    metrics.QueryCount++
    metrics.LastQueryTime = time.Now()
    
    if err != nil {
        metrics.ErrorCount++
    }
    
    // æ›´æ–°å»¶è¿Ÿç»Ÿè®¡
    if latency > metrics.MaxLatency {
        metrics.MaxLatency = latency
    }
    
    // ç®€å•çš„å¹³å‡å»¶è¿Ÿè®¡ç®—
    metrics.AvgLatency = (metrics.AvgLatency*time.Duration(metrics.QueryCount-1) + latency) / time.Duration(metrics.QueryCount)
}

func (sm *ShardingMonitor) GetMetrics() map[string]*ShardMetrics {
    sm.mu.RLock()
    defer sm.mu.RUnlock()
    
    result := make(map[string]*ShardMetrics)
    for k, v := range sm.metrics {
        result[k] = v
    }
    return result
}

### 3.2 åˆ†ç‰‡é…ç½®ç¤ºä¾‹

#### ç”µå•†è®¢å•è¡¨åˆ†ç‰‡é…ç½®

```json
{
  "databases": [
    {
      "name": "order_db_0",
      "dsn": "user:password@tcp(192.168.1.10:3306)/order_db_0?charset=utf8mb4",
      "weight": 1,
      "max_conn": 100,
      "max_idle": 10,
      "max_lifetime": "1h"
    },
    {
      "name": "order_db_1",
      "dsn": "user:password@tcp(192.168.1.11:3306)/order_db_1?charset=utf8mb4",
      "weight": 1,
      "max_conn": 100,
      "max_idle": 10,
      "max_lifetime": "1h"
    }
  ],
  "tables": [
    {
      "logic_table": "orders",
      "actual_tables": [
        "orders_0", "orders_1", "orders_2", "orders_3",
        "orders_4", "orders_5", "orders_6", "orders_7"
      ],
      "sharding_key": "user_id",
      "sharding_algo": "hash",
      "database_shards": ["order_db_0", "order_db_1"]
    },
    {
      "logic_table": "order_items",
      "actual_tables": [
        "order_items_0", "order_items_1", "order_items_2", "order_items_3"
      ],
      "sharding_key": "order_id",
      "sharding_algo": "hash",
      "database_shards": ["order_db_0", "order_db_1"]
    }
  ],
  "rules": [
    {
      "table_name": "orders",
      "sharding_key": "user_id",
      "algorithm": "hash",
      "shard_count": 8,
      "database_rule": "order_db_${user_id % 2}",
      "table_rule": "orders_${user_id % 8}"
    }
  ]
}
```

#### ä½¿ç”¨ç¤ºä¾‹

```go
func main() {
    // 1. åŠ è½½é…ç½®
    config := &ShardingConfig{}
    configData, _ := ioutil.ReadFile("sharding_config.json")
    json.Unmarshal(configData, config)
    
    // 2. åˆ›å»ºåˆ†ç‰‡ä¸­é—´ä»¶
    middleware, err := NewShardingMiddleware(config)
    if err != nil {
        log.Fatal("åˆ›å»ºåˆ†ç‰‡ä¸­é—´ä»¶å¤±è´¥:", err)
    }
    
    // 3. æ‰§è¡ŒæŸ¥è¯¢
    ctx := context.Background()
    
    // å•åˆ†ç‰‡æŸ¥è¯¢ï¼ˆåŒ…å«åˆ†ç‰‡é”®ï¼‰
    result, err := middleware.Execute(ctx, 
        "SELECT * FROM orders WHERE user_id = 12345")
    if err != nil {
        log.Printf("æŸ¥è¯¢å¤±è´¥: %v", err)
    } else {
        log.Printf("æŸ¥è¯¢ç»“æœ: %+v", result)
    }
    
    // å¤šåˆ†ç‰‡æŸ¥è¯¢ï¼ˆä¸åŒ…å«åˆ†ç‰‡é”®ï¼‰
    result, err = middleware.Execute(ctx, 
        "SELECT COUNT(*) FROM orders WHERE status = 'completed'")
    if err != nil {
        log.Printf("æŸ¥è¯¢å¤±è´¥: %v", err)
    } else {
        log.Printf("æŸ¥è¯¢ç»“æœ: %+v", result)
    }
    
    // æ’å…¥æ•°æ®
    result, err = middleware.Execute(ctx, 
        "INSERT INTO orders (user_id, total, status) VALUES (12345, 99.99, 'pending')")
    if err != nil {
        log.Printf("æ’å…¥å¤±è´¥: %v", err)
    } else {
        log.Printf("æ’å…¥ç»“æœ: %+v", result)
    }
}
```

---

## 4. å¤‡ä»½æ¢å¤è‡ªåŠ¨åŒ–è¿ç»´

### 4.1 mysqldumpè‡ªåŠ¨åŒ–å¤‡ä»½è„šæœ¬

#### å…¨é‡å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# MySQLå…¨é‡å¤‡ä»½è„šæœ¬
# æ–‡ä»¶å: mysql_full_backup.sh

set -e

# é…ç½®å‚æ•°
MYSQL_HOST="localhost"
MYSQL_PORT="3306"
MYSQL_USER="backup_user"
MYSQL_PASS="backup_password"
BACKUP_DIR="/data/mysql_backup"
LOG_FILE="/var/log/mysql_backup.log"
RETENTION_DAYS=7
COMPRESSION=true
ENCRYPTION=true
ENCRYPTION_KEY="/etc/mysql/backup.key"

# æ•°æ®åº“åˆ—è¡¨ï¼ˆç©ºè¡¨ç¤ºå¤‡ä»½æ‰€æœ‰æ•°æ®åº“ï¼‰
DATABASES=("production_db" "user_db" "order_db")

# é‚®ä»¶é€šçŸ¥é…ç½®
EMAIL_ENABLED=true
EMAIL_TO="admin@company.com"
SMTP_SERVER="smtp.company.com"

# é’‰é’‰é€šçŸ¥é…ç½®
DINGTALK_ENABLED=true
DINGTALK_WEBHOOK="https://oapi.dingtalk.com/robot/send?access_token=YOUR_TOKEN"

# æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# é”™è¯¯å¤„ç†å‡½æ•°
error_exit() {
    log "âŒ é”™è¯¯: $1"
    send_alert "å¤‡ä»½å¤±è´¥" "$1"
    exit 1
}

# å‘é€å‘Šè­¦
send_alert() {
    local title="$1"
    local message="$2"
    
    if [ "$EMAIL_ENABLED" = true ]; then
        echo "$message" | mail -s "$title" "$EMAIL_TO"
    fi
    
    if [ "$DINGTALK_ENABLED" = true ]; then
        curl -X POST "$DINGTALK_WEBHOOK" \
            -H 'Content-Type: application/json' \
            -d "{
                \"msgtype\": \"text\",
                \"text\": {
                    \"content\": \"ğŸš¨ MySQLå¤‡ä»½å‘Šè­¦\\næ ‡é¢˜: $title\\nè¯¦æƒ…: $message\\næ—¶é—´: $(date)\"
                }
            }"
    fi
}

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    log "ğŸ” æ£€æŸ¥ä¾èµ–..."
    
    command -v mysqldump >/dev/null 2>&1 || error_exit "mysqldump æœªå®‰è£…"
    command -v mysql >/dev/null 2>&1 || error_exit "mysql å®¢æˆ·ç«¯æœªå®‰è£…"
    
    if [ "$COMPRESSION" = true ]; then
        command -v gzip >/dev/null 2>&1 || error_exit "gzip æœªå®‰è£…"
    fi
    
    if [ "$ENCRYPTION" = true ]; then
        command -v openssl >/dev/null 2>&1 || error_exit "openssl æœªå®‰è£…"
        [ -f "$ENCRYPTION_KEY" ] || error_exit "åŠ å¯†å¯†é’¥æ–‡ä»¶ä¸å­˜åœ¨: $ENCRYPTION_KEY"
    fi
}

# æµ‹è¯•æ•°æ®åº“è¿æ¥
test_connection() {
    log "ğŸ”— æµ‹è¯•æ•°æ®åº“è¿æ¥..."
    
    mysql -h"$MYSQL_HOST" -P"$MYSQL_PORT" -u"$MYSQL_USER" -p"$MYSQL_PASS" \
        -e "SELECT 1" >/dev/null 2>&1 || error_exit "æ•°æ®åº“è¿æ¥å¤±è´¥"
    
    log "âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸"
}

# åˆ›å»ºå¤‡ä»½ç›®å½•
create_backup_dir() {
    local backup_date=$(date +%Y%m%d_%H%M%S)
    CURRENT_BACKUP_DIR="$BACKUP_DIR/$backup_date"
    
    mkdir -p "$CURRENT_BACKUP_DIR" || error_exit "åˆ›å»ºå¤‡ä»½ç›®å½•å¤±è´¥: $CURRENT_BACKUP_DIR"
    log "ğŸ“ åˆ›å»ºå¤‡ä»½ç›®å½•: $CURRENT_BACKUP_DIR"
}

# å¤‡ä»½å•ä¸ªæ•°æ®åº“
backup_database() {
    local db_name="$1"
    local backup_file="$CURRENT_BACKUP_DIR/${db_name}_$(date +%Y%m%d_%H%M%S).sql"
    
    log "ğŸ“¦ å¼€å§‹å¤‡ä»½æ•°æ®åº“: $db_name"
    
    # mysqldumpå‚æ•°è¯´æ˜:
    # --single-transaction: ä¿è¯InnoDBè¡¨çš„ä¸€è‡´æ€§
    # --routines: å¤‡ä»½å­˜å‚¨è¿‡ç¨‹å’Œå‡½æ•°
    # --triggers: å¤‡ä»½è§¦å‘å™¨
    # --events: å¤‡ä»½äº‹ä»¶
    # --hex-blob: ä½¿ç”¨åå…­è¿›åˆ¶æ ¼å¼å¤‡ä»½BLOBå­—æ®µ
    # --master-data=2: è®°å½•binlogä½ç½®ï¼ˆæ³¨é‡Šå½¢å¼ï¼‰
    # --flush-logs: åˆ·æ–°æ—¥å¿—
    mysqldump \
        -h"$MYSQL_HOST" \
        -P"$MYSQL_PORT" \
        -u"$MYSQL_USER" \
        -p"$MYSQL_PASS" \
        --single-transaction \
        --routines \
        --triggers \
        --events \
        --hex-blob \
        --master-data=2 \
        --flush-logs \
        --default-character-set=utf8mb4 \
        "$db_name" > "$backup_file" || error_exit "å¤‡ä»½æ•°æ®åº“ $db_name å¤±è´¥"
    
    # å‹ç¼©å¤‡ä»½æ–‡ä»¶
    if [ "$COMPRESSION" = true ]; then
        log "ğŸ—œï¸ å‹ç¼©å¤‡ä»½æ–‡ä»¶..."
        gzip "$backup_file" || error_exit "å‹ç¼©å¤‡ä»½æ–‡ä»¶å¤±è´¥"
        backup_file="${backup_file}.gz"
    fi
    
    # åŠ å¯†å¤‡ä»½æ–‡ä»¶
    if [ "$ENCRYPTION" = true ]; then
        log "ğŸ” åŠ å¯†å¤‡ä»½æ–‡ä»¶..."
        openssl enc -aes-256-cbc -salt -in "$backup_file" -out "${backup_file}.enc" \
            -pass file:"$ENCRYPTION_KEY" || error_exit "åŠ å¯†å¤‡ä»½æ–‡ä»¶å¤±è´¥"
        rm "$backup_file"
        backup_file="${backup_file}.enc"
    fi
    
    # è®¡ç®—æ–‡ä»¶å¤§å°å’Œæ ¡éªŒå’Œ
    local file_size=$(du -h "$backup_file" | cut -f1)
    local checksum=$(sha256sum "$backup_file" | cut -d' ' -f1)
    
    log "âœ… æ•°æ®åº“ $db_name å¤‡ä»½å®Œæˆ"
    log "   æ–‡ä»¶: $backup_file"
    log "   å¤§å°: $file_size"
    log "   æ ¡éªŒå’Œ: $checksum"
    
    # è®°å½•å¤‡ä»½ä¿¡æ¯
    echo "$db_name|$backup_file|$file_size|$checksum|$(date)" >> "$CURRENT_BACKUP_DIR/backup_info.txt"
}

# å¤‡ä»½æ‰€æœ‰æ•°æ®åº“
backup_all_databases() {
    if [ ${#DATABASES[@]} -eq 0 ]; then
        # è·å–æ‰€æœ‰æ•°æ®åº“åˆ—è¡¨
        local all_dbs=$(mysql -h"$MYSQL_HOST" -P"$MYSQL_PORT" -u"$MYSQL_USER" -p"$MYSQL_PASS" \
            -e "SHOW DATABASES;" | grep -v -E '^(Database|information_schema|performance_schema|mysql|sys)$')
        
        for db in $all_dbs; do
            backup_database "$db"
        done
    else
        for db in "${DATABASES[@]}"; do
            backup_database "$db"
        done
    fi
}

# æ¸…ç†æ—§å¤‡ä»½
cleanup_old_backups() {
    log "ğŸ§¹ æ¸…ç† $RETENTION_DAYS å¤©å‰çš„å¤‡ä»½..."
    
    find "$BACKUP_DIR" -type d -name "20*" -mtime +$RETENTION_DAYS -exec rm -rf {} + 2>/dev/null || true
    
    log "âœ… æ—§å¤‡ä»½æ¸…ç†å®Œæˆ"
}

# éªŒè¯å¤‡ä»½å®Œæ•´æ€§
verify_backup() {
    log "ğŸ” éªŒè¯å¤‡ä»½å®Œæ•´æ€§..."
    
    local backup_info_file="$CURRENT_BACKUP_DIR/backup_info.txt"
    if [ ! -f "$backup_info_file" ]; then
        error_exit "å¤‡ä»½ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨"
    fi
    
    while IFS='|' read -r db_name backup_file file_size checksum backup_time; do
        if [ ! -f "$backup_file" ]; then
            error_exit "å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $backup_file"
        fi
        
        local current_checksum=$(sha256sum "$backup_file" | cut -d' ' -f1)
        if [ "$checksum" != "$current_checksum" ]; then
            error_exit "å¤‡ä»½æ–‡ä»¶æ ¡éªŒå’Œä¸åŒ¹é…: $backup_file"
        fi
        
        log "âœ… $db_name å¤‡ä»½æ–‡ä»¶éªŒè¯é€šè¿‡"
    done < "$backup_info_file"
}

# ç”Ÿæˆå¤‡ä»½æŠ¥å‘Š
generate_report() {
    local report_file="$CURRENT_BACKUP_DIR/backup_report.txt"
    local total_size=$(du -sh "$CURRENT_BACKUP_DIR" | cut -f1)
    local backup_count=$(ls -1 "$CURRENT_BACKUP_DIR"/*.sql* 2>/dev/null | wc -l)
    
    cat > "$report_file" << EOF
MySQLå¤‡ä»½æŠ¥å‘Š
=============

å¤‡ä»½æ—¶é—´: $(date)
å¤‡ä»½ç›®å½•: $CURRENT_BACKUP_DIR
å¤‡ä»½æ•°é‡: $backup_count ä¸ªæ•°æ®åº“
æ€»å¤§å°: $total_size

å¤‡ä»½è¯¦æƒ…:
EOF
    
    if [ -f "$CURRENT_BACKUP_DIR/backup_info.txt" ]; then
        echo "" >> "$report_file"
        printf "%-20s %-50s %-10s %-20s\n" "æ•°æ®åº“" "å¤‡ä»½æ–‡ä»¶" "å¤§å°" "å¤‡ä»½æ—¶é—´" >> "$report_file"
        printf "%-20s %-50s %-10s %-20s\n" "--------" "--------" "----" "--------" >> "$report_file"
        
        while IFS='|' read -r db_name backup_file file_size checksum backup_time; do
            local short_file=$(basename "$backup_file")
            printf "%-20s %-50s %-10s %-20s\n" "$db_name" "$short_file" "$file_size" "$backup_time" >> "$report_file"
        done < "$CURRENT_BACKUP_DIR/backup_info.txt"
    fi
    
    log "ğŸ“Š å¤‡ä»½æŠ¥å‘Šå·²ç”Ÿæˆ: $report_file"
}

# ä¸»å‡½æ•°
main() {
    log "ğŸš€ å¼€å§‹MySQLå…¨é‡å¤‡ä»½..."
    
    check_dependencies
    test_connection
    create_backup_dir
    backup_all_databases
    verify_backup
    generate_report
    cleanup_old_backups
    
    local total_time=$(($(date +%s) - $start_time))
    log "âœ… å¤‡ä»½å®Œæˆï¼Œæ€»è€—æ—¶: ${total_time}ç§’"
    
    # å‘é€æˆåŠŸé€šçŸ¥
    local report_content=$(cat "$CURRENT_BACKUP_DIR/backup_report.txt")
    send_alert "MySQLå¤‡ä»½æˆåŠŸ" "$report_content"
}

# è„šæœ¬å…¥å£
start_time=$(date +%s)
trap 'error_exit "è„šæœ¬è¢«ä¸­æ–­"' INT TERM

main "$@"
```

#### å¢é‡å¤‡ä»½è„šæœ¬ï¼ˆåŸºäºbinlogï¼‰

```bash
#!/bin/bash
# MySQLå¢é‡å¤‡ä»½è„šæœ¬ï¼ˆåŸºäºbinlogï¼‰
# æ–‡ä»¶å: mysql_incremental_backup.sh

set -e

# é…ç½®å‚æ•°
MYSQL_HOST="localhost"
MYSQL_PORT="3306"
MYSQL_USER="backup_user"
MYSQL_PASS="backup_password"
BACKUP_DIR="/data/mysql_backup/incremental"
LOG_FILE="/var/log/mysql_incremental_backup.log"
BINLOG_DIR="/var/lib/mysql"
LAST_BACKUP_INFO="/data/mysql_backup/last_backup_info.txt"
RETENTION_DAYS=30

# æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# é”™è¯¯å¤„ç†å‡½æ•°
error_exit() {
    log "âŒ é”™è¯¯: $1"
    exit 1
}

# è·å–å½“å‰binlogä½ç½®
get_current_binlog_position() {
    mysql -h"$MYSQL_HOST" -P"$MYSQL_PORT" -u"$MYSQL_USER" -p"$MYSQL_PASS" \
        -e "SHOW MASTER STATUS\G" | grep -E "(File|Position)" | awk '{print $2}' | tr '\n' '|'
}

# è·å–ä¸Šæ¬¡å¤‡ä»½çš„binlogä½ç½®
get_last_backup_position() {
    if [ -f "$LAST_BACKUP_INFO" ]; then
        cat "$LAST_BACKUP_INFO"
    else
        echo "|"
    fi
}

# å¤‡ä»½binlog
backup_binlog() {
    local current_position=$(get_current_binlog_position)
    local last_position=$(get_last_backup_position)
    
    local current_file=$(echo "$current_position" | cut -d'|' -f1)
    local current_pos=$(echo "$current_position" | cut -d'|' -f2)
    local last_file=$(echo "$last_position" | cut -d'|' -f1)
    local last_pos=$(echo "$last_position" | cut -d'|' -f2)
    
    log "ğŸ“ å½“å‰binlogä½ç½®: $current_file:$current_pos"
    log "ğŸ“ ä¸Šæ¬¡å¤‡ä»½ä½ç½®: $last_file:$last_pos"
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•
    local backup_date=$(date +%Y%m%d_%H%M%S)
    local current_backup_dir="$BACKUP_DIR/$backup_date"
    mkdir -p "$current_backup_dir"
    
    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å¤‡ä»½ï¼Œä»å½“å‰ä½ç½®å¼€å§‹
    if [ -z "$last_file" ]; then
        log "ğŸ”„ é¦–æ¬¡å¢é‡å¤‡ä»½ï¼Œä»å½“å‰ä½ç½®å¼€å§‹"
        echo "$current_position" > "$LAST_BACKUP_INFO"
        return 0
    fi
    
    # ä½¿ç”¨mysqlbinlogå¤‡ä»½æŒ‡å®šèŒƒå›´çš„binlog
    local backup_file="$current_backup_dir/binlog_${backup_date}.sql"
    
    if [ "$last_file" = "$current_file" ]; then
        # åŒä¸€ä¸ªbinlogæ–‡ä»¶å†…çš„å¢é‡
        mysqlbinlog --start-position="$last_pos" --stop-position="$current_pos" \
            "$BINLOG_DIR/$current_file" > "$backup_file" || error_exit "å¤‡ä»½binlogå¤±è´¥"
    else
        # è·¨å¤šä¸ªbinlogæ–‡ä»¶çš„å¢é‡
        # è·å–binlogæ–‡ä»¶åˆ—è¡¨
        local binlog_files=$(mysql -h"$MYSQL_HOST" -P"$MYSQL_PORT" -u"$MYSQL_USER" -p"$MYSQL_PASS" \
            -e "SHOW BINARY LOGS" | awk 'NR>1 {print $1}' | \
            awk -v start="$last_file" -v end="$current_file" '
                $0 >= start && $0 <= end {print}'
        )
        
        > "$backup_file"  # æ¸…ç©ºæ–‡ä»¶
        
        local first_file=true
        for binlog_file in $binlog_files; do
            if [ "$first_file" = true ]; then
                # ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼šä»æŒ‡å®šä½ç½®å¼€å§‹
                mysqlbinlog --start-position="$last_pos" \
                    "$BINLOG_DIR/$binlog_file" >> "$backup_file"
                first_file=false
            elif [ "$binlog_file" = "$current_file" ]; then
                # æœ€åä¸€ä¸ªæ–‡ä»¶ï¼šåˆ°æŒ‡å®šä½ç½®ç»“æŸ
                mysqlbinlog --stop-position="$current_pos" \
                    "$BINLOG_DIR/$binlog_file" >> "$backup_file"
            else
                # ä¸­é—´æ–‡ä»¶ï¼šå®Œæ•´å¤‡ä»½
                mysqlbinlog "$BINLOG_DIR/$binlog_file" >> "$backup_file"
            fi
        done
    fi
    
    # å‹ç¼©å¤‡ä»½æ–‡ä»¶
    gzip "$backup_file"
    backup_file="${backup_file}.gz"
    
    # è®¡ç®—æ–‡ä»¶å¤§å°å’Œæ ¡éªŒå’Œ
    local file_size=$(du -h "$backup_file" | cut -f1)
    local checksum=$(sha256sum "$backup_file" | cut -d' ' -f1)
    
    log "âœ… å¢é‡å¤‡ä»½å®Œæˆ"
    log "   æ–‡ä»¶: $backup_file"
    log "   å¤§å°: $file_size"
    log "   æ ¡éªŒå’Œ: $checksum"
    log "   èŒƒå›´: $last_file:$last_pos -> $current_file:$current_pos"
    
    # æ›´æ–°å¤‡ä»½ä½ç½®ä¿¡æ¯
    echo "$current_position" > "$LAST_BACKUP_INFO"
    
    # è®°å½•å¤‡ä»½ä¿¡æ¯
    echo "$backup_date|$backup_file|$file_size|$checksum|$last_file:$last_pos|$current_file:$current_pos|$(date)" \
        >> "$BACKUP_DIR/incremental_backup_log.txt"
}

# æ¸…ç†æ—§çš„å¢é‡å¤‡ä»½
cleanup_old_incremental_backups() {
    log "ğŸ§¹ æ¸…ç† $RETENTION_DAYS å¤©å‰çš„å¢é‡å¤‡ä»½..."
    
    find "$BACKUP_DIR" -type d -name "20*" -mtime +$RETENTION_DAYS -exec rm -rf {} + 2>/dev/null || true
    
    log "âœ… æ—§å¢é‡å¤‡ä»½æ¸…ç†å®Œæˆ"
}

# ä¸»å‡½æ•°
main() {
    log "ğŸš€ å¼€å§‹MySQLå¢é‡å¤‡ä»½..."
    
    backup_binlog
    cleanup_old_incremental_backups
    
    log "âœ… å¢é‡å¤‡ä»½å®Œæˆ"
}

# è„šæœ¬å…¥å£
trap 'error_exit "è„šæœ¬è¢«ä¸­æ–­"' INT TERM
main "$@"
```

### 4.2 binlogæ¢å¤è„šæœ¬

#### è‡ªåŠ¨åŒ–æ¢å¤è„šæœ¬

```bash
#!/bin/bash
# MySQL binlogæ¢å¤è„šæœ¬
# æ–‡ä»¶å: mysql_recovery.sh

set -e

# é…ç½®å‚æ•°
MYSQL_HOST="localhost"
MYSQL_PORT="3306"
MYSQL_USER="root"
MYSQL_PASS="root_password"
BACKUP_DIR="/data/mysql_backup"
LOG_FILE="/var/log/mysql_recovery.log"
TEMP_DIR="/tmp/mysql_recovery"

# æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# é”™è¯¯å¤„ç†å‡½æ•°
error_exit() {
    log "âŒ é”™è¯¯: $1"
    exit 1
}

# æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
show_usage() {
    cat << EOF
ä½¿ç”¨è¯´æ˜:
$0 [é€‰é¡¹]

é€‰é¡¹:
  -t, --type TYPE           æ¢å¤ç±»å‹: full|incremental|point_in_time
  -d, --database DB         è¦æ¢å¤çš„æ•°æ®åº“å
  -f, --full-backup FILE    å…¨é‡å¤‡ä»½æ–‡ä»¶è·¯å¾„
  -i, --incremental-dir DIR å¢é‡å¤‡ä»½ç›®å½•
  -p, --point-in-time TIME  æ¢å¤åˆ°æŒ‡å®šæ—¶é—´ç‚¹ (æ ¼å¼: YYYY-MM-DD HH:MM:SS)
  -s, --start-time TIME     å¼€å§‹æ—¶é—´ (æ ¼å¼: YYYY-MM-DD HH:MM:SS)
  -e, --end-time TIME       ç»“æŸæ—¶é—´ (æ ¼å¼: YYYY-MM-DD HH:MM:SS)
  -n, --dry-run             è¯•è¿è¡Œæ¨¡å¼ï¼Œä¸å®é™…æ‰§è¡Œæ¢å¤
  -h, --help                æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹:
  # å…¨é‡æ¢å¤
  $0 -t full -d mydb -f /data/backup/mydb_20231201.sql.gz
  
  # å¢é‡æ¢å¤
  $0 -t incremental -d mydb -f /data/backup/mydb_20231201.sql.gz -i /data/backup/incremental
  
  # æ—¶é—´ç‚¹æ¢å¤
  $0 -t point_in_time -d mydb -f /data/backup/mydb_20231201.sql.gz -p "2023-12-01 15:30:00"
EOF
}

# è§£æå‘½ä»¤è¡Œå‚æ•°
parse_args() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            -t|--type)
                RECOVERY_TYPE="$2"
                shift 2
                ;;
            -d|--database)
                DATABASE="$2"
                shift 2
                ;;
            -f|--full-backup)
                FULL_BACKUP_FILE="$2"
                shift 2
                ;;
            -i|--incremental-dir)
                INCREMENTAL_DIR="$2"
                shift 2
                ;;
            -p|--point-in-time)
                POINT_IN_TIME="$2"
                shift 2
                ;;
            -s|--start-time)
                START_TIME="$2"
                shift 2
                ;;
            -e|--end-time)
                END_TIME="$2"
                shift 2
                ;;
            -n|--dry-run)
                DRY_RUN=true
                shift
                ;;
            -h|--help)
                show_usage
                exit 0
                ;;
            *)
                echo "æœªçŸ¥å‚æ•°: $1"
                show_usage
                exit 1
                ;;
        esac
    done
    
    # éªŒè¯å¿…éœ€å‚æ•°
    if [ -z "$RECOVERY_TYPE" ]; then
        error_exit "å¿…é¡»æŒ‡å®šæ¢å¤ç±»å‹ (-t|--type)"
    fi
    
    if [ -z "$DATABASE" ]; then
        error_exit "å¿…é¡»æŒ‡å®šæ•°æ®åº“å (-d|--database)"
    fi
    
    if [ -z "$FULL_BACKUP_FILE" ]; then
        error_exit "å¿…é¡»æŒ‡å®šå…¨é‡å¤‡ä»½æ–‡ä»¶ (-f|--full-backup)"
    fi
}

# éªŒè¯å¤‡ä»½æ–‡ä»¶
validate_backup_files() {
    log "ğŸ” éªŒè¯å¤‡ä»½æ–‡ä»¶..."
    
    if [ ! -f "$FULL_BACKUP_FILE" ]; then
        error_exit "å…¨é‡å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $FULL_BACKUP_FILE"
    fi
    
    if [ "$RECOVERY_TYPE" = "incremental" ] && [ ! -d "$INCREMENTAL_DIR" ]; then
        error_exit "å¢é‡å¤‡ä»½ç›®å½•ä¸å­˜åœ¨: $INCREMENTAL_DIR"
    fi
    
    log "âœ… å¤‡ä»½æ–‡ä»¶éªŒè¯é€šè¿‡"
}

# åˆ›å»ºä¸´æ—¶ç›®å½•
create_temp_dir() {
    rm -rf "$TEMP_DIR"
    mkdir -p "$TEMP_DIR"
    log "ğŸ“ åˆ›å»ºä¸´æ—¶ç›®å½•: $TEMP_DIR"
}

# è§£å‹å¤‡ä»½æ–‡ä»¶
extract_backup_file() {
    local backup_file="$1"
    local output_file="$2"
    
    log "ğŸ“¦ è§£å‹å¤‡ä»½æ–‡ä»¶: $backup_file"
    
    if [[ "$backup_file" == *.gz ]]; then
        gunzip -c "$backup_file" > "$output_file"
    elif [[ "$backup_file" == *.enc ]]; then
        # å¦‚æœæ˜¯åŠ å¯†æ–‡ä»¶ï¼Œéœ€è¦å…ˆè§£å¯†
        local encryption_key="/etc/mysql/backup.key"
        if [ ! -f "$encryption_key" ]; then
            error_exit "åŠ å¯†å¯†é’¥æ–‡ä»¶ä¸å­˜åœ¨: $encryption_key"
        fi
        openssl enc -aes-256-cbc -d -in "$backup_file" -out "$output_file" \
            -pass file:"$encryption_key" || error_exit "è§£å¯†å¤‡ä»½æ–‡ä»¶å¤±è´¥"
    else
        cp "$backup_file" "$output_file"
    fi
}

# æ‰§è¡ŒSQLæ–‡ä»¶
execute_sql_file() {
    local sql_file="$1"
    local description="$2"
    
    log "ğŸ”„ æ‰§è¡Œ$description: $sql_file"
    
    if [ "$DRY_RUN" = true ]; then
        log "ğŸ” [è¯•è¿è¡Œ] è·³è¿‡æ‰§è¡Œ: $sql_file"
        return 0
    fi
    
    mysql -h"$MYSQL_HOST" -P"$MYSQL_PORT" -u"$MYSQL_USER" -p"$MYSQL_PASS" \
        "$DATABASE" < "$sql_file" || error_exit "æ‰§è¡Œ$descriptionå¤±è´¥"
    
    log "âœ… $descriptionæ‰§è¡Œå®Œæˆ"
}

# å…¨é‡æ¢å¤
full_recovery() {
    log "ğŸš€ å¼€å§‹å…¨é‡æ¢å¤..."
    
    local temp_sql="$TEMP_DIR/full_backup.sql"
    extract_backup_file "$FULL_BACKUP_FILE" "$temp_sql"
    
    # åˆ›å»ºæ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if [ "$DRY_RUN" != true ]; then
        mysql -h"$MYSQL_HOST" -P"$MYSQL_PORT" -u"$MYSQL_USER" -p"$MYSQL_PASS" \
            -e "CREATE DATABASE IF NOT EXISTS \`$DATABASE\`" || error_exit "åˆ›å»ºæ•°æ®åº“å¤±è´¥"
    fi
    
    execute_sql_file "$temp_sql" "å…¨é‡å¤‡ä»½æ¢å¤"
    
    log "âœ… å…¨é‡æ¢å¤å®Œæˆ"
}

# å¢é‡æ¢å¤
incremental_recovery() {
    log "ğŸš€ å¼€å§‹å¢é‡æ¢å¤..."
    
    # å…ˆæ‰§è¡Œå…¨é‡æ¢å¤
    full_recovery
    
    # è·å–å¢é‡å¤‡ä»½æ–‡ä»¶åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
    local incremental_files=$(find "$INCREMENTAL_DIR" -name "*.sql.gz" -type f | sort)
    
    if [ -z "$incremental_files" ]; then
        log "âš ï¸ æœªæ‰¾åˆ°å¢é‡å¤‡ä»½æ–‡ä»¶"
        return 0
    fi
    
    # é€ä¸ªåº”ç”¨å¢é‡å¤‡ä»½
    for inc_file in $incremental_files; do
        local temp_inc_sql="$TEMP_DIR/$(basename "$inc_file" .gz)"
        extract_backup_file "$inc_file" "$temp_inc_sql"
        execute_sql_file "$temp_inc_sql" "å¢é‡å¤‡ä»½æ¢å¤ ($(basename "$inc_file"))"
        rm -f "$temp_inc_sql"
    done
    
    log "âœ… å¢é‡æ¢å¤å®Œæˆ"
}

# æ—¶é—´ç‚¹æ¢å¤
point_in_time_recovery() {
    log "ğŸš€ å¼€å§‹æ—¶é—´ç‚¹æ¢å¤..."
    
    if [ -z "$POINT_IN_TIME" ]; then
        error_exit "æ—¶é—´ç‚¹æ¢å¤éœ€è¦æŒ‡å®šç›®æ ‡æ—¶é—´ (-p|--point-in-time)"
    fi
    
    # å…ˆæ‰§è¡Œå…¨é‡æ¢å¤
    full_recovery
    
    # åº”ç”¨binlogåˆ°æŒ‡å®šæ—¶é—´ç‚¹
    local binlog_files=$(find "$INCREMENTAL_DIR" -name "*.sql.gz" -type f | sort)
    
    for binlog_file in $binlog_files; do
        local temp_binlog="$TEMP_DIR/$(basename "$binlog_file" .gz)"
        extract_backup_file "$binlog_file" "$temp_binlog"
        
        # ä½¿ç”¨mysqlbinlogè¿‡æ»¤åˆ°æŒ‡å®šæ—¶é—´ç‚¹
        local filtered_binlog="$TEMP_DIR/filtered_$(basename "$temp_binlog")"
        
        if [ -n "$START_TIME" ]; then
            mysqlbinlog --start-datetime="$START_TIME" --stop-datetime="$POINT_IN_TIME" \
                "$temp_binlog" > "$filtered_binlog"
        else
            mysqlbinlog --stop-datetime="$POINT_IN_TIME" \
                "$temp_binlog" > "$filtered_binlog"
        fi
        
        if [ -s "$filtered_binlog" ]; then
            execute_sql_file "$filtered_binlog" "æ—¶é—´ç‚¹æ¢å¤ (åˆ° $POINT_IN_TIME)"
        fi
        
        rm -f "$temp_binlog" "$filtered_binlog"
    done
    
    log "âœ… æ—¶é—´ç‚¹æ¢å¤å®Œæˆ"
}

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
cleanup() {
    log "ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶..."
    rm -rf "$TEMP_DIR"
}

# ä¸»å‡½æ•°
main() {
    log "ğŸš€ å¼€å§‹MySQLæ•°æ®æ¢å¤..."
    log "æ¢å¤ç±»å‹: $RECOVERY_TYPE"
    log "ç›®æ ‡æ•°æ®åº“: $DATABASE"
    log "å…¨é‡å¤‡ä»½: $FULL_BACKUP_FILE"
    
    if [ "$DRY_RUN" = true ]; then
        log "ğŸ” è¯•è¿è¡Œæ¨¡å¼å·²å¯ç”¨"
    fi
    
    validate_backup_files
    create_temp_dir
    
    case "$RECOVERY_TYPE" in
        "full")
            full_recovery
            ;;
        "incremental")
            incremental_recovery
            ;;
        "point_in_time")
            point_in_time_recovery
            ;;
        *)
            error_exit "ä¸æ”¯æŒçš„æ¢å¤ç±»å‹: $RECOVERY_TYPE"
            ;;
    esac
    
    cleanup
    
    log "âœ… æ•°æ®æ¢å¤å®Œæˆ"
}

# è„šæœ¬å…¥å£
trap 'cleanup; error_exit "è„šæœ¬è¢«ä¸­æ–­"' INT TERM

# è§£æå‚æ•°
parse_args "$@"

# æ‰§è¡Œä¸»å‡½æ•°
main
```

### 4.3 è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬é›†æˆ

#### å¤‡ä»½æ¢å¤ç®¡ç†è„šæœ¬

```bash
#!/bin/bash
# MySQLå¤‡ä»½æ¢å¤ç®¡ç†è„šæœ¬
# æ–‡ä»¶å: mysql_backup_manager.sh

set -e

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE="/etc/mysql/backup_config.conf"
SCRIPT_DIR="/opt/mysql_scripts"
LOG_DIR="/var/log/mysql_backup"

# é»˜è®¤é…ç½®
DEFAULT_CONFIG="
# MySQLå¤‡ä»½é…ç½®æ–‡ä»¶
MYSQL_HOST=localhost
MYSQL_PORT=3306
MYSQL_USER=backup_user
MYSQL_PASS=backup_password
BACKUP_DIR=/data/mysql_backup
FULL_BACKUP_SCHEDULE='0 2 * * 0'  # æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹
INCREMENTAL_BACKUP_SCHEDULE='0 */6 * * *'  # æ¯6å°æ—¶
RETENTION_DAYS=30
COMPRESSION=true
ENCRYPTION=true
ENCRYPTION_KEY=/etc/mysql/backup.key
EMAIL_ENABLED=true
EMAIL_TO=admin@company.com
DINGTALK_ENABLED=true
DINGTALK_WEBHOOK=https://oapi.dingtalk.com/robot/send?access_token=YOUR_TOKEN
"

# æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_DIR/manager.log"
}

# é”™è¯¯å¤„ç†å‡½æ•°
error_exit() {
    log "âŒ é”™è¯¯: $1"
    exit 1
}

# æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
show_usage() {
    cat << EOF
MySQLå¤‡ä»½æ¢å¤ç®¡ç†å·¥å…·

ä½¿ç”¨è¯´æ˜:
$0 [å‘½ä»¤] [é€‰é¡¹]

å‘½ä»¤:
  install           å®‰è£…å¤‡ä»½ç³»ç»Ÿ
  uninstall         å¸è½½å¤‡ä»½ç³»ç»Ÿ
  start             å¯åŠ¨å¤‡ä»½æœåŠ¡
  stop              åœæ­¢å¤‡ä»½æœåŠ¡
  status            æŸ¥çœ‹å¤‡ä»½çŠ¶æ€
  backup-now        ç«‹å³æ‰§è¡Œå¤‡ä»½
  list-backups      åˆ—å‡ºæ‰€æœ‰å¤‡ä»½
  restore           æ¢å¤æ•°æ®
  test              æµ‹è¯•å¤‡ä»½ç³»ç»Ÿ
  config            é…ç½®ç®¡ç†
  logs              æŸ¥çœ‹æ—¥å¿—

é€‰é¡¹:
  -h, --help        æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
  -v, --verbose     è¯¦ç»†è¾“å‡º
  -c, --config FILE æŒ‡å®šé…ç½®æ–‡ä»¶

ç¤ºä¾‹:
  $0 install                    # å®‰è£…å¤‡ä»½ç³»ç»Ÿ
  $0 backup-now --type full     # ç«‹å³æ‰§è¡Œå…¨é‡å¤‡ä»½
  $0 restore --help             # æŸ¥çœ‹æ¢å¤é€‰é¡¹
  $0 list-backups --last 7      # åˆ—å‡ºæœ€è¿‘7å¤©çš„å¤‡ä»½
EOF
}

# åŠ è½½é…ç½®
load_config() {
    if [ -f "$CONFIG_FILE" ]; then
        source "$CONFIG_FILE"
    else
        log "âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®"
    fi
}

# å®‰è£…å¤‡ä»½ç³»ç»Ÿ
install_backup_system() {
    log "ğŸš€ å¼€å§‹å®‰è£…MySQLå¤‡ä»½ç³»ç»Ÿ..."
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    mkdir -p "$SCRIPT_DIR" "$LOG_DIR" "$(dirname "$CONFIG_FILE")"
    mkdir -p "$BACKUP_DIR" "$BACKUP_DIR/incremental"
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    if [ ! -f "$CONFIG_FILE" ]; then
        echo "$DEFAULT_CONFIG" > "$CONFIG_FILE"
        log "ğŸ“ åˆ›å»ºé…ç½®æ–‡ä»¶: $CONFIG_FILE"
    fi
    
    # å¤åˆ¶è„šæœ¬æ–‡ä»¶
    cp "mysql_full_backup.sh" "$SCRIPT_DIR/"
    cp "mysql_incremental_backup.sh" "$SCRIPT_DIR/"
    cp "mysql_recovery.sh" "$SCRIPT_DIR/"
    chmod +x "$SCRIPT_DIR"/*.sh
    
    # åˆ›å»ºcrontabä»»åŠ¡
    local cron_file="/tmp/mysql_backup_cron"
    cat > "$cron_file" << EOF
# MySQLè‡ªåŠ¨å¤‡ä»½ä»»åŠ¡
$FULL_BACKUP_SCHEDULE $SCRIPT_DIR/mysql_full_backup.sh
$INCREMENTAL_BACKUP_SCHEDULE $SCRIPT_DIR/mysql_incremental_backup.sh
EOF
    
    crontab "$cron_file"
    rm "$cron_file"
    
    # ç”ŸæˆåŠ å¯†å¯†é’¥
    if [ ! -f "$ENCRYPTION_KEY" ]; then
        openssl rand -base64 32 > "$ENCRYPTION_KEY"
        chmod 600 "$ENCRYPTION_KEY"
        log "ğŸ” ç”ŸæˆåŠ å¯†å¯†é’¥: $ENCRYPTION_KEY"
    fi
    
    log "âœ… MySQLå¤‡ä»½ç³»ç»Ÿå®‰è£…å®Œæˆ"
    log "ğŸ“‹ è¯·ç¼–è¾‘é…ç½®æ–‡ä»¶: $CONFIG_FILE"
    log "ğŸ”‘ è¯·å¦¥å–„ä¿ç®¡åŠ å¯†å¯†é’¥: $ENCRYPTION_KEY"
}

# å¸è½½å¤‡ä»½ç³»ç»Ÿ
uninstall_backup_system() {
    log "ğŸ—‘ï¸ å¼€å§‹å¸è½½MySQLå¤‡ä»½ç³»ç»Ÿ..."
    
    # åˆ é™¤crontabä»»åŠ¡
    crontab -l | grep -v "mysql_.*_backup.sh" | crontab -
    
    # åˆ é™¤è„šæœ¬æ–‡ä»¶
    rm -rf "$SCRIPT_DIR"
    
    log "âœ… MySQLå¤‡ä»½ç³»ç»Ÿå¸è½½å®Œæˆ"
    log "âš ï¸ å¤‡ä»½æ•°æ®å’Œé…ç½®æ–‡ä»¶å·²ä¿ç•™"
}

# ç«‹å³æ‰§è¡Œå¤‡ä»½
backup_now() {
    local backup_type="$1"
    
    case "$backup_type" in
        "full")
            log "ğŸš€ æ‰§è¡Œå…¨é‡å¤‡ä»½..."
            "$SCRIPT_DIR/mysql_full_backup.sh"
            ;;
        "incremental")
            log "ğŸš€ æ‰§è¡Œå¢é‡å¤‡ä»½..."
            "$SCRIPT_DIR/mysql_incremental_backup.sh"
            ;;
        *)
            error_exit "ä¸æ”¯æŒçš„å¤‡ä»½ç±»å‹: $backup_type"
            ;;
    esac
}

# åˆ—å‡ºå¤‡ä»½
list_backups() {
    local days="${1:-30}"
    
    log "ğŸ“‹ æœ€è¿‘ $days å¤©çš„å¤‡ä»½åˆ—è¡¨:"
    
    echo "å…¨é‡å¤‡ä»½:"
    find "$BACKUP_DIR" -name "*.sql*" -mtime -"$days" -type f | sort -r | head -20
    
    echo "\nå¢é‡å¤‡ä»½:"
    find "$BACKUP_DIR/incremental" -name "*.sql*" -mtime -"$days" -type f | sort -r | head -20
}

# æŸ¥çœ‹å¤‡ä»½çŠ¶æ€
show_status() {
    log "ğŸ“Š MySQLå¤‡ä»½ç³»ç»ŸçŠ¶æ€:"
    
    # æ£€æŸ¥crontabä»»åŠ¡
    echo "å®šæ—¶ä»»åŠ¡çŠ¶æ€:"
    crontab -l | grep "mysql_.*_backup.sh" || echo "æœªæ‰¾åˆ°å®šæ—¶ä»»åŠ¡"
    
    # æ£€æŸ¥æœ€è¿‘çš„å¤‡ä»½
    echo "\næœ€è¿‘çš„å…¨é‡å¤‡ä»½:"
    find "$BACKUP_DIR" -name "*.sql*" -mtime -7 -type f | sort -r | head -5
    
    echo "\næœ€è¿‘çš„å¢é‡å¤‡ä»½:"
    find "$BACKUP_DIR/incremental" -name "*.sql*" -mtime -1 -type f | sort -r | head -5
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    echo "\nå¤‡ä»½ç›®å½•ç£ç›˜ä½¿ç”¨æƒ…å†µ:"
    du -sh "$BACKUP_DIR"
}

# æŸ¥çœ‹æ—¥å¿—
show_logs() {
    local log_type="${1:-all}"
    local lines="${2:-50}"
    
    case "$log_type" in
        "full")
            tail -n "$lines" "/var/log/mysql_backup.log"
            ;;
        "incremental")
            tail -n "$lines" "/var/log/mysql_incremental_backup.log"
            ;;
        "recovery")
            tail -n "$lines" "/var/log/mysql_recovery.log"
            ;;
        "all")
            echo "=== å…¨é‡å¤‡ä»½æ—¥å¿— ==="
            tail -n 20 "/var/log/mysql_backup.log" 2>/dev/null || echo "æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨"
            echo "\n=== å¢é‡å¤‡ä»½æ—¥å¿— ==="
            tail -n 20 "/var/log/mysql_incremental_backup.log" 2>/dev/null || echo "æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨"
            echo "\n=== æ¢å¤æ—¥å¿— ==="
            tail -n 20 "/var/log/mysql_recovery.log" 2>/dev/null || echo "æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨"
            ;;
        *)
            error_exit "ä¸æ”¯æŒçš„æ—¥å¿—ç±»å‹: $log_type"
            ;;
    esac
}

# æµ‹è¯•å¤‡ä»½ç³»ç»Ÿ
test_backup_system() {
    log "ğŸ§ª æµ‹è¯•MySQLå¤‡ä»½ç³»ç»Ÿ..."
    
    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    mysql -h"$MYSQL_HOST" -P"$MYSQL_PORT" -u"$MYSQL_USER" -p"$MYSQL_PASS" \
        -e "SELECT 1" >/dev/null 2>&1 || error_exit "æ•°æ®åº“è¿æ¥å¤±è´¥"
    log "âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸"
    
    # æµ‹è¯•å¤‡ä»½ç›®å½•æƒé™
    touch "$BACKUP_DIR/test_file" && rm "$BACKUP_DIR/test_file" || error_exit "å¤‡ä»½ç›®å½•æƒé™ä¸è¶³"
    log "âœ… å¤‡ä»½ç›®å½•æƒé™æ­£å¸¸"
    
    # æµ‹è¯•åŠ å¯†å¯†é’¥
    if [ "$ENCRYPTION" = true ] && [ ! -f "$ENCRYPTION_KEY" ]; then
        error_exit "åŠ å¯†å¯†é’¥æ–‡ä»¶ä¸å­˜åœ¨: $ENCRYPTION_KEY"
    fi
    log "âœ… åŠ å¯†é…ç½®æ­£å¸¸"
    
    log "âœ… å¤‡ä»½ç³»ç»Ÿæµ‹è¯•é€šè¿‡"
}

# ä¸»å‡½æ•°
main() {
    local command="$1"
    shift
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    mkdir -p "$LOG_DIR"
    
    case "$command" in
        "install")
            install_backup_system
            ;;
        "uninstall")
            uninstall_backup_system
            ;;
        "backup-now")
            load_config
            backup_now "${1:-full}"
            ;;
        "list-backups")
            load_config
            list_backups "$1"
            ;;
        "status")
            load_config
            show_status
            ;;
        "logs")
            show_logs "$1" "$2"
            ;;
        "test")
            load_config
            test_backup_system
            ;;
        "restore")
            load_config
            "$SCRIPT_DIR/mysql_recovery.sh" "$@"
            ;;
        "config")
            echo "é…ç½®æ–‡ä»¶ä½ç½®: $CONFIG_FILE"
            if [ "$1" = "edit" ]; then
                ${EDITOR:-vi} "$CONFIG_FILE"
            fi
            ;;
        "help"|-h|--help)
            show_usage
            ;;
        *)
            echo "æœªçŸ¥å‘½ä»¤: $command"
            show_usage
            exit 1
            ;;
    esac
}

# è„šæœ¬å…¥å£
if [ $# -eq 0 ]; then
    show_usage
    exit 1
fi

main "$@"
```

---

## 5. æ€»ç»“ä¸æœ€ä½³å®è·µ

### 5.1 æ ¸å¿ƒä¼˜åŒ–è¦ç‚¹

#### æ…¢æŸ¥è¯¢ä¼˜åŒ–
- **EXPLAINåˆ†æ**: é‡ç‚¹å…³æ³¨typeã€keyã€rowsã€Extraå­—æ®µ
- **ç´¢å¼•è®¾è®¡**: éµå¾ªæœ€å·¦å‰ç¼€åŸåˆ™ï¼Œé¿å…è¿‡åº¦ç´¢å¼•
- **æŸ¥è¯¢é‡å†™**: é¿å…SELECT *ï¼Œåˆç†ä½¿ç”¨LIMIT
- **ç»Ÿè®¡ä¿¡æ¯**: å®šæœŸæ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯

#### ä¸»ä»åŒæ­¥ä¼˜åŒ–
- **å»¶è¿Ÿç›‘æ§**: å®æ—¶ç›‘æ§Seconds_Behind_Master
- **å¹¶è¡Œå¤åˆ¶**: å¯ç”¨åŸºäºLOGICAL_CLOCKçš„å¹¶è¡Œå¤åˆ¶
- **åŠåŒæ­¥å¤åˆ¶**: ä¿è¯æ•°æ®ä¸€è‡´æ€§
- **è¯»å†™åˆ†ç¦»**: åˆç†åˆ†é…è¯»å†™æµé‡

#### åˆ†åº“åˆ†è¡¨ç­–ç•¥
- **åˆ†ç‰‡é”®é€‰æ‹©**: é€‰æ‹©åˆ†å¸ƒå‡åŒ€çš„å­—æ®µä½œä¸ºåˆ†ç‰‡é”®
- **è·¯ç”±ç®—æ³•**: æ ¹æ®ä¸šåŠ¡ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„åˆ†ç‰‡ç®—æ³•
- **è·¨åˆ†ç‰‡æŸ¥è¯¢**: å°½é‡é¿å…ï¼Œå¿…è¦æ—¶ä½¿ç”¨ç»“æœåˆå¹¶
- **æ•°æ®è¿ç§»**: åˆ¶å®šå®Œå–„çš„æ‰©å®¹æ–¹æ¡ˆ

#### å¤‡ä»½æ¢å¤è‡ªåŠ¨åŒ–
- **å¤‡ä»½ç­–ç•¥**: å…¨é‡+å¢é‡çš„ç»„åˆå¤‡ä»½ç­–ç•¥
- **å‹ç¼©åŠ å¯†**: èŠ‚çœå­˜å‚¨ç©ºé—´ï¼Œä¿è¯æ•°æ®å®‰å…¨
- **éªŒè¯æœºåˆ¶**: å®šæœŸéªŒè¯å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§
- **æ¢å¤æ¼”ç»ƒ**: å®šæœŸè¿›è¡Œæ¢å¤æ¼”ç»ƒ

### 5.2 ç”Ÿäº§ç¯å¢ƒå»ºè®®

1. **ç›‘æ§å‘Šè­¦**: å»ºç«‹å®Œå–„çš„ç›‘æ§å‘Šè­¦ä½“ç³»
2. **å®¹é‡è§„åˆ’**: æå‰è¿›è¡Œå®¹é‡è§„åˆ’å’Œæ‰©å®¹å‡†å¤‡
3. **å®‰å…¨åŠ å›º**: å®æ–½æ•°æ®åº“å®‰å…¨æœ€ä½³å®è·µ
4. **æ–‡æ¡£ç®¡ç†**: ç»´æŠ¤è¯¦ç»†çš„è¿ç»´æ–‡æ¡£
5. **å›¢é˜ŸåŸ¹è®­**: å®šæœŸè¿›è¡ŒæŠ€æœ¯åŸ¹è®­å’Œæ¼”ç»ƒ

é€šè¿‡ä»¥ä¸Šæ·±åº¦ä¼˜åŒ–æ–¹æ¡ˆï¼Œå¯ä»¥æ˜¾è‘—æå‡MySQLæ•°æ®åº“çš„æ€§èƒ½ã€å¯ç”¨æ€§å’Œå¯ç»´æŠ¤æ€§ï¼Œä¸ºä¸šåŠ¡å‘å±•æä¾›å¼ºæœ‰åŠ›çš„æ•°æ®æ”¯æ’‘ã€‚