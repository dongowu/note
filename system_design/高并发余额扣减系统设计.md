# 高并发余额扣减系统设计

## 业务场景概述

### 核心问题
高并发场景分为高并发读和高并发写，账户余额扣减属于高并发写场景，处理难度更大。最难处理的是具有**写热点**的场景：
- **分散热点场景**：每秒钟有一万个账户同时进行一笔余额扣减操作（相对简单）
- **集中热点场景**：每秒钟有一个账户进行一万笔余额扣减操作（极其困难）

### 典型应用场景
**广告计费平台**是典型的集中热点场景：
- 用户每点击/浏览一次广告，都会对广告主账户余额扣减一次
- 扣减操作必须实时，否则账户没钱但广告继续展示会造成平台损失
- 大广告主可能产生每秒上万次扣费操作

## 系统演进方案

### 1. 原始状态（基础版本）

#### 业务流程
```
用户点击广告 → 记录流水 → 反作弊验证 → 账户扣费 → 更新流水 → 后续处理
```

#### 核心步骤
1. **记录扣费流水**：初始状态，防止Kafka宕机或消费失败导致无法补偿
2. **反作弊验证**：验证点击的真实性
3. **账户扣费**：核心业务逻辑
4. **更新流水**：将流水状态更新为"已扣费"
5. **后续处理**：账户余额为0时通知上游系统下线广告

#### 性能瓶颈
- 数据库负载和IO成为瓶颈
- 无法承载大广告主的高并发投放

### 2. 异步消峰（优化版本1）

#### 架构改进
```
用户点击 → 持久化流水 → 投递Kafka → 异步消费处理
```

#### 核心优势
- 引入Kafka进行异步消峰
- 流水持久化保证可回溯性、可补偿性
- 消费者按自身节奏处理，避免瞬时压力

#### 存在问题
- 瞬时高并发导致Kafka消息积压
- 广告下线不及时，产生平台资损

### 3. 并行化处理（优化版本2）

#### 技术方案
```go
// Kafka消费者配置
max.poll.records = 500  // 批量拉取消息数量

// 线程池并行处理
type MessageProcessor struct {
    workerPool *sync.Pool
    semaphore  chan struct{}
}

func (p *MessageProcessor) ProcessBatch(messages []Message) {
    for _, msg := range messages {
        p.semaphore <- struct{}{} // 控制并发数
        go func(m Message) {
            defer func() { <-p.semaphore }()
            p.processMessage(m)
        }(msg)
    }
}
```

#### 核心改进
- 通过线程池并行处理消息提升吞吐量
- 可配置并发度控制资源使用

#### 注意事项
- 无法保证消息有序性（广告计费场景可接受）
- 可能将瓶颈转移到数据库

### 4. 分库分表（优化版本3）

#### 数据库设计
```sql
-- 扣费流水表（按广告主ID分片）
CREATE TABLE billing_log_${shard} (
    id BIGINT PRIMARY KEY,
    advertiser_id BIGINT,
    amount DECIMAL(10,2),
    status TINYINT,
    created_at TIMESTAMP,
    INDEX idx_advertiser_time (advertiser_id, created_at)
);

-- 广告主账户余额表（按广告主ID分片）
CREATE TABLE advertiser_balance_${shard} (
    advertiser_id BIGINT PRIMARY KEY,
    balance DECIMAL(10,2),
    updated_at TIMESTAMP,
    version INT  -- 乐观锁版本号
);
```

#### 分片策略
- **Sharding Key**：广告主ID
- **分片算法**：`advertiser_id % shard_count`
- **冷数据归档**：定期归档历史流水数据

#### 性能提升
- 分散数据库压力
- 提高并发处理能力
- 支持水平扩展

### 5. 分散热点（终极方案）

#### 核心思想
将一个广告主的主账户拆分成多个子账户，分散到不同库表中。

#### 技术实现
```go
type AccountSplitter struct {
    subAccountCount int
    shardCount     int
}

// 账户拆分策略
func (as *AccountSplitter) GetSubAccountID(advertiserID int64, requestID string) int64 {
    // 基于请求ID哈希，确保同一请求路由到同一子账户
    hash := crc32.ChecksumIEEE([]byte(requestID))
    subIndex := hash % uint32(as.subAccountCount)
    return advertiserID*1000 + int64(subIndex)
}

// 余额扣减逻辑
func (as *AccountSplitter) DeductBalance(advertiserID int64, amount decimal.Decimal, requestID string) error {
    subAccountID := as.GetSubAccountID(advertiserID, requestID)
    
    // 路由到对应分片
    shardIndex := subAccountID % int64(as.shardCount)
    db := as.getShardDB(shardIndex)
    
    // 执行扣减操作
    return as.executeDeduction(db, subAccountID, amount)
}
```

#### 子账户管理
```go
type SubAccountManager struct {
    redis *redis.Client
}

// 子账户余额分配
func (sam *SubAccountManager) DistributeBalance(advertiserID int64, totalAmount decimal.Decimal) error {
    subAccounts := sam.getSubAccounts(advertiserID)
    avgAmount := totalAmount.Div(decimal.NewFromInt(int64(len(subAccounts))))
    
    for _, subAccountID := range subAccounts {
        if err := sam.updateSubAccountBalance(subAccountID, avgAmount); err != nil {
            return err
        }
    }
    return nil
}

// 余额聚合查询
func (sam *SubAccountManager) GetTotalBalance(advertiserID int64) (decimal.Decimal, error) {
    subAccounts := sam.getSubAccounts(advertiserID)
    total := decimal.Zero
    
    for _, subAccountID := range subAccounts {
        balance, err := sam.getSubAccountBalance(subAccountID)
        if err != nil {
            return decimal.Zero, err
        }
        total = total.Add(balance)
    }
    return total, nil
}
```

## 核心技术要点

### 1. 数据库行锁优化
```sql
-- 使用乐观锁避免长时间锁定
UPDATE advertiser_balance 
SET balance = balance - ?, version = version + 1, updated_at = NOW()
WHERE advertiser_id = ? AND version = ? AND balance >= ?;
```

### 2. 缓存策略
```go
type BalanceCache struct {
    redis *redis.Client
    ttl   time.Duration
}

// 缓存余额信息，减少数据库查询
func (bc *BalanceCache) GetBalance(accountID int64) (decimal.Decimal, error) {
    key := fmt.Sprintf("balance:%d", accountID)
    
    // 先查缓存
    if cached, err := bc.redis.Get(key).Result(); err == nil {
        return decimal.NewFromString(cached)
    }
    
    // 缓存未命中，查数据库
    balance, err := bc.getBalanceFromDB(accountID)
    if err != nil {
        return decimal.Zero, err
    }
    
    // 更新缓存
    bc.redis.SetEX(key, balance.String(), bc.ttl)
    return balance, nil
}
```

### 3. 监控告警
```go
type PerformanceMonitor struct {
    metrics map[string]int64
    mutex   sync.RWMutex
}

func (pm *PerformanceMonitor) RecordDeduction(advertiserID int64, duration time.Duration) {
    pm.mutex.Lock()
    defer pm.mutex.Unlock()
    
    // 记录扣减耗时
    pm.metrics["deduction_duration"] += duration.Milliseconds()
    pm.metrics["deduction_count"]++
    
    // 热点账户监控
    if pm.metrics[fmt.Sprintf("hot_account:%d", advertiserID)]++; 
       pm.metrics[fmt.Sprintf("hot_account:%d", advertiserID)] > 1000 {
        pm.alertHotAccount(advertiserID)
    }
}
```

## 系统架构总结

### 技术栈选型
- **消息队列**：Kafka（异步消峰）
- **数据库**：MySQL分库分表
- **缓存**：Redis（余额缓存、热点检测）
- **监控**：Prometheus + Grafana
- **负载均衡**：Nginx/LVS

### 性能指标
- **吞吐量**：支持单账户每秒万级扣减操作
- **延迟**：P99延迟 < 100ms
- **可用性**：99.99%
- **数据一致性**：最终一致性

### 扩展性考虑
1. **水平扩展**：支持动态增加分片
2. **垂直扩展**：支持单机性能提升
3. **跨地域部署**：支持多机房容灾
4. **弹性伸缩**：根据流量自动调整资源

## 风险控制

### 1. 数据一致性
- 使用分布式事务（Seata/DTM）
- 实现补偿机制
- 定期数据校验

### 2. 系统容错
- 熔断降级（Hystrix/Sentinel）
- 限流控制
- 优雅降级策略

### 3. 运维监控
- 实时监控关键指标
- 自动告警机制
- 故障快速恢复

这套方案通过**异步消峰 → 并行处理 → 分库分表 → 分散热点**的渐进式优化，最终实现了高并发场景下的账户余额扣减系统，可支撑大规模广告计费业务的性能需求。