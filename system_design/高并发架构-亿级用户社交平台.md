# 高并发架构：亿级用户社交平台的技术架构设计

## 1. 系统概述

### 1.1 业务场景
- **用户规模**: 10亿+注册用户，1亿+日活用户
- **核心功能**: 用户关系、动态发布、消息推送、内容推荐
- **性能要求**: 
  - QPS峰值: 100万+
  - 响应时间: P99 < 100ms
  - 可用性: 99.99%

### 1.2 技术挑战
- **海量数据存储**: PB级数据存储与检索
- **高并发读写**: 千万级并发请求处理
- **实时性要求**: 毫秒级消息推送
- **全球化部署**: 多地域容灾与就近访问

---

## 2. 整体架构设计

### 2.1 分层架构

```
┌─────────────────────────────────────────────────────────────┐
│                        CDN + 边缘节点                        │
├─────────────────────────────────────────────────────────────┤
│                      负载均衡层 (LVS/Nginx)                   │
├─────────────────────────────────────────────────────────────┤
│                      API网关层 (Kong/Zuul)                   │
├─────────────────────────────────────────────────────────────┤
│  业务服务层  │  用户服务  │  关系服务  │  内容服务  │  推送服务  │
├─────────────────────────────────────────────────────────────┤
│              中间件层 (Redis/Kafka/ElasticSearch)             │
├─────────────────────────────────────────────────────────────┤
│                数据存储层 (MySQL/MongoDB/HBase)               │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 核心组件

#### 接入层
- **CDN**: 静态资源缓存，全球节点部署
- **负载均衡**: LVS四层 + Nginx七层负载均衡
- **API网关**: 统一入口，限流熔断，安全认证

#### 服务层
- **用户服务**: 用户信息管理，认证授权
- **关系服务**: 好友关系，关注粉丝
- **内容服务**: 动态发布，内容审核
- **推送服务**: 实时消息推送

#### 数据层
- **缓存层**: Redis集群，多级缓存
- **消息队列**: Kafka集群，异步处理
- **存储层**: 分库分表，读写分离

---

## 3. 核心服务设计

### 3.1 用户服务架构

#### 用户信息存储
```go
// 用户基础信息
type User struct {
    UserID      int64     `json:"user_id" db:"user_id"`
    Username    string    `json:"username" db:"username"`
    Email       string    `json:"email" db:"email"`
    Phone       string    `json:"phone" db:"phone"`
    Avatar      string    `json:"avatar" db:"avatar"`
    Status      int       `json:"status" db:"status"`
    CreateTime  time.Time `json:"create_time" db:"create_time"`
    UpdateTime  time.Time `json:"update_time" db:"update_time"`
}

// 用户扩展信息
type UserProfile struct {
    UserID      int64  `json:"user_id" db:"user_id"`
    Nickname    string `json:"nickname" db:"nickname"`
    Gender      int    `json:"gender" db:"gender"`
    Birthday    string `json:"birthday" db:"birthday"`
    Location    string `json:"location" db:"location"`
    Bio         string `json:"bio" db:"bio"`
    FollowCount int64  `json:"follow_count" db:"follow_count"`
    FansCount   int64  `json:"fans_count" db:"fans_count"`
}
```

#### 分库分表策略
```sql
-- 用户基础信息表（按user_id分片）
CREATE TABLE user_info_0 (
    user_id BIGINT PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE,
    phone VARCHAR(20) UNIQUE,
    password_hash VARCHAR(128) NOT NULL,
    status TINYINT DEFAULT 1,
    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_username (username),
    INDEX idx_email (email),
    INDEX idx_phone (phone)
) ENGINE=InnoDB;

-- 分片规则：user_id % 256
-- 总共256个分片，分布在32个数据库实例上
```

#### 用户服务实现
```go
type UserService struct {
    userRepo    *UserRepository
    cache       *redis.Client
    idGenerator *snowflake.Node
}

// 用户注册
func (s *UserService) Register(ctx context.Context, req *RegisterRequest) (*User, error) {
    // 1. 参数验证
    if err := s.validateRegisterRequest(req); err != nil {
        return nil, err
    }
    
    // 2. 检查用户名/邮箱是否已存在
    exists, err := s.userRepo.CheckUserExists(ctx, req.Username, req.Email)
    if err != nil {
        return nil, err
    }
    if exists {
        return nil, errors.New("用户名或邮箱已存在")
    }
    
    // 3. 生成用户ID
    userID := s.idGenerator.Generate().Int64()
    
    // 4. 密码加密
    passwordHash, err := bcrypt.GenerateFromPassword([]byte(req.Password), bcrypt.DefaultCost)
    if err != nil {
        return nil, err
    }
    
    // 5. 创建用户
    user := &User{
        UserID:     userID,
        Username:   req.Username,
        Email:      req.Email,
        Phone:      req.Phone,
        Status:     1,
        CreateTime: time.Now(),
        UpdateTime: time.Now(),
    }
    
    // 6. 数据库事务
    tx, err := s.userRepo.BeginTx(ctx)
    if err != nil {
        return nil, err
    }
    defer tx.Rollback()
    
    // 插入用户基础信息
    if err := s.userRepo.CreateUser(ctx, tx, user); err != nil {
        return nil, err
    }
    
    // 插入用户扩展信息
    profile := &UserProfile{
        UserID:      userID,
        Nickname:    req.Username,
        FollowCount: 0,
        FansCount:   0,
    }
    if err := s.userRepo.CreateUserProfile(ctx, tx, profile); err != nil {
        return nil, err
    }
    
    if err := tx.Commit(); err != nil {
        return nil, err
    }
    
    // 7. 缓存用户信息
    s.cacheUser(ctx, user)
    
    return user, nil
}

// 获取用户信息（多级缓存）
func (s *UserService) GetUser(ctx context.Context, userID int64) (*User, error) {
    // L1缓存：本地缓存
    if user := s.getFromLocalCache(userID); user != nil {
        return user, nil
    }
    
    // L2缓存：Redis缓存
    if user := s.getFromRedisCache(ctx, userID); user != nil {
        s.setToLocalCache(userID, user)
        return user, nil
    }
    
    // L3：数据库查询
    user, err := s.userRepo.GetUserByID(ctx, userID)
    if err != nil {
        return nil, err
    }
    
    // 回写缓存
    s.setToRedisCache(ctx, userID, user)
    s.setToLocalCache(userID, user)
    
    return user, nil
}
```

### 3.2 关系服务架构

#### 关系数据模型
```go
// 关注关系
type Follow struct {
    ID         int64     `json:"id" db:"id"`
    FollowerID int64     `json:"follower_id" db:"follower_id"` // 关注者
    FolloweeID int64     `json:"followee_id" db:"followee_id"` // 被关注者
    Status     int       `json:"status" db:"status"`           // 1:关注 0:取消关注
    CreateTime time.Time `json:"create_time" db:"create_time"`
    UpdateTime time.Time `json:"update_time" db:"update_time"`
}

// 好友关系
type Friendship struct {
    ID         int64     `json:"id" db:"id"`
    UserID1    int64     `json:"user_id1" db:"user_id1"`
    UserID2    int64     `json:"user_id2" db:"user_id2"`
    Status     int       `json:"status" db:"status"` // 1:好友 2:拉黑
    CreateTime time.Time `json:"create_time" db:"create_time"`
    UpdateTime time.Time `json:"update_time" db:"update_time"`
}
```

#### 关系存储优化
```sql
-- 关注关系表（双向存储）
CREATE TABLE follow_relation_0 (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    follower_id BIGINT NOT NULL,
    followee_id BIGINT NOT NULL,
    status TINYINT DEFAULT 1,
    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    UNIQUE KEY uk_follow (follower_id, followee_id),
    INDEX idx_follower (follower_id, status),
    INDEX idx_followee (followee_id, status)
) ENGINE=InnoDB;

-- 分片策略：follower_id % 64
-- 同时维护反向索引表用于快速查询粉丝列表
```

#### 关系服务实现
```go
type RelationService struct {
    relationRepo *RelationRepository
    cache        *redis.Client
    mq           *kafka.Producer
}

// 关注用户
func (s *RelationService) Follow(ctx context.Context, followerID, followeeID int64) error {
    // 1. 参数验证
    if followerID == followeeID {
        return errors.New("不能关注自己")
    }
    
    // 2. 检查是否已关注
    exists, err := s.relationRepo.IsFollowing(ctx, followerID, followeeID)
    if err != nil {
        return err
    }
    if exists {
        return errors.New("已经关注过了")
    }
    
    // 3. 创建关注关系
    follow := &Follow{
        FollowerID: followerID,
        FolloweeID: followeeID,
        Status:     1,
        CreateTime: time.Now(),
        UpdateTime: time.Now(),
    }
    
    // 4. 数据库事务
    tx, err := s.relationRepo.BeginTx(ctx)
    if err != nil {
        return err
    }
    defer tx.Rollback()
    
    // 插入关注关系
    if err := s.relationRepo.CreateFollow(ctx, tx, follow); err != nil {
        return err
    }
    
    // 更新用户关注数和粉丝数
    if err := s.relationRepo.IncrFollowCount(ctx, tx, followerID); err != nil {
        return err
    }
    if err := s.relationRepo.IncrFansCount(ctx, tx, followeeID); err != nil {
        return err
    }
    
    if err := tx.Commit(); err != nil {
        return err
    }
    
    // 5. 异步处理
    go func() {
        // 清除相关缓存
        s.clearRelationCache(followerID, followeeID)
        
        // 发送关注事件到消息队列
        event := &FollowEvent{
            FollowerID: followerID,
            FolloweeID: followeeID,
            Action:     "follow",
            Timestamp:  time.Now().Unix(),
        }
        s.mq.SendMessage("follow_events", event)
    }()
    
    return nil
}

// 获取关注列表（分页）
func (s *RelationService) GetFollowList(ctx context.Context, userID int64, offset, limit int) ([]*User, error) {
    // 1. 从缓存获取关注ID列表
    cacheKey := fmt.Sprintf("follow_list:%d", userID)
    followIDs, err := s.cache.ZRevRange(ctx, cacheKey, int64(offset), int64(offset+limit-1)).Result()
    if err == nil && len(followIDs) > 0 {
        // 批量获取用户信息
        return s.batchGetUsers(ctx, followIDs)
    }
    
    // 2. 从数据库查询
    follows, err := s.relationRepo.GetFollowList(ctx, userID, offset, limit)
    if err != nil {
        return nil, err
    }
    
    // 3. 获取用户详细信息
    var userIDs []int64
    for _, follow := range follows {
        userIDs = append(userIDs, follow.FolloweeID)
    }
    
    users, err := s.batchGetUsers(ctx, userIDs)
    if err != nil {
        return nil, err
    }
    
    // 4. 异步更新缓存
    go s.updateFollowListCache(userID, follows)
    
    return users, nil
}
```

### 3.3 内容服务架构

#### 内容数据模型
```go
// 动态内容
type Post struct {
    PostID      int64     `json:"post_id" db:"post_id"`
    UserID      int64     `json:"user_id" db:"user_id"`
    Content     string    `json:"content" db:"content"`
    MediaURLs   string    `json:"media_urls" db:"media_urls"` // JSON格式存储
    Location    string    `json:"location" db:"location"`
    Privacy     int       `json:"privacy" db:"privacy"`       // 1:公开 2:好友可见 3:私密
    Status      int       `json:"status" db:"status"`         // 1:正常 2:删除 3:审核中
    LikeCount   int64     `json:"like_count" db:"like_count"`
    CommentCount int64    `json:"comment_count" db:"comment_count"`
    ShareCount  int64     `json:"share_count" db:"share_count"`
    CreateTime  time.Time `json:"create_time" db:"create_time"`
    UpdateTime  time.Time `json:"update_time" db:"update_time"`
}

// 点赞记录
type Like struct {
    ID         int64     `json:"id" db:"id"`
    PostID     int64     `json:"post_id" db:"post_id"`
    UserID     int64     `json:"user_id" db:"user_id"`
    Status     int       `json:"status" db:"status"` // 1:点赞 0:取消点赞
    CreateTime time.Time `json:"create_time" db:"create_time"`
    UpdateTime time.Time `json:"update_time" db:"update_time"`
}

// 评论
type Comment struct {
    CommentID  int64     `json:"comment_id" db:"comment_id"`
    PostID     int64     `json:"post_id" db:"post_id"`
    UserID     int64     `json:"user_id" db:"user_id"`
    ParentID   int64     `json:"parent_id" db:"parent_id"` // 回复的评论ID
    Content    string    `json:"content" db:"content"`
    Status     int       `json:"status" db:"status"`
    LikeCount  int64     `json:"like_count" db:"like_count"`
    CreateTime time.Time `json:"create_time" db:"create_time"`
    UpdateTime time.Time `json:"update_time" db:"update_time"`
}
```

#### 内容存储策略
```sql
-- 动态内容表（按时间分片）
CREATE TABLE post_202312 (
    post_id BIGINT PRIMARY KEY,
    user_id BIGINT NOT NULL,
    content TEXT,
    media_urls JSON,
    location VARCHAR(100),
    privacy TINYINT DEFAULT 1,
    status TINYINT DEFAULT 1,
    like_count BIGINT DEFAULT 0,
    comment_count BIGINT DEFAULT 0,
    share_count BIGINT DEFAULT 0,
    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_user_time (user_id, create_time DESC),
    INDEX idx_create_time (create_time DESC)
) ENGINE=InnoDB;

-- 按月分表，便于数据归档和查询优化
```

#### 内容服务实现
```go
type PostService struct {
    postRepo     *PostRepository
    cache        *redis.Client
    es           *elasticsearch.Client
    mq           *kafka.Producer
    idGenerator  *snowflake.Node
}

// 发布动态
func (s *PostService) CreatePost(ctx context.Context, req *CreatePostRequest) (*Post, error) {
    // 1. 内容审核
    if err := s.contentAudit(req.Content); err != nil {
        return nil, err
    }
    
    // 2. 生成动态ID
    postID := s.idGenerator.Generate().Int64()
    
    // 3. 处理媒体文件
    mediaURLs, err := s.processMediaFiles(req.MediaFiles)
    if err != nil {
        return nil, err
    }
    
    // 4. 创建动态
    post := &Post{
        PostID:    postID,
        UserID:    req.UserID,
        Content:   req.Content,
        MediaURLs: mediaURLs,
        Location:  req.Location,
        Privacy:   req.Privacy,
        Status:    1,
        CreateTime: time.Now(),
        UpdateTime: time.Now(),
    }
    
    // 5. 保存到数据库
    if err := s.postRepo.CreatePost(ctx, post); err != nil {
        return nil, err
    }
    
    // 6. 异步处理
    go func() {
        // 索引到ES
        s.indexToES(post)
        
        // 推送到时间线
        s.pushToTimeline(post)
        
        // 发送动态事件
        event := &PostEvent{
            PostID:    postID,
            UserID:    req.UserID,
            Action:    "create",
            Timestamp: time.Now().Unix(),
        }
        s.mq.SendMessage("post_events", event)
    }()
    
    return post, nil
}

// 获取时间线（Feed流）
func (s *PostService) GetTimeline(ctx context.Context, userID int64, lastPostID int64, limit int) ([]*Post, error) {
    // 1. 获取关注列表
    followList, err := s.getFollowList(ctx, userID)
    if err != nil {
        return nil, err
    }
    
    // 2. 构建时间线缓存key
    cacheKey := fmt.Sprintf("timeline:%d", userID)
    
    // 3. 从Redis获取时间线
    var posts []*Post
    if lastPostID == 0 {
        // 首次加载，从缓存获取
        posts, err = s.getTimelineFromCache(ctx, cacheKey, limit)
        if err == nil && len(posts) > 0 {
            return posts, nil
        }
    }
    
    // 4. 实时计算时间线
    posts, err = s.calculateTimeline(ctx, followList, lastPostID, limit)
    if err != nil {
        return nil, err
    }
    
    // 5. 异步更新缓存
    if lastPostID == 0 {
        go s.updateTimelineCache(cacheKey, posts)
    }
    
    return posts, nil
}

// 实时计算时间线
func (s *PostService) calculateTimeline(ctx context.Context, followList []int64, lastPostID int64, limit int) ([]*Post, error) {
    // 使用归并排序算法合并多个用户的动态
    var allPosts []*Post
    
    // 并发获取每个关注用户的最新动态
    ch := make(chan []*Post, len(followList))
    for _, userID := range followList {
        go func(uid int64) {
            posts, _ := s.postRepo.GetUserPosts(ctx, uid, lastPostID, limit)
            ch <- posts
        }(userID)
    }
    
    // 收集结果
    for i := 0; i < len(followList); i++ {
        posts := <-ch
        allPosts = append(allPosts, posts...)
    }
    
    // 按时间排序
    sort.Slice(allPosts, func(i, j int) bool {
        return allPosts[i].CreateTime.After(allPosts[j].CreateTime)
    })
    
    // 截取指定数量
    if len(allPosts) > limit {
        allPosts = allPosts[:limit]
    }
    
    return allPosts, nil
}
```

---

## 4. 性能优化策略

### 4.1 缓存策略

#### 多级缓存架构
```go
type CacheManager struct {
    l1Cache *bigcache.BigCache  // 本地缓存
    l2Cache *redis.Client      // Redis缓存
    l3Cache *memcached.Client  // Memcached缓存
}

// 缓存读取策略
func (c *CacheManager) Get(key string) (interface{}, error) {
    // L1: 本地缓存（最快）
    if data, err := c.l1Cache.Get(key); err == nil {
        return data, nil
    }
    
    // L2: Redis缓存（中等速度）
    if data, err := c.l2Cache.Get(context.Background(), key).Result(); err == nil {
        // 回写L1缓存
        c.l1Cache.Set(key, []byte(data))
        return data, nil
    }
    
    // L3: Memcached缓存（备用）
    if data, err := c.l3Cache.Get(key); err == nil {
        // 回写L1和L2缓存
        c.l1Cache.Set(key, data)
        c.l2Cache.Set(context.Background(), key, data, time.Hour)
        return data, nil
    }
    
    return nil, errors.New("cache miss")
}
```

#### 缓存更新策略
```go
// 缓存一致性保证
type CacheConsistency struct {
    cache *redis.Client
    mq    *kafka.Producer
}

// 延迟双删策略
func (c *CacheConsistency) UpdateWithDelayedDoubleDelete(key string, updateFunc func() error) error {
    // 1. 删除缓存
    c.cache.Del(context.Background(), key)
    
    // 2. 更新数据库
    if err := updateFunc(); err != nil {
        return err
    }
    
    // 3. 延迟删除缓存
    time.AfterFunc(500*time.Millisecond, func() {
        c.cache.Del(context.Background(), key)
    })
    
    return nil
}

// 基于消息队列的缓存更新
func (c *CacheConsistency) UpdateWithMQ(key string, data interface{}) {
    // 发送缓存更新消息
    event := &CacheUpdateEvent{
        Key:       key,
        Data:      data,
        Action:    "update",
        Timestamp: time.Now().Unix(),
    }
    c.mq.SendMessage("cache_updates", event)
}
```

### 4.2 数据库优化

#### 读写分离
```go
type DatabaseManager struct {
    master *sql.DB
    slaves []*sql.DB
    lb     *LoadBalancer
}

// 读操作路由到从库
func (dm *DatabaseManager) Query(query string, args ...interface{}) (*sql.Rows, error) {
    slave := dm.lb.SelectSlave()
    return slave.Query(query, args...)
}

// 写操作路由到主库
func (dm *DatabaseManager) Exec(query string, args ...interface{}) (sql.Result, error) {
    return dm.master.Exec(query, args...)
}

// 负载均衡器
type LoadBalancer struct {
    slaves []*sql.DB
    index  int64
}

func (lb *LoadBalancer) SelectSlave() *sql.DB {
    // 轮询算法
    idx := atomic.AddInt64(&lb.index, 1) % int64(len(lb.slaves))
    return lb.slaves[idx]
}
```

#### 分库分表
```go
type ShardingManager struct {
    shards map[string]*sql.DB
}

// 分片路由
func (sm *ShardingManager) GetShard(userID int64) *sql.DB {
    // 按用户ID分片
    shardKey := fmt.Sprintf("shard_%d", userID%256)
    return sm.shards[shardKey]
}

// 跨分片查询
func (sm *ShardingManager) QueryAcrossShards(query string, args ...interface{}) ([]*sql.Rows, error) {
    var results []*sql.Rows
    var wg sync.WaitGroup
    var mu sync.Mutex
    
    for _, db := range sm.shards {
        wg.Add(1)
        go func(database *sql.DB) {
            defer wg.Done()
            rows, err := database.Query(query, args...)
            if err == nil {
                mu.Lock()
                results = append(results, rows)
                mu.Unlock()
            }
        }(db)
    }
    
    wg.Wait()
    return results, nil
}
```

### 4.3 异步处理

#### 消息队列架构
```go
type MessageQueue struct {
    producer *kafka.Producer
    consumer *kafka.Consumer
}

// 异步任务处理
func (mq *MessageQueue) ProcessAsync(topic string, handler func([]byte) error) {
    go func() {
        for {
            msg, err := mq.consumer.ReadMessage(-1)
            if err != nil {
                log.Printf("Consumer error: %v", err)
                continue
            }
            
            // 处理消息
            if err := handler(msg.Value); err != nil {
                log.Printf("Handler error: %v", err)
                // 重试机制
                mq.retryMessage(msg)
            }
        }
    }()
}

// 消息重试
func (mq *MessageQueue) retryMessage(msg *kafka.Message) {
    retryCount := mq.getRetryCount(msg)
    if retryCount < 3 {
        // 延迟重试
        time.AfterFunc(time.Duration(retryCount+1)*time.Second, func() {
            mq.producer.Produce(&kafka.Message{
                TopicPartition: msg.TopicPartition,
                Value:         msg.Value,
                Headers:       append(msg.Headers, kafka.Header{Key: "retry_count", Value: []byte(fmt.Sprintf("%d", retryCount+1))}),
            }, nil)
        })
    } else {
        // 发送到死信队列
        mq.sendToDeadLetterQueue(msg)
    }
}
```

---

## 5. 监控与运维

### 5.1 监控体系

#### 核心指标监控
```go
type MetricsCollector struct {
    prometheus *prometheus.Registry
    counters   map[string]prometheus.Counter
    histograms map[string]prometheus.Histogram
    gauges     map[string]prometheus.Gauge
}

// 请求计数
func (mc *MetricsCollector) IncrementRequest(endpoint string) {
    counter := mc.counters[fmt.Sprintf("requests_total_%s", endpoint)]
    counter.Inc()
}

// 响应时间统计
func (mc *MetricsCollector) RecordLatency(endpoint string, duration time.Duration) {
    histogram := mc.histograms[fmt.Sprintf("request_duration_%s", endpoint)]
    histogram.Observe(duration.Seconds())
}

// 系统资源监控
func (mc *MetricsCollector) UpdateSystemMetrics() {
    // CPU使用率
    cpuUsage := mc.getCPUUsage()
    mc.gauges["cpu_usage"].Set(cpuUsage)
    
    // 内存使用率
    memUsage := mc.getMemoryUsage()
    mc.gauges["memory_usage"].Set(memUsage)
    
    // 数据库连接数
    dbConnections := mc.getDBConnections()
    mc.gauges["db_connections"].Set(float64(dbConnections))
}
```

#### 告警系统
```go
type AlertManager struct {
    rules []AlertRule
    notifiers []Notifier
}

type AlertRule struct {
    Name      string
    Condition func(float64) bool
    Threshold float64
    Duration  time.Duration
}

// 检查告警规则
func (am *AlertManager) CheckAlerts(metrics map[string]float64) {
    for _, rule := range am.rules {
        value := metrics[rule.Name]
        if rule.Condition(value) {
            alert := &Alert{
                Name:      rule.Name,
                Value:     value,
                Threshold: rule.Threshold,
                Timestamp: time.Now(),
                Level:     am.getAlertLevel(value, rule.Threshold),
            }
            am.sendAlert(alert)
        }
    }
}

// 发送告警
func (am *AlertManager) sendAlert(alert *Alert) {
    for _, notifier := range am.notifiers {
        go notifier.Send(alert)
    }
}
```

### 5.2 容灾与高可用

#### 多地域部署
```yaml
# 部署配置
regions:
  - name: "us-east-1"
    primary: true
    datacenters:
      - "us-east-1a"
      - "us-east-1b"
      - "us-east-1c"
  - name: "us-west-2"
    primary: false
    datacenters:
      - "us-west-2a"
      - "us-west-2b"
  - name: "eu-west-1"
    primary: false
    datacenters:
      - "eu-west-1a"
      - "eu-west-1b"

# 流量分配
traffic_routing:
  - region: "us-east-1"
    weight: 50
  - region: "us-west-2"
    weight: 30
  - region: "eu-west-1"
    weight: 20
```

#### 故障转移
```go
type FailoverManager struct {
    healthChecker *HealthChecker
    loadBalancer  *LoadBalancer
    alertManager  *AlertManager
}

// 健康检查
func (fm *FailoverManager) StartHealthCheck() {
    ticker := time.NewTicker(30 * time.Second)
    go func() {
        for range ticker.C {
            fm.checkServiceHealth()
        }
    }()
}

func (fm *FailoverManager) checkServiceHealth() {
    services := fm.loadBalancer.GetServices()
    for _, service := range services {
        if !fm.healthChecker.IsHealthy(service) {
            // 从负载均衡中移除不健康的服务
            fm.loadBalancer.RemoveService(service)
            
            // 发送告警
            alert := &Alert{
                Name:    "service_unhealthy",
                Message: fmt.Sprintf("Service %s is unhealthy", service.Name),
                Level:   "critical",
            }
            fm.alertManager.SendAlert(alert)
            
            // 尝试自动恢复
            go fm.attemptRecovery(service)
        }
    }
}

// 自动恢复
func (fm *FailoverManager) attemptRecovery(service *Service) {
    maxRetries := 3
    for i := 0; i < maxRetries; i++ {
        time.Sleep(time.Duration(i+1) * time.Minute)
        
        if fm.healthChecker.IsHealthy(service) {
            // 服务恢复，重新加入负载均衡
            fm.loadBalancer.AddService(service)
            
            alert := &Alert{
                Name:    "service_recovered",
                Message: fmt.Sprintf("Service %s has recovered", service.Name),
                Level:   "info",
            }
            fm.alertManager.SendAlert(alert)
            return
        }
    }
    
    // 自动恢复失败，需要人工介入
    alert := &Alert{
        Name:    "service_recovery_failed",
        Message: fmt.Sprintf("Failed to recover service %s after %d attempts", service.Name, maxRetries),
        Level:   "critical",
    }
    fm.alertManager.SendAlert(alert)
}
```

---

## 6. 总结

### 6.1 架构优势

1. **高可扩展性**: 微服务架构支持水平扩展
2. **高可用性**: 多地域部署，故障自动转移
3. **高性能**: 多级缓存，读写分离，异步处理
4. **高并发**: 分库分表，负载均衡，连接池优化

### 6.2 技术选型

- **编程语言**: Go（高并发性能优异）
- **数据库**: MySQL（主从复制）+ Redis（缓存）+ MongoDB（日志存储）
- **消息队列**: Kafka（高吞吐量）
- **搜索引擎**: Elasticsearch（内容检索）
- **监控**: Prometheus + Grafana
- **容器化**: Docker + Kubernetes

### 6.3 性能指标

- **QPS**: 100万+
- **响应时间**: P99 < 100ms
- **可用性**: 99.99%
- **数据一致性**: 最终一致性
- **扩展性**: 支持10倍用户增长

通过以上架构设计，可以支撑亿级用户的社交平台稳定运行，满足高并发、高可用、高性能的业务需求。