# 分库分表方案设计

## 目录
- [概述与背景](#概述与背景)
- [底层原理](#底层原理)
- [技术实现](#技术实现)
- [解决的问题](#解决的问题)
- [方案设计](#方案设计)
- [MySQL实现细节](#mysql实现细节)
- [注意事项与坑点](#注意事项与坑点)
- [高频面试题](#高频面试题)
- [实战场景分析](#实战场景分析)
- [Go语言实现](#go语言实现)

## 概述与背景

### 什么是分库分表

分库分表是一种数据库水平扩展技术，通过将单一数据库的数据分散到多个数据库实例（分库）或将单一表的数据分散到多个表（分表）来解决单库单表的性能瓶颈。

### 实现背景

**业务发展驱动的技术需求：**

1. **数据量爆炸式增长**
   - 单表数据量超过千万级别
   - 存储空间不足
   - 索引效率急剧下降

2. **并发访问压力**
   - QPS/TPS超过单库承载能力
   - 连接数达到上限
   - 锁竞争激烈

3. **业务复杂度提升**
   - 多业务模块数据隔离需求
   - 不同业务的性能要求差异
   - 数据安全和合规要求

**技术演进路径：**
```
单库单表 → 读写分离 → 垂直分库 → 水平分库 → 分库分表
```

### 架构演进

**1. 从简单到复杂的演进路径：**

- **单体应用阶段：** 初期业务简单，数据量小，单库单表足以支撑。关注点在于快速迭代和功能实现。
- **读写分离阶段：** 随着读请求增多，数据库读压力增大，引入读写分离，将读请求分发到只读副本，提高并发能力。
- **垂直分库阶段：** 业务模块增多，不同模块数据耦合，通过垂直分库将不同业务的数据拆分到独立的数据库，实现业务解耦和数据隔离。
- **水平分库分表阶段：** 单一业务数据量持续增长，单表或单库性能达到瓶颈，引入水平分库分表，将数据分散到多个数据库和表，实现数据库的水平扩展。
- **分布式事务与数据一致性：** 随着分库分表的引入，跨库事务成为挑战，需要引入分布式事务解决方案（如2PC、TCC、SAGA）和最终一致性保障机制。
- **云原生与自动化运维：** 结合容器化（Docker）、容器编排（Kubernetes）和自动化运维工具，实现分库分表集群的弹性伸缩、高可用和自动化管理。

**2. 不同阶段的技术选型依据：**

- **业务需求：** 根据业务的QPS、TPS、数据量、并发量、数据一致性要求等核心指标进行技术选型。
- **团队能力：** 评估团队对特定技术的掌握程度和运维能力，选择团队熟悉且有能力驾驭的技术栈。
- **成本考量：** 综合考虑硬件成本、软件授权成本、人力成本和运维成本。
- **社区生态与成熟度：** 优先选择社区活跃、文档完善、有成功案例的成熟技术，降低风险。
- **可扩展性与可维护性：** 考虑未来业务发展，选择易于扩展和维护的架构和技术。

**3. 重构和优化的时机判断：**

- **性能瓶颈出现：** 当数据库CPU、IO、连接数等指标持续告警，或用户反馈响应时间变长时，是进行性能优化的时机。
- **业务快速增长：** 预判到业务即将迎来爆发式增长，提前进行架构升级和容量规划。
- **技术债务积累：** 当现有架构难以满足新业务需求，或维护成本过高时，考虑进行技术重构。
- **技术栈升级：** 当有更先进、更高效的技术出现，且能带来显著收益时，可以考虑技术栈升级。

**4. 技术债务的管理策略：**

- **定期评估：** 定期对系统进行技术债务评估，识别和量化技术债务。
- **优先级排序：** 根据业务影响、风险程度和修复成本，对技术债务进行优先级排序。
- **持续投入：** 将技术债务的偿还纳入日常开发计划，分配专门资源进行修复。
- **小步快跑：** 避免一次性大规模重构，采用小步迭代的方式逐步偿还债务。
- **文档化：** 详细记录技术债务的产生原因、影响和解决方案，避免重复犯错。
- **代码规范与Code Review：** 建立严格的代码规范和Code Review机制，从源头减少技术债务的产生。

### 技术选型决策矩阵

| 技术方案 | 复杂度 | 性能 | 可维护性 | 成本 | 适用场景 | 推荐指数 |
|----------|--------|------|----------|------|----------|----------|
| **MySQL分区表** | 低 | 中 | 高 | 低 | 单机扩展 | ⭐⭐⭐ |
| **读写分离** | 低 | 中 | 高 | 低 | 读多写少 | ⭐⭐⭐⭐ |
| **垂直分库** | 中 | 中 | 中 | 中 | 业务解耦 | ⭐⭐⭐⭐ |
| **水平分库分表** | 高 | 高 | 低 | 高 | 海量数据 | ⭐⭐⭐⭐⭐ |
| **分布式数据库** | 高 | 高 | 中 | 高 | 云原生 | ⭐⭐⭐⭐ |

### 技术债务管理工具（Go语言实现）

```go
package main

import (
	"encoding/json"
	"fmt"
	"log"
	"time"
)

// TechnicalDebt 技术债务结构
type TechnicalDebt struct {
	ID          string    `json:"id"`
	Title       string    `json:"title"`
	Description string    `json:"description"`
	Severity    string    `json:"severity"` // Critical, High, Medium, Low
	Component   string    `json:"component"`
	EstimatedEffort int   `json:"estimated_effort"` // 人天
	BusinessImpact  int   `json:"business_impact"`  // 1-10分
	TechnicalRisk   int   `json:"technical_risk"`   // 1-10分
	CreatedAt   time.Time `json:"created_at"`
	Status      string    `json:"status"` // Open, InProgress, Resolved
}

// DebtManager 技术债务管理器
type DebtManager struct {
	debts []TechnicalDebt
}

// AddDebt 添加技术债务
func (dm *DebtManager) AddDebt(debt TechnicalDebt) {
	debt.ID = fmt.Sprintf("DEBT-%d", time.Now().Unix())
	debt.CreatedAt = time.Now()
	debt.Status = "Open"
	dm.debts = append(dm.debts, debt)
}

// CalculatePriority 计算优先级分数
func (dm *DebtManager) CalculatePriority(debt TechnicalDebt) float64 {
	// 优先级 = (业务影响 * 0.4 + 技术风险 * 0.3 + 紧急程度 * 0.3) / 预估工作量
	severityScore := map[string]float64{
		"Critical": 10,
		"High":     8,
		"Medium":   5,
		"Low":      2,
	}
	
	urgency := severityScore[debt.Severity]
	priority := (float64(debt.BusinessImpact)*0.4 + float64(debt.TechnicalRisk)*0.3 + urgency*0.3) / float64(debt.EstimatedEffort)
	return priority
}

// GetPrioritizedDebts 获取按优先级排序的技术债务
func (dm *DebtManager) GetPrioritizedDebts() []TechnicalDebt {
	type debtWithPriority struct {
		debt     TechnicalDebt
		priority float64
	}
	
	var debtsWithPriority []debtWithPriority
	for _, debt := range dm.debts {
		if debt.Status == "Open" {
			priority := dm.CalculatePriority(debt)
			debtsWithPriority = append(debtsWithPriority, debtWithPriority{debt, priority})
		}
	}
	
	// 按优先级排序
	for i := 0; i < len(debtsWithPriority)-1; i++ {
		for j := i + 1; j < len(debtsWithPriority); j++ {
			if debtsWithPriority[i].priority < debtsWithPriority[j].priority {
				debtsWithPriority[i], debtsWithPriority[j] = debtsWithPriority[j], debtsWithPriority[i]
			}
		}
	}
	
	var result []TechnicalDebt
	for _, item := range debtsWithPriority {
		result = append(result, item.debt)
	}
	return result
}

// GenerateReport 生成技术债务报告
func (dm *DebtManager) GenerateReport() string {
	prioritizedDebts := dm.GetPrioritizedDebts()
	report := "=== 技术债务优先级报告 ===\n\n"
	
	for i, debt := range prioritizedDebts {
		priority := dm.CalculatePriority(debt)
		report += fmt.Sprintf("%d. [%s] %s\n", i+1, debt.Severity, debt.Title)
		report += fmt.Sprintf("   组件: %s\n", debt.Component)
		report += fmt.Sprintf("   优先级分数: %.2f\n", priority)
		report += fmt.Sprintf("   预估工作量: %d人天\n", debt.EstimatedEffort)
		report += fmt.Sprintf("   业务影响: %d/10\n", debt.BusinessImpact)
		report += fmt.Sprintf("   技术风险: %d/10\n\n", debt.TechnicalRisk)
	}
	
	return report
}

func main() {
	dm := &DebtManager{}
	
	// 添加示例技术债务
	dm.AddDebt(TechnicalDebt{
		Title:           "分库分表中间件升级",
		Description:     "当前使用的ShardingSphere版本过旧，存在性能问题",
		Severity:        "High",
		Component:       "数据访问层",
		EstimatedEffort: 15,
		BusinessImpact:  8,
		TechnicalRisk:   7,
	})
	
	dm.AddDebt(TechnicalDebt{
		Title:           "跨分片查询优化",
		Description:     "当前跨分片聚合查询性能较差，需要引入缓存",
		Severity:        "Medium",
		Component:       "查询引擎",
		EstimatedEffort: 8,
		BusinessImpact:  6,
		TechnicalRisk:   5,
	})
	
	dm.AddDebt(TechnicalDebt{
		Title:           "分片扩容自动化",
		Description:     "当前分片扩容需要手动操作，存在风险",
		Severity:        "Critical",
		Component:       "运维工具",
		EstimatedEffort: 20,
		BusinessImpact:  9,
		TechnicalRisk:   8,
	})
	
	// 生成报告
	report := dm.GenerateReport()
	fmt.Println(report)
	
	// 输出JSON格式
	jsonData, _ := json.MarshalIndent(dm.GetPrioritizedDebts(), "", "  ")
	log.Printf("技术债务JSON: %s", string(jsonData))
}
```

### API版本管理策略

```go
package main

import (
	"fmt"
	"net/http"
	"strconv"
	"strings"
)

// APIVersion API版本信息
type APIVersion struct {
	Version     string
	Deprecated  bool
	SunsetDate  string
	Description string
}

// VersionManager 版本管理器
type VersionManager struct {
	supportedVersions map[string]APIVersion
	defaultVersion    string
}

// NewVersionManager 创建版本管理器
func NewVersionManager() *VersionManager {
	return &VersionManager{
		supportedVersions: map[string]APIVersion{
			"v1": {Version: "v1", Deprecated: true, SunsetDate: "2024-12-31", Description: "Legacy API"},
			"v2": {Version: "v2", Deprecated: false, Description: "Current stable API"},
			"v3": {Version: "v3", Deprecated: false, Description: "Latest API with enhanced features"},
		},
		defaultVersion: "v2",
	}
}

// ExtractVersion 从请求中提取版本信息
func (vm *VersionManager) ExtractVersion(r *http.Request) string {
	// 1. 从Header中获取
	if version := r.Header.Get("API-Version"); version != "" {
		return version
	}
	
	// 2. 从URL路径中获取
	path := r.URL.Path
	parts := strings.Split(path, "/")
	for _, part := range parts {
		if strings.HasPrefix(part, "v") {
			if _, err := strconv.Atoi(part[1:]); err == nil {
				return part
			}
		}
	}
	
	// 3. 从查询参数中获取
	if version := r.URL.Query().Get("version"); version != "" {
		return version
	}
	
	// 4. 返回默认版本
	return vm.defaultVersion
}

// ValidateVersion 验证版本是否支持
func (vm *VersionManager) ValidateVersion(version string) (APIVersion, error) {
	apiVersion, exists := vm.supportedVersions[version]
	if !exists {
		return APIVersion{}, fmt.Errorf("unsupported API version: %s", version)
	}
	return apiVersion, nil
}

// VersionMiddleware 版本管理中间件
func (vm *VersionManager) VersionMiddleware(next http.HandlerFunc) http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {
		version := vm.ExtractVersion(r)
		apiVersion, err := vm.ValidateVersion(version)
		
		if err != nil {
			http.Error(w, err.Error(), http.StatusBadRequest)
			return
		}
		
		// 设置版本信息到上下文
		w.Header().Set("API-Version", apiVersion.Version)
		
		// 如果版本已废弃，添加警告头
		if apiVersion.Deprecated {
			w.Header().Set("Warning", fmt.Sprintf("299 - \"API version %s is deprecated. Sunset date: %s\"", 
				apiVersion.Version, apiVersion.SunsetDate))
		}
		
		next.ServeHTTP(w, r)
	}
}

func main() {
	vm := NewVersionManager()
	
	// 示例API处理器
	handler := func(w http.ResponseWriter, r *http.Request) {
		version := vm.ExtractVersion(r)
		fmt.Fprintf(w, "Hello from API %s!", version)
	}
	
	// 注册路由
	http.HandleFunc("/api/", vm.VersionMiddleware(handler))
	http.HandleFunc("/api/v1/", vm.VersionMiddleware(handler))
	http.HandleFunc("/api/v2/", vm.VersionMiddleware(handler))
	http.HandleFunc("/api/v3/", vm.VersionMiddleware(handler))
	
	fmt.Println("Server starting on :8080")
	http.ListenAndServe(":8080", nil)
}
```

## 底层原理

### 1. 分库原理

**垂直分库（按业务模块）：**
```
原始架构：
┌─────────────────┐
│   单一数据库    │
│  ┌─────────────┐│
│  │ 用户表      ││
│  │ 订单表      ││
│  │ 商品表      ││
│  │ 支付表      ││
│  └─────────────┘│
└─────────────────┘

垂直分库后：
┌─────────────┐ ┌─────────────┐ ┌─────────────┐
│  用户数据库  │ │  订单数据库  │ │  商品数据库  │
│ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
│ │ 用户表  │ │ │ │ 订单表  │ │ │ │ 商品表  │ │
│ │ 用户详情│ │ │ │ 订单项  │ │ │ │ 商品详情│ │
│ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
└─────────────┘ └─────────────┘ └─────────────┘
```

**水平分库（按数据特征）：**
```
按用户ID分库：
┌─────────────┐ ┌─────────────┐ ┌─────────────┐
│   DB_0      │ │   DB_1      │ │   DB_2      │
│ user_id%3=0 │ │ user_id%3=1 │ │ user_id%3=2 │
│ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
│ │ 用户表  │ │ │ │ 用户表  │ │ │ │ 用户表  │ │
│ │ 订单表  │ │ │ │ 订单表  │ │ │ │ 订单表  │ │
│ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
└─────────────┘ └─────────────┘ └─────────────┘
```

### 2. 分表原理

**水平分表：**
```sql
-- 原始表
CREATE TABLE orders (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    order_time DATETIME,
    amount DECIMAL(10,2)
);

-- 分表后
CREATE TABLE orders_0 (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    order_time DATETIME,
    amount DECIMAL(10,2)
);

CREATE TABLE orders_1 (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    order_time DATETIME,
    amount DECIMAL(10,2)
);
```

**垂直分表：**
```sql
-- 原始表（字段过多）
CREATE TABLE user_info (
    id BIGINT PRIMARY KEY,
    username VARCHAR(50),
    email VARCHAR(100),
    phone VARCHAR(20),
    -- 基础信息
    real_name VARCHAR(50),
    id_card VARCHAR(18),
    -- 扩展信息（大字段）
    avatar LONGTEXT,
    description TEXT,
    preferences JSON
);

-- 垂直分表后
CREATE TABLE user_basic (
    id BIGINT PRIMARY KEY,
    username VARCHAR(50),
    email VARCHAR(100),
    phone VARCHAR(20),
    real_name VARCHAR(50),
    id_card VARCHAR(18)
);

CREATE TABLE user_profile (
    user_id BIGINT PRIMARY KEY,
    avatar LONGTEXT,
    description TEXT,
    preferences JSON,
    FOREIGN KEY (user_id) REFERENCES user_basic(id)
);
```

### 3. 路由算法

**哈希取模算法：**
```go
func HashRoute(shardKey string, shardCount int) int {
    hash := crc32.ChecksumIEEE([]byte(shardKey))
    return int(hash) % shardCount
}

// 示例：用户ID路由
userID := "12345"
dbIndex := HashRoute(userID, 4) // 分4个库
tableIndex := HashRoute(userID, 8) // 每库8张表
```

**范围路由算法：**
```go
func RangeRoute(value int64, ranges []Range) int {
    for i, r := range ranges {
        if value >= r.Start && value < r.End {
            return i
        }
    }
    return -1
}

// 示例：按时间范围路由
type Range struct {
    Start int64
    End   int64
}

ranges := []Range{
    {Start: 0, End: 202401},      // 2024年1月
    {Start: 202401, End: 202402}, // 2024年2月
    {Start: 202402, End: 202403}, // 2024年3月
}
```

**一致性哈希算法：**
```go
type ConsistentHash struct {
    ring     map[uint32]string
    sortedKeys []uint32
    replicas int
}

func (ch *ConsistentHash) Add(nodes ...string) {
    for _, node := range nodes {
        for i := 0; i < ch.replicas; i++ {
            key := ch.hash(fmt.Sprintf("%s:%d", node, i))
            ch.ring[key] = node
            ch.sortedKeys = append(ch.sortedKeys, key)
        }
    }
    sort.Slice(ch.sortedKeys, func(i, j int) bool {
        return ch.sortedKeys[i] < ch.sortedKeys[j]
    })
}

func (ch *ConsistentHash) Get(key string) string {
    if len(ch.ring) == 0 {
        return ""
    }
    
    hash := ch.hash(key)
    idx := sort.Search(len(ch.sortedKeys), func(i int) bool {
        return ch.sortedKeys[i] >= hash
    })
    
    if idx == len(ch.sortedKeys) {
        idx = 0
    }
    
    return ch.ring[ch.sortedKeys[idx]]
}
```

## 技术实现

### 1. 中间件架构

**代理层架构：**
```
┌─────────────┐
│ 应用层      │
└─────────────┘
       │
┌─────────────┐
│ 分库分表    │
│ 中间件      │ ← SQL解析、路由、聚合
└─────────────┘
       │
┌─────────────┐
│ 数据库集群  │
└─────────────┘
```

**主流中间件对比：**

| 中间件 | 类型 | 语言 | 特点 | 适用场景 |
|--------|------|------|------|----------|
| **ShardingSphere** | 客户端 | Java | 功能全面、生态完善 | 企业级应用 |
| **MyCat** | 代理 | Java | 独立部署、透明化 | 中大型系统 |
| **Vitess** | 代理 | Go | 云原生、高性能 | 大规模分布式 |
| **TDDL** | 客户端 | Java | 阿里开源、稳定 | 电商场景 |
| **Cobar** | 代理 | Java | 轻量级、简单 | 中小型系统 |

### 2. 分片策略

**按业务维度分片：**
```yaml
# 配置示例
sharding:
  databases:
    user_db:
      shardingColumn: user_id
      algorithmExpression: user_db_${user_id % 4}
    order_db:
      shardingColumn: user_id
      algorithmExpression: order_db_${user_id % 4}
  
  tables:
    orders:
      actualDataNodes: order_db_${0..3}.orders_${0..7}
      databaseStrategy:
        shardingColumn: user_id
        algorithmExpression: order_db_${user_id % 4}
      tableStrategy:
        shardingColumn: order_id
        algorithmExpression: orders_${order_id % 8}
```

**按时间维度分片：**
```sql
-- 按月分表
CREATE TABLE orders_202401 (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    order_time DATETIME,
    amount DECIMAL(10,2),
    INDEX idx_user_time (user_id, order_time)
) PARTITION BY RANGE (YEAR(order_time)*100 + MONTH(order_time)) (
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    PARTITION p202403 VALUES LESS THAN (202404)
);
```

### 3. 实战案例

**案例一：电商订单系统分库分表实践**

**业务场景：** 某电商平台，日订单量千万级，用户量过亿。订单数据增长迅速，单库已无法支撑高并发写入和查询。

**技术方案：**
- **分片键：** 订单表以 `user_id` 作为分库键，以 `order_id` 作为分表键。用户表以 `user_id` 作为分库键。
- **分片策略：** 采用哈希取模，`user_id % 64` 分64个库，每个库内 `order_id % 16` 分16张表。
- **中间件：** 选用 ShardingSphere 作为数据分片中间件，以客户端模式集成到业务服务中。

**Go语言实现示例（简化版）：**
```go
package main

import (
	"database/sql"
	"fmt"
	"log"
	"strconv"
	"time"

	_ "github.com/go-sql-driver/mysql"
)

// Order represents a simplified order struct
type Order struct {
	ID        int64
	UserID    int64
	Amount    float64
	Status    int
	CreatedAt time.Time
}

// ShardConfig holds sharding configuration
type ShardConfig struct {
	DBCount    int
	TableCount int
	DSNs       map[string]string
}

// NewShardConfig creates a new ShardConfig
func NewShardConfig() *ShardConfig {
	// In a real application, DSNs would be loaded from config files or a discovery service
	dsns := make(map[string]string)
	for i := 0; i < 64; i++ {
		dsns[fmt.Sprintf("db_%d", i)] = fmt.Sprintf("root:password@tcp(127.0.0.1:330%d)/shard_db_%d?parseTime=true", i, i)
	}
	return &ShardConfig{
		DBCount:    64,
		TableCount: 16,
		DSNs:       dsns,
	}
}

// GetDBConnection gets a database connection for a given user ID
func (sc *ShardConfig) GetDBConnection(userID int64) (*sql.DB, error) {
	dbIndex := userID % int64(sc.DBCount)
	dbName := fmt.Sprintf("db_%d", dbIndex)
	dsn, ok := sc.DSNs[dbName]
	if !ok {
		return nil, fmt.Errorf("DSN not found for db: %s", dbName)
	}
	// In a real application, connection pools would be managed globally
	db, err := sql.Open("mysql", dsn)
	if err != nil {
		return nil, fmt.Errorf("failed to open database connection: %w", err)
	}
	db.SetMaxOpenConns(100)
	db.SetMaxIdleConns(10)
	db.SetConnMaxLifetime(5 * time.Minute)
	return db, nil
}

// GetTableName gets the sharded table name for a given order ID
func (sc *ShardConfig) GetTableName(orderID int64) string {
	tableIndex := orderID % int64(sc.TableCount)
	return fmt.Sprintf("orders_%d", tableIndex)
}

// CreateOrder inserts a new order into the sharded database
func (sc *ShardConfig) CreateOrder(order *Order) error {
	db, err := sc.GetDBConnection(order.UserID)
	if err != nil {
		return err
	}
	defer db.Close() // In real app, manage connection pool properly

	tableName := sc.GetTableName(order.ID)
	query := fmt.Sprintf("INSERT INTO %s (id, user_id, amount, status, created_at) VALUES (?, ?, ?, ?, ?)", tableName)

	stmt, err := db.Prepare(query)
	if err != nil {
		return fmt.Errorf("failed to prepare statement: %w", err)
	}
	defer stmt.Close()

	_, err = stmt.Exec(order.ID, order.UserID, order.Amount, order.Status, order.CreatedAt)
	if err != nil {
		return fmt.Errorf("failed to execute insert: %w", err)
	}
	log.Printf("Order %d created in %s.%s\n", order.ID, db.Stats().DriverName, tableName)
	return nil
}

// GetOrderByID retrieves an order by its ID (requires knowing user_id for routing)
func (sc *ShardConfig) GetOrderByID(orderID, userID int64) (*Order, error) {
	db, err := sc.GetDBConnection(userID)
	if err != nil {
		return nil, err
	}
	defer db.Close()

	tableName := sc.GetTableName(orderID)
	query := fmt.Sprintf("SELECT id, user_id, amount, status, created_at FROM %s WHERE id = ? AND user_id = ?", tableName)

	row := db.QueryRow(query, orderID, userID)
	order := &Order{}
	err = row.Scan(&order.ID, &order.UserID, &order.Amount, &order.Status, &order.CreatedAt)
	if err == nil {
		log.Printf("Order %d retrieved from %s.%s\n", order.ID, db.Stats().DriverName, tableName)
	}
	return order, err
}

func main() {
	sc := NewShardConfig()

	// Simulate creating orders
	for i := 1; i <= 10; i++ {
		orderID := int64(1000 + i)
		userID := int64(i % 10) // Distribute users across shards
		order := &Order{
			ID:        orderID,
			UserID:    userID,
			Amount:    float64(100*i),
			Status:    1,
			CreatedAt: time.Now(),
		}
		err := sc.CreateOrder(order)
		if err != nil {
			log.Printf("Error creating order: %v\n", err)
		}
	}

	// Simulate retrieving an order
	retrievedOrder, err := sc.GetOrderByID(1005, 5) // Assuming order 1005 belongs to user 5
	if err != nil {
		log.Printf("Error retrieving order: %v\n", err)
	} else {
		log.Printf("Retrieved Order: %+v\n", retrievedOrder)
	}
}
```

**性能测试数据（模拟）：**
- **分片前：** 单库单表，1亿条订单数据，查询平均响应时间 800ms，写入 QPS 500。
- **分片后：** 64库16表，总计1024个物理分片，查询平均响应时间 50ms，写入 QPS 15000+。

**踩坑经验与解决方案：**
1. **跨库Join/事务问题：**
   - **问题：** 业务查询需要关联不同分库的表，或涉及跨库事务。
   - **解决方案：** 尽量避免跨库Join，通过业务层组装数据或数据冗余解决。分布式事务采用最终一致性方案（如消息队列、TCC），避免强一致性带来的性能损耗。
2. **热点数据问题：**
   - **问题：** 某些用户或特定时间段的数据访问量巨大，导致单个分片成为瓶颈。
   - **解决方案：** 针对热点数据进行特殊处理，如独立分片、读写分离、缓存预热等。分片键的选择至关重要，需确保数据分布均匀。
3. **数据迁移与扩容：**
   - **问题：** 随着数据量增长，需要进行分片扩容，数据迁移过程复杂且影响线上服务。
   - **解决方案：** 提前规划好分片数量，预留足够的扩容空间。采用平滑扩容方案，如双写、渐进式迁移，结合数据校验确保数据一致性。利用中间件的在线扩容能力。
4. **全局唯一ID生成：**
   - **问题：** 分库分表后，数据库自增ID无法保证全局唯一性。
   - **解决方案：** 引入独立ID生成服务（如Snowflake算法、UUID、Redis自增ID），保证ID的全局唯一性。
5. **复杂查询与聚合：**
   - **问题：** 跨分片的复杂查询（如聚合、排序、分页）性能低下。
   - **解决方案：** 针对复杂查询，考虑引入离线数仓（如Hadoop、Spark）或实时数仓（如ClickHouse、Elasticsearch）进行数据分析和报表生成。业务层进行结果归并和二次处理。

## 实战案例深度解析

### 案例二：某金融支付平台分库分表实践

**业务场景：** 某第三方支付平台，日交易量5000万笔，峰值QPS达到10万，涉及支付订单、账户流水、清算数据等核心业务数据。

**技术挑战：**
- 数据量：单日新增数据超过1TB
- 并发量：支付高峰期QPS超过数据库承载能力
- 一致性：涉及资金安全，要求强一致性
- 合规性：需要满足金融监管要求，数据不能丢失

**Go语言技术方案：**

```go
package main

import (
	"context"
	"database/sql"
	"fmt"
	"log"
	"sync"
	"time"

	_ "github.com/go-sql-driver/mysql"
)

// PaymentOrder 支付订单结构
type PaymentOrder struct {
	OrderID     string    `json:"order_id"`
	UserID      string    `json:"user_id"`
	MerchantID  string    `json:"merchant_id"`
	Amount      int64     `json:"amount"` // 分为单位
	Currency    string    `json:"currency"`
	Status      int       `json:"status"`
	CreatedAt   time.Time `json:"created_at"`
	UpdatedAt   time.Time `json:"updated_at"`
}

// AccountFlow 账户流水结构
type AccountFlow struct {
	FlowID      string    `json:"flow_id"`
	AccountID   string    `json:"account_id"`
	OrderID     string    `json:"order_id"`
	FlowType    int       `json:"flow_type"` // 1:收入 2:支出
	Amount      int64     `json:"amount"`
	Balance     int64     `json:"balance"`
	CreatedAt   time.Time `json:"created_at"`
}

// PaymentShardingManager 支付分库分表管理器
type PaymentShardingManager struct {
	orderDBs   map[string]*sql.DB
	flowDBs    map[string]*sql.DB
	mutex      sync.RWMutex
	dbCount    int
	tableCount int
}

// NewPaymentShardingManager 创建支付分库分表管理器
func NewPaymentShardingManager(dbCount, tableCount int) *PaymentShardingManager {
	return &PaymentShardingManager{
		orderDBs:   make(map[string]*sql.DB),
		flowDBs:    make(map[string]*sql.DB),
		dbCount:    dbCount,
		tableCount: tableCount,
	}
}

// InitDatabases 初始化数据库连接
func (psm *PaymentShardingManager) InitDatabases() error {
	// 初始化订单库
	for i := 0; i < psm.dbCount; i++ {
		dsn := fmt.Sprintf("root:password@tcp(order-db-%d:3306)/payment_order_%d?parseTime=true", i, i)
		db, err := sql.Open("mysql", dsn)
		if err != nil {
			return fmt.Errorf("failed to connect to order db %d: %w", i, err)
		}
		
		// 配置连接池
		db.SetMaxOpenConns(200)
		db.SetMaxIdleConns(50)
		db.SetConnMaxLifetime(5 * time.Minute)
		
		psm.orderDBs[fmt.Sprintf("order_db_%d", i)] = db
	}
	
	// 初始化流水库
	for i := 0; i < psm.dbCount; i++ {
		dsn := fmt.Sprintf("root:password@tcp(flow-db-%d:3306)/account_flow_%d?parseTime=true", i, i)
		db, err := sql.Open("mysql", dsn)
		if err != nil {
			return fmt.Errorf("failed to connect to flow db %d: %w", i, err)
		}
		
		db.SetMaxOpenConns(200)
		db.SetMaxIdleConns(50)
		db.SetConnMaxLifetime(5 * time.Minute)
		
		psm.flowDBs[fmt.Sprintf("flow_db_%d", i)] = db
	}
	
	return nil
}

// GetOrderShardInfo 获取订单分片信息
func (psm *PaymentShardingManager) GetOrderShardInfo(userID string) (string, string) {
	hash := psm.calculateHash(userID)
	dbIndex := hash % psm.dbCount
	tableIndex := hash % psm.tableCount
	
	dbName := fmt.Sprintf("order_db_%d", dbIndex)
	tableName := fmt.Sprintf("payment_orders_%d", tableIndex)
	
	return dbName, tableName
}

// GetFlowShardInfo 获取流水分片信息
func (psm *PaymentShardingManager) GetFlowShardInfo(accountID string) (string, string) {
	hash := psm.calculateHash(accountID)
	dbIndex := hash % psm.dbCount
	tableIndex := hash % psm.tableCount
	
	dbName := fmt.Sprintf("flow_db_%d", dbIndex)
	tableName := fmt.Sprintf("account_flows_%d", tableIndex)
	
	return dbName, tableName
}

// calculateHash 计算哈希值
func (psm *PaymentShardingManager) calculateHash(key string) int {
	hash := 0
	for _, char := range key {
		hash = hash*31 + int(char)
	}
	if hash < 0 {
		hash = -hash
	}
	return hash
}

// CreatePaymentOrder 创建支付订单（分布式事务）
func (psm *PaymentShardingManager) CreatePaymentOrder(ctx context.Context, order *PaymentOrder, flow *AccountFlow) error {
	// 获取分片信息
	orderDBName, orderTableName := psm.GetOrderShardInfo(order.UserID)
	flowDBName, flowTableName := psm.GetFlowShardInfo(flow.AccountID)
	
	orderDB := psm.orderDBs[orderDBName]
	flowDB := psm.flowDBs[flowDBName]
	
	// 如果跨库，使用分布式事务
	if orderDBName != flowDBName {
		return psm.createOrderWithDistributedTx(ctx, order, flow, orderDB, orderTableName, flowDB, flowTableName)
	}
	
	// 同库事务
	tx, err := orderDB.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin transaction: %w", err)
	}
	defer tx.Rollback()
	
	// 插入订单
	orderSQL := fmt.Sprintf(`INSERT INTO %s (order_id, user_id, merchant_id, amount, currency, status, created_at, updated_at) 
							 VALUES (?, ?, ?, ?, ?, ?, ?, ?)`, orderTableName)
	_, err = tx.ExecContext(ctx, orderSQL, order.OrderID, order.UserID, order.MerchantID, 
		order.Amount, order.Currency, order.Status, order.CreatedAt, order.UpdatedAt)
	if err != nil {
		return fmt.Errorf("failed to insert order: %w", err)
	}
	
	// 插入流水
	flowSQL := fmt.Sprintf(`INSERT INTO %s (flow_id, account_id, order_id, flow_type, amount, balance, created_at) 
							 VALUES (?, ?, ?, ?, ?, ?, ?)`, flowTableName)
	_, err = tx.ExecContext(ctx, flowSQL, flow.FlowID, flow.AccountID, flow.OrderID, 
		flow.FlowType, flow.Amount, flow.Balance, flow.CreatedAt)
	if err != nil {
		return fmt.Errorf("failed to insert flow: %w", err)
	}
	
	return tx.Commit()
}

// createOrderWithDistributedTx 分布式事务处理
func (psm *PaymentShardingManager) createOrderWithDistributedTx(ctx context.Context, order *PaymentOrder, flow *AccountFlow, 
	orderDB *sql.DB, orderTableName string, flowDB *sql.DB, flowTableName string) error {
	
	// 简化的2PC实现（生产环境建议使用成熟的分布式事务框架）
	orderTx, err := orderDB.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin order transaction: %w", err)
	}
	defer orderTx.Rollback()
	
	flowTx, err := flowDB.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin flow transaction: %w", err)
	}
	defer flowTx.Rollback()
	
	// Phase 1: Prepare
	orderSQL := fmt.Sprintf(`INSERT INTO %s (order_id, user_id, merchant_id, amount, currency, status, created_at, updated_at) 
							 VALUES (?, ?, ?, ?, ?, ?, ?, ?)`, orderTableName)
	_, err = orderTx.ExecContext(ctx, orderSQL, order.OrderID, order.UserID, order.MerchantID, 
		order.Amount, order.Currency, order.Status, order.CreatedAt, order.UpdatedAt)
	if err != nil {
		return fmt.Errorf("failed to prepare order: %w", err)
	}
	
	flowSQL := fmt.Sprintf(`INSERT INTO %s (flow_id, account_id, order_id, flow_type, amount, balance, created_at) 
							 VALUES (?, ?, ?, ?, ?, ?, ?)`, flowTableName)
	_, err = flowTx.ExecContext(ctx, flowSQL, flow.FlowID, flow.AccountID, flow.OrderID, 
		flow.FlowType, flow.Amount, flow.Balance, flow.CreatedAt)
	if err != nil {
		return fmt.Errorf("failed to prepare flow: %w", err)
	}
	
	// Phase 2: Commit
	if err := orderTx.Commit(); err != nil {
		return fmt.Errorf("failed to commit order transaction: %w", err)
	}
	
	if err := flowTx.Commit(); err != nil {
		// 这里需要补偿机制，回滚已提交的订单
		log.Printf("Flow transaction failed, need compensation for order %s", order.OrderID)
		return fmt.Errorf("failed to commit flow transaction: %w", err)
	}
	
	return nil
}

// QueryOrdersByUser 查询用户订单（带分页）
func (psm *PaymentShardingManager) QueryOrdersByUser(ctx context.Context, userID string, limit, offset int) ([]*PaymentOrder, error) {
	dbName, tableName := psm.GetOrderShardInfo(userID)
	db := psm.orderDBs[dbName]
	
	query := fmt.Sprintf(`SELECT order_id, user_id, merchant_id, amount, currency, status, created_at, updated_at 
							 FROM %s WHERE user_id = ? ORDER BY created_at DESC LIMIT ? OFFSET ?`, tableName)
	
	rows, err := db.QueryContext(ctx, query, userID, limit, offset)
	if err != nil {
		return nil, fmt.Errorf("failed to query orders: %w", err)
	}
	defer rows.Close()
	
	var orders []*PaymentOrder
	for rows.Next() {
		order := &PaymentOrder{}
		err := rows.Scan(&order.OrderID, &order.UserID, &order.MerchantID, 
			&order.Amount, &order.Currency, &order.Status, &order.CreatedAt, &order.UpdatedAt)
		if err != nil {
			return nil, fmt.Errorf("failed to scan order: %w", err)
		}
		orders = append(orders, order)
	}
	
	return orders, nil
}

func main() {
	psm := NewPaymentShardingManager(8, 16) // 8个库，每库16张表
	
	if err := psm.InitDatabases(); err != nil {
		log.Fatalf("Failed to initialize databases: %v", err)
	}
	
	// 模拟创建支付订单
	order := &PaymentOrder{
		OrderID:    "ORDER_20240101_001",
		UserID:     "USER_12345",
		MerchantID: "MERCHANT_001",
		Amount:     10000, // 100.00元
		Currency:   "CNY",
		Status:     1,
		CreatedAt:  time.Now(),
		UpdatedAt:  time.Now(),
	}
	
	flow := &AccountFlow{
		FlowID:    "FLOW_20240101_001",
		AccountID: "ACCOUNT_12345",
		OrderID:   "ORDER_20240101_001",
		FlowType:  2, // 支出
		Amount:    10000,
		Balance:   90000, // 余额900.00元
		CreatedAt: time.Now(),
	}
	
	ctx := context.Background()
	err := psm.CreatePaymentOrder(ctx, order, flow)
	if err != nil {
		log.Printf("Failed to create payment order: %v", err)
	} else {
		log.Printf("Payment order created successfully: %s", order.OrderID)
	}
	
	// 查询用户订单
	orders, err := psm.QueryOrdersByUser(ctx, "USER_12345", 10, 0)
	if err != nil {
		log.Printf("Failed to query orders: %v", err)
	} else {
		log.Printf("Found %d orders for user", len(orders))
	}
}
```

**性能测试数据：**
- **分片前：** 单库单表，1000万订单数据，写入QPS 2000，查询平均响应时间 300ms
- **分片后：** 8库128表，写入QPS 25000+，查询平均响应时间 50ms
- **并发测试：** 1000并发下，99%请求响应时间小于100ms
- **数据一致性：** 分布式事务成功率99.9%，失败订单通过补偿机制处理

**踩坑经验与解决方案：**

1. **分布式事务性能问题：**
   - **问题：** 2PC分布式事务导致性能下降，锁等待时间长
   - **解决方案：** 引入TCC（Try-Confirm-Cancel）模式，将长事务拆分为多个短事务，通过业务补偿保证最终一致性

2. **热点账户问题：**
   - **问题：** 某些大商户账户访问频繁，导致单个分片成为瓶颈
   - **解决方案：** 对热点账户进行特殊处理，使用独立的高性能实例，并增加读写分离和缓存

3. **跨分片查询性能：**
   - **问题：** 商户需要查询所有用户的交易数据，跨分片聚合查询慢
   - **解决方案：** 建立数据仓库，通过ETL将分片数据同步到ClickHouse，支持复杂分析查询

4. **数据一致性监控：**
   - **问题：** 分布式环境下数据不一致难以发现和修复
   - **解决方案：** 建立数据一致性检查机制，定期对比订单和流水数据，发现不一致及时告警和修复

### 案例三：某社交平台用户关系分库分表

**业务场景：** 某社交平台，用户量5亿，关注关系数据100亿条，需要支持快速的关注/取关操作和好友推荐功能。

**技术挑战：**
- 数据规模：用户关系图数据量巨大
- 查询复杂：需要支持双向关系查询
- 实时性：关注状态变更需要实时生效
- 推荐算法：需要支持复杂的图算法计算

**Go语言技术方案：**

```go
package main

import (
	"context"
	"database/sql"
	"fmt"
	"log"
	"sync"
	"time"

	"github.com/go-redis/redis/v8"
	_ "github.com/go-sql-driver/mysql"
)

// UserRelation 用户关系结构
type UserRelation struct {
	ID         int64     `json:"id"`
	FollowerID int64     `json:"follower_id"` // 关注者ID
	FolloweeID int64     `json:"followee_id"` // 被关注者ID
	Status     int       `json:"status"`     // 1:关注 0:取关
	CreatedAt  time.Time `json:"created_at"`
	UpdatedAt  time.Time `json:"updated_at"`
}

// SocialGraphManager 社交图管理器
type SocialGraphManager struct {
	followDBs   map[string]*sql.DB // 关注关系库
	fansDBs     map[string]*sql.DB // 粉丝关系库
	redisClient *redis.Client
	mutex       sync.RWMutex
	shardCount  int
}

// NewSocialGraphManager 创建社交图管理器
func NewSocialGraphManager(shardCount int, redisAddr string) *SocialGraphManager {
	return &SocialGraphManager{
		followDBs:  make(map[string]*sql.DB),
		fansDBs:    make(map[string]*sql.DB),
		shardCount: shardCount,
		redisClient: redis.NewClient(&redis.Options{
			Addr:     redisAddr,
			Password: "",
			DB:       0,
		}),
	}
}

// InitDatabases 初始化数据库连接
func (sgm *SocialGraphManager) InitDatabases() error {
	// 初始化关注关系库（按关注者分片）
	for i := 0; i < sgm.shardCount; i++ {
		dsn := fmt.Sprintf("root:password@tcp(follow-db-%d:3306)/social_follow_%d?parseTime=true", i, i)
		db, err := sql.Open("mysql", dsn)
		if err != nil {
			return fmt.Errorf("failed to connect to follow db %d: %w", i, err)
		}
		
		db.SetMaxOpenConns(100)
		db.SetMaxIdleConns(20)
		db.SetConnMaxLifetime(5 * time.Minute)
		
		sgm.followDBs[fmt.Sprintf("follow_db_%d", i)] = db
	}
	
	// 初始化粉丝关系库（按被关注者分片）
	for i := 0; i < sgm.shardCount; i++ {
		dsn := fmt.Sprintf("root:password@tcp(fans-db-%d:3306)/social_fans_%d?parseTime=true", i, i)
		db, err := sql.Open("mysql", dsn)
		if err != nil {
			return fmt.Errorf("failed to connect to fans db %d: %w", i, err)
		}
		
		db.SetMaxOpenConns(100)
		db.SetMaxIdleConns(20)
		db.SetConnMaxLifetime(5 * time.Minute)
		
		sgm.fansDBs[fmt.Sprintf("fans_db_%d", i)] = db
	}
	
	return nil
}

// GetFollowShard 获取关注关系分片
func (sgm *SocialGraphManager) GetFollowShard(followerID int64) string {
	shardIndex := followerID % int64(sgm.shardCount)
	return fmt.Sprintf("follow_db_%d", shardIndex)
}

// GetFansShard 获取粉丝关系分片
func (sgm *SocialGraphManager) GetFansShard(followeeID int64) string {
	shardIndex := followeeID % int64(sgm.shardCount)
	return fmt.Sprintf("fans_db_%d", shardIndex)
}

// Follow 关注用户
func (sgm *SocialGraphManager) Follow(ctx context.Context, followerID, followeeID int64) error {
	if followerID == followeeID {
		return fmt.Errorf("cannot follow yourself")
	}
	
	// 双写：同时写入关注表和粉丝表
	followDB := sgm.followDBs[sgm.GetFollowShard(followerID)]
	fansDB := sgm.fansDBs[sgm.GetFansShard(followeeID)]
	
	now := time.Now()
	
	// 写入关注表
	followSQL := `INSERT INTO user_follows (follower_id, followee_id, status, created_at, updated_at) 
				  VALUES (?, ?, 1, ?, ?) ON DUPLICATE KEY UPDATE status=1, updated_at=?`
	_, err := followDB.ExecContext(ctx, followSQL, followerID, followeeID, now, now, now)
	if err != nil {
		return fmt.Errorf("failed to insert follow relation: %w", err)
	}
	
	// 写入粉丝表
	fansSQL := `INSERT INTO user_fans (followee_id, follower_id, status, created_at, updated_at) 
				 VALUES (?, ?, 1, ?, ?) ON DUPLICATE KEY UPDATE status=1, updated_at=?`
	_, err = fansDB.ExecContext(ctx, fansSQL, followeeID, followerID, now, now, now)
	if err != nil {
		// 补偿：回滚关注表
		sgm.compensateFollow(ctx, followerID, followeeID, false)
		return fmt.Errorf("failed to insert fans relation: %w", err)
	}
	
	// 更新缓存
	sgm.updateFollowCache(ctx, followerID, followeeID, true)
	
	return nil
}

// Unfollow 取消关注
func (sgm *SocialGraphManager) Unfollow(ctx context.Context, followerID, followeeID int64) error {
	followDB := sgm.followDBs[sgm.GetFollowShard(followerID)]
	fansDB := sgm.fansDBs[sgm.GetFansShard(followeeID)]
	
	now := time.Now()
	
	// 更新关注表
	followSQL := `UPDATE user_follows SET status=0, updated_at=? WHERE follower_id=? AND followee_id=?`
	_, err := followDB.ExecContext(ctx, followSQL, now, followerID, followeeID)
	if err != nil {
		return fmt.Errorf("failed to update follow relation: %w", err)
	}
	
	// 更新粉丝表
	fansSQL := `UPDATE user_fans SET status=0, updated_at=? WHERE followee_id=? AND follower_id=?`
	_, err = fansDB.ExecContext(ctx, fansSQL, now, followeeID, followerID)
	if err != nil {
		// 补偿：回滚关注表
		sgm.compensateFollow(ctx, followerID, followeeID, true)
		return fmt.Errorf("failed to update fans relation: %w", err)
	}
	
	// 更新缓存
	sgm.updateFollowCache(ctx, followerID, followeeID, false)
	
	return nil
}

// GetFollowList 获取关注列表
func (sgm *SocialGraphManager) GetFollowList(ctx context.Context, followerID int64, limit, offset int) ([]int64, error) {
	// 先从缓存获取
	cacheKey := fmt.Sprintf("follow_list:%d", followerID)
	cachedList, err := sgm.redisClient.ZRevRange(ctx, cacheKey, int64(offset), int64(offset+limit-1)).Result()
	if err == nil && len(cachedList) > 0 {
		var followeeIDs []int64
		for _, idStr := range cachedList {
			if id, err := sgm.parseInt64(idStr); err == nil {
				followeeIDs = append(followeeIDs, id)
			}
		}
		return followeeIDs, nil
	}
	
	// 从数据库获取
	followDB := sgm.followDBs[sgm.GetFollowShard(followerID)]
	query := `SELECT followee_id FROM user_follows WHERE follower_id=? AND status=1 
			  ORDER BY created_at DESC LIMIT ? OFFSET ?`
	
	rows, err := followDB.QueryContext(ctx, query, followerID, limit, offset)
	if err != nil {
		return nil, fmt.Errorf("failed to query follow list: %w", err)
	}
	defer rows.Close()
	
	var followeeIDs []int64
	for rows.Next() {
		var followeeID int64
		if err := rows.Scan(&followeeID); err != nil {
			return nil, fmt.Errorf("failed to scan followee_id: %w", err)
		}
		followeeIDs = append(followeeIDs, followeeID)
	}
	
	// 异步更新缓存
	go sgm.refreshFollowListCache(context.Background(), followerID)
	
	return followeeIDs, nil
}

// GetFansList 获取粉丝列表
func (sgm *SocialGraphManager) GetFansList(ctx context.Context, followeeID int64, limit, offset int) ([]int64, error) {
	fansDB := sgm.fansDBs[sgm.GetFansShard(followeeID)]
	query := `SELECT follower_id FROM user_fans WHERE followee_id=? AND status=1 
			  ORDER BY created_at DESC LIMIT ? OFFSET ?`
	
	rows, err := fansDB.QueryContext(ctx, query, followeeID, limit, offset)
	if err != nil {
		return nil, fmt.Errorf("failed to query fans list: %w", err)
	}
	defer rows.Close()
	
	var followerIDs []int64
	for rows.Next() {
		var followerID int64
		if err := rows.Scan(&followerID); err != nil {
			return nil, fmt.Errorf("failed to scan follower_id: %w", err)
		}
		followerIDs = append(followerIDs, followerID)
	}
	
	return followerIDs, nil
}

// IsFollowing 检查是否关注
func (sgm *SocialGraphManager) IsFollowing(ctx context.Context, followerID, followeeID int64) (bool, error) {
	// 先从缓存检查
	cacheKey := fmt.Sprintf("follow:%d:%d", followerID, followeeID)
	result, err := sgm.redisClient.Get(ctx, cacheKey).Result()
	if err == nil {
		return result == "1", nil
	}
	
	// 从数据库检查
	followDB := sgm.followDBs[sgm.GetFollowShard(followerID)]
	query := `SELECT status FROM user_follows WHERE follower_id=? AND followee_id=?`
	
	var status int
	err = followDB.QueryRowContext(ctx, query, followerID, followeeID).Scan(&status)
	if err == sql.ErrNoRows {
		// 缓存结果
		sgm.redisClient.Set(ctx, cacheKey, "0", 5*time.Minute)
		return false, nil
	}
	if err != nil {
		return false, fmt.Errorf("failed to check follow status: %w", err)
	}
	
	isFollowing := status == 1
	// 缓存结果
	if isFollowing {
		sgm.redisClient.Set(ctx, cacheKey, "1", 5*time.Minute)
	} else {
		sgm.redisClient.Set(ctx, cacheKey, "0", 5*time.Minute)
	}
	
	return isFollowing, nil
}

// 辅助方法
func (sgm *SocialGraphManager) compensateFollow(ctx context.Context, followerID, followeeID int64, revert bool) {
	followDB := sgm.followDBs[sgm.GetFollowShard(followerID)]
	status := 0
	if revert {
		status = 1
	}
	
	compensateSQL := `UPDATE user_follows SET status=?, updated_at=? WHERE follower_id=? AND followee_id=?`
	followDB.ExecContext(ctx, compensateSQL, status, time.Now(), followerID, followeeID)
}

func (sgm *SocialGraphManager) updateFollowCache(ctx context.Context, followerID, followeeID int64, isFollow bool) {
	cacheKey := fmt.Sprintf("follow:%d:%d", followerID, followeeID)
	if isFollow {
		sgm.redisClient.Set(ctx, cacheKey, "1", 5*time.Minute)
	} else {
		sgm.redisClient.Set(ctx, cacheKey, "0", 5*time.Minute)
	}
	
	// 清除关注列表缓存
	followListKey := fmt.Sprintf("follow_list:%d", followerID)
	sgm.redisClient.Del(ctx, followListKey)
}

func (sgm *SocialGraphManager) refreshFollowListCache(ctx context.Context, followerID int64) {
	// 实现关注列表缓存刷新逻辑
	// 这里简化处理，实际应该分页加载并缓存到Redis ZSet
}

func (sgm *SocialGraphManager) parseInt64(s string) (int64, error) {
	// 实现字符串转int64的逻辑
	return 0, nil
}

func main() {
	sgm := NewSocialGraphManager(16, "localhost:6379")
	
	if err := sgm.InitDatabases(); err != nil {
		log.Fatalf("Failed to initialize databases: %v", err)
	}
	
	ctx := context.Background()
	
	// 模拟关注操作
	err := sgm.Follow(ctx, 12345, 67890)
	if err != nil {
		log.Printf("Failed to follow: %v", err)
	} else {
		log.Println("Follow operation successful")
	}
	
	// 检查关注状态
	isFollowing, err := sgm.IsFollowing(ctx, 12345, 67890)
	if err != nil {
		log.Printf("Failed to check follow status: %v", err)
	} else {
		log.Printf("Is following: %v", isFollowing)
	}
	
	// 获取关注列表
	followList, err := sgm.GetFollowList(ctx, 12345, 10, 0)
	if err != nil {
		log.Printf("Failed to get follow list: %v", err)
	} else {
		log.Printf("Follow list: %v", followList)
	}
}
```

**性能测试数据：**
- **分片前：** 单表存储，1亿关系数据，关注操作QPS 500，查询响应时间 200ms
- **分片后：** 16分片，关注操作QPS 8000+，查询响应时间 20ms
- **缓存命中率：** Redis缓存命中率达到95%，热点数据响应时间 < 5ms
- **双写一致性：** 关注/粉丝表数据一致性达到99.99%

**踩坑经验与解决方案：**

1. **双写数据不一致：**
   - **问题：** 关注表和粉丝表双写时，可能出现一个成功一个失败的情况
   - **解决方案：** 引入补偿机制，失败时回滚已成功的操作；定期数据一致性检查和修复

2. **热点用户性能问题：**
   - **问题：** 大V用户粉丝数量巨大，单个分片压力过大
   - **解决方案：** 对热点用户进行特殊处理，使用独立的高性能实例，粉丝列表分页缓存

3. **缓存穿透和雪崩：**
   - **问题：** 大量不存在的关系查询导致缓存穿透，缓存同时失效导致雪崩
   - **解决方案：** 使用布隆过滤器防止穿透，缓存过期时间加随机值防止雪崩

4. **分片扩容困难：**
   - **问题：** 用户增长导致需要扩容，但关系数据迁移复杂
   - **解决方案：** 采用一致性哈希算法，预分配足够的虚拟节点，扩容时只需迁移部分数据

## 性能优化要点

### 数据库层面优化

**1. 索引策略优化**

```sql
-- 分片表索引设计
CREATE TABLE user_orders_0 (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id BIGINT NOT NULL,
    order_id VARCHAR(64) NOT NULL UNIQUE,
    merchant_id BIGINT,
    amount DECIMAL(10,2),
    status TINYINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    -- 核心索引
    INDEX idx_user_id_created (user_id, created_at DESC),
    INDEX idx_order_id (order_id),
    INDEX idx_merchant_status (merchant_id, status),
    INDEX idx_created_status (created_at, status)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

-- 分区表索引优化
CREATE TABLE payment_logs (
    id BIGINT AUTO_INCREMENT,
    transaction_id VARCHAR(64),
    user_id BIGINT,
    amount DECIMAL(10,2),
    log_time TIMESTAMP,
    PRIMARY KEY (id, log_time),
    INDEX idx_user_time (user_id, log_time),
    INDEX idx_transaction (transaction_id)
) ENGINE=InnoDB
PARTITION BY RANGE (UNIX_TIMESTAMP(log_time)) (
    PARTITION p202401 VALUES LESS THAN (UNIX_TIMESTAMP('2024-02-01')),
    PARTITION p202402 VALUES LESS THAN (UNIX_TIMESTAMP('2024-03-01')),
    PARTITION p202403 VALUES LESS THAN (UNIX_TIMESTAMP('2024-04-01')),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);
```

**2. 查询优化策略**

```go
package main

import (
	"context"
	"database/sql"
	"fmt"
	"strings"
	"sync"
	"time"
)

// QueryOptimizer 查询优化器
type QueryOptimizer struct {
	shardingManager *ShardingManager
	queryCache      map[string]interface{}
	cacheMutex      sync.RWMutex
}

// OptimizedQuery 优化查询结构
type OptimizedQuery struct {
	SQL        string
	Params     []interface{}
	ShardKeys  []string
	CacheKey   string
	CacheTTL   time.Duration
}

// NewQueryOptimizer 创建查询优化器
func NewQueryOptimizer(sm *ShardingManager) *QueryOptimizer {
	return &QueryOptimizer{
		shardingManager: sm,
		queryCache:      make(map[string]interface{}),
	}
}

// OptimizeUserOrderQuery 优化用户订单查询
func (qo *QueryOptimizer) OptimizeUserOrderQuery(userID string, startTime, endTime time.Time, limit int) *OptimizedQuery {
	// 1. 确定分片键
	shardKey := userID
	
	// 2. 构建优化的SQL
	sql := `SELECT order_id, user_id, merchant_id, amount, status, created_at 
			FROM %s 
			WHERE user_id = ? AND created_at BETWEEN ? AND ? 
			ORDER BY created_at DESC 
			LIMIT ?`
	
	// 3. 生成缓存键
	cacheKey := fmt.Sprintf("user_orders:%s:%d:%d:%d", userID, startTime.Unix(), endTime.Unix(), limit)
	
	return &OptimizedQuery{
		SQL:       sql,
		Params:    []interface{}{userID, startTime, endTime, limit},
		ShardKeys: []string{shardKey},
		CacheKey:  cacheKey,
		CacheTTL:  5 * time.Minute,
	}
}

// ExecuteOptimizedQuery 执行优化查询
func (qo *QueryOptimizer) ExecuteOptimizedQuery(ctx context.Context, query *OptimizedQuery) ([]map[string]interface{}, error) {
	// 1. 检查缓存
	if cachedResult := qo.getFromCache(query.CacheKey); cachedResult != nil {
		return cachedResult.([]map[string]interface{}), nil
	}
	
	// 2. 确定目标分片
	var results []map[string]interface{}
	for _, shardKey := range query.ShardKeys {
		db, tableName := qo.shardingManager.GetShardInfo(shardKey)
		
		// 3. 执行查询
		finalSQL := fmt.Sprintf(query.SQL, tableName)
		rows, err := db.QueryContext(ctx, finalSQL, query.Params...)
		if err != nil {
			return nil, fmt.Errorf("failed to execute query on shard %s: %w", tableName, err)
		}
		defer rows.Close()
		
		// 4. 处理结果
		shardResults, err := qo.scanRows(rows)
		if err != nil {
			return nil, err
		}
		results = append(results, shardResults...)
	}
	
	// 5. 缓存结果
	qo.setCache(query.CacheKey, results, query.CacheTTL)
	
	return results, nil
}

// BatchQuery 批量查询优化
func (qo *QueryOptimizer) BatchQuery(ctx context.Context, userIDs []string, queryTemplate string) (map[string][]map[string]interface{}, error) {
	// 按分片分组
	shardGroups := make(map[string][]string)
	for _, userID := range userIDs {
		_, tableName := qo.shardingManager.GetShardInfo(userID)
		shardGroups[tableName] = append(shardGroups[tableName], userID)
	}
	
	results := make(map[string][]map[string]interface{})
	var wg sync.WaitGroup
	var mutex sync.Mutex
	
	// 并行查询各分片
	for tableName, userIDGroup := range shardGroups {
		wg.Add(1)
		go func(table string, users []string) {
			defer wg.Done()
			
			// 构建IN查询
			placeholders := strings.Repeat("?,", len(users))
			placeholders = placeholders[:len(placeholders)-1]
			
			sql := fmt.Sprintf(queryTemplate, table, placeholders)
			params := make([]interface{}, len(users))
			for i, userID := range users {
				params[i] = userID
			}
			
			db, _ := qo.shardingManager.GetShardInfo(users[0])
			rows, err := db.QueryContext(ctx, sql, params...)
			if err != nil {
				return
			}
			defer rows.Close()
			
			shardResults, _ := qo.scanRows(rows)
			
			mutex.Lock()
			for _, userID := range users {
				results[userID] = shardResults
			}
			mutex.Unlock()
		}(tableName, userIDGroup)
	}
	
	wg.Wait()
	return results, nil
}

// 缓存相关方法
func (qo *QueryOptimizer) getFromCache(key string) interface{} {
	qo.cacheMutex.RLock()
	defer qo.cacheMutex.RUnlock()
	return qo.queryCache[key]
}

func (qo *QueryOptimizer) setCache(key string, value interface{}, ttl time.Duration) {
	qo.cacheMutex.Lock()
	defer qo.cacheMutex.Unlock()
	qo.queryCache[key] = value
	
	// 简单的TTL实现（生产环境建议使用Redis）
	go func() {
		time.Sleep(ttl)
		qo.cacheMutex.Lock()
		delete(qo.queryCache, key)
		qo.cacheMutex.Unlock()
	}()
}

func (qo *QueryOptimizer) scanRows(rows *sql.Rows) ([]map[string]interface{}, error) {
	columns, err := rows.Columns()
	if err != nil {
		return nil, err
	}
	
	var results []map[string]interface{}
	for rows.Next() {
		values := make([]interface{}, len(columns))
		valuePtrs := make([]interface{}, len(columns))
		for i := range values {
			valuePtrs[i] = &values[i]
		}
		
		if err := rows.Scan(valuePtrs...); err != nil {
			return nil, err
		}
		
		row := make(map[string]interface{})
		for i, col := range columns {
			row[col] = values[i]
		}
		results = append(results, row)
	}
	
	return results, nil
}
```

### 缓存层优化

**多级缓存架构**

```go
package main

import (
	"context"
	"encoding/json"
	"fmt"
	"sync"
	"time"

	"github.com/go-redis/redis/v8"
)

// MultiLevelCache 多级缓存
type MultiLevelCache struct {
	l1Cache     map[string]*CacheItem // 本地缓存
	l2Cache     *redis.Client         // Redis缓存
	l1Mutex     sync.RWMutex
	l1MaxSize   int
	l1TTL       time.Duration
	l2TTL       time.Duration
}

// CacheItem 缓存项
type CacheItem struct {
	Value     interface{}
	ExpireAt  time.Time
	AccessAt  time.Time
	HitCount  int64
}

// NewMultiLevelCache 创建多级缓存
func NewMultiLevelCache(redisAddr string, l1MaxSize int, l1TTL, l2TTL time.Duration) *MultiLevelCache {
	return &MultiLevelCache{
		l1Cache:   make(map[string]*CacheItem),
		l2Cache:   redis.NewClient(&redis.Options{Addr: redisAddr}),
		l1MaxSize: l1MaxSize,
		l1TTL:     l1TTL,
		l2TTL:     l2TTL,
	}
}

// Get 获取缓存数据
func (mlc *MultiLevelCache) Get(ctx context.Context, key string) (interface{}, bool) {
	// 1. 尝试从L1缓存获取
	if value, found := mlc.getFromL1(key); found {
		return value, true
	}
	
	// 2. 尝试从L2缓存获取
	if value, found := mlc.getFromL2(ctx, key); found {
		// 回写到L1缓存
		mlc.setToL1(key, value)
		return value, true
	}
	
	return nil, false
}

// Set 设置缓存数据
func (mlc *MultiLevelCache) Set(ctx context.Context, key string, value interface{}) error {
	// 同时写入L1和L2缓存
	mlc.setToL1(key, value)
	return mlc.setToL2(ctx, key, value)
}

// getFromL1 从L1缓存获取
func (mlc *MultiLevelCache) getFromL1(key string) (interface{}, bool) {
	mlc.l1Mutex.RLock()
	defer mlc.l1Mutex.RUnlock()
	
	item, exists := mlc.l1Cache[key]
	if !exists || time.Now().After(item.ExpireAt) {
		return nil, false
	}
	
	// 更新访问信息
	item.AccessAt = time.Now()
	item.HitCount++
	
	return item.Value, true
}

// setToL1 设置到L1缓存
func (mlc *MultiLevelCache) setToL1(key string, value interface{}) {
	mlc.l1Mutex.Lock()
	defer mlc.l1Mutex.Unlock()
	
	// 检查容量限制
	if len(mlc.l1Cache) >= mlc.l1MaxSize {
		mlc.evictL1()
	}
	
	mlc.l1Cache[key] = &CacheItem{
		Value:    value,
		ExpireAt: time.Now().Add(mlc.l1TTL),
		AccessAt: time.Now(),
		HitCount: 0,
	}
}

// evictL1 L1缓存淘汰策略（LRU）
func (mlc *MultiLevelCache) evictL1() {
	var oldestKey string
	var oldestTime time.Time = time.Now()
	
	for key, item := range mlc.l1Cache {
		if item.AccessAt.Before(oldestTime) {
			oldestTime = item.AccessAt
			oldestKey = key
		}
	}
	
	if oldestKey != "" {
		delete(mlc.l1Cache, oldestKey)
	}
}

// getFromL2 从L2缓存获取
func (mlc *MultiLevelCache) getFromL2(ctx context.Context, key string) (interface{}, bool) {
	result, err := mlc.l2Cache.Get(ctx, key).Result()
	if err != nil {
		return nil, false
	}
	
	var value interface{}
	if err := json.Unmarshal([]byte(result), &value); err != nil {
		return nil, false
	}
	
	return value, true
}

// setToL2 设置到L2缓存
func (mlc *MultiLevelCache) setToL2(ctx context.Context, key string, value interface{}) error {
	data, err := json.Marshal(value)
	if err != nil {
		return err
	}
	
	return mlc.l2Cache.Set(ctx, key, data, mlc.l2TTL).Err()
}

// ShardingCacheManager 分片缓存管理器
type ShardingCacheManager struct {
	caches      []*MultiLevelCache
	shardCount  int
}

// NewShardingCacheManager 创建分片缓存管理器
func NewShardingCacheManager(redisAddrs []string, shardCount int) *ShardingCacheManager {
	caches := make([]*MultiLevelCache, shardCount)
	for i := 0; i < shardCount; i++ {
		redisAddr := redisAddrs[i%len(redisAddrs)]
		caches[i] = NewMultiLevelCache(redisAddr, 1000, 5*time.Minute, 30*time.Minute)
	}
	
	return &ShardingCacheManager{
		caches:     caches,
		shardCount: shardCount,
	}
}

// GetCache 获取对应分片的缓存
func (scm *ShardingCacheManager) GetCache(shardKey string) *MultiLevelCache {
	hash := scm.calculateHash(shardKey)
	index := hash % scm.shardCount
	return scm.caches[index]
}

func (scm *ShardingCacheManager) calculateHash(key string) int {
	hash := 0
	for _, char := range key {
		hash = hash*31 + int(char)
	}
	if hash < 0 {
		hash = -hash
	}
	return hash
}

// 智能缓存预热
type CacheWarmer struct {
	cacheManager *ShardingCacheManager
	shardingMgr  *ShardingManager
}

// WarmupUserData 预热用户数据
func (cw *CacheWarmer) WarmupUserData(ctx context.Context, userIDs []string) error {
	var wg sync.WaitGroup
	errorChan := make(chan error, len(userIDs))
	
	for _, userID := range userIDs {
		wg.Add(1)
		go func(uid string) {
			defer wg.Done()
			
			// 预热用户基本信息
			if err := cw.warmupUserProfile(ctx, uid); err != nil {
				errorChan <- err
				return
			}
			
			// 预热用户订单数据
			if err := cw.warmupUserOrders(ctx, uid); err != nil {
				errorChan <- err
				return
			}
		}(userID)
	}
	
	wg.Wait()
	close(errorChan)
	
	// 检查错误
	for err := range errorChan {
		if err != nil {
			return err
		}
	}
	
	return nil
}

func (cw *CacheWarmer) warmupUserProfile(ctx context.Context, userID string) error {
	cache := cw.cacheManager.GetCache(userID)
	cacheKey := fmt.Sprintf("user_profile:%s", userID)
	
	// 检查缓存是否已存在
	if _, exists := cache.Get(ctx, cacheKey); exists {
		return nil
	}
	
	// 从数据库加载数据
	db, tableName := cw.shardingMgr.GetShardInfo(userID)
	query := fmt.Sprintf("SELECT * FROM %s WHERE user_id = ?", tableName)
	
	rows, err := db.QueryContext(ctx, query, userID)
	if err != nil {
		return err
	}
	defer rows.Close()
	
	// 处理数据并缓存
	// ... 数据处理逻辑
	
	return cache.Set(ctx, cacheKey, "user_data")
}

func (cw *CacheWarmer) warmupUserOrders(ctx context.Context, userID string) error {
	// 类似的预热逻辑
	return nil
}
```

### 连接池优化

```go
// 数据库连接池优化配置
func OptimizeDBConnections(db *sql.DB, maxConns, maxIdle int) {
	// 设置最大连接数
	db.SetMaxOpenConns(maxConns)
	
	// 设置最大空闲连接数
	db.SetMaxIdleConns(maxIdle)
	
	// 设置连接最大生存时间
	db.SetConnMaxLifetime(5 * time.Minute)
	
	// 设置连接最大空闲时间
	db.SetConnMaxIdleTime(1 * time.Minute)
}

// 连接池监控
type ConnectionPoolMonitor struct {
	dbs map[string]*sql.DB
}

func (cpm *ConnectionPoolMonitor) MonitorPools() {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()
	
	for range ticker.C {
		for name, db := range cpm.dbs {
			stats := db.Stats()
			fmt.Printf("DB %s - Open: %d, InUse: %d, Idle: %d\n", 
				name, stats.OpenConnections, stats.InUse, stats.Idle)
			
			// 告警逻辑
			if stats.InUse > stats.MaxOpenConnections*8/10 {
				fmt.Printf("WARNING: DB %s connection usage high: %d/%d\n", 
					name, stats.InUse, stats.MaxOpenConnections)
			}
		}
	}
}
```

## 生产实践经验

### 容量规划与扩容策略

**1. 容量评估模型**

```go
package main

import (
	"fmt"
	"math"
	"time"
)

// CapacityPlanner 容量规划器
type CapacityPlanner struct {
	CurrentQPS       int64   // 当前QPS
	PeakQPSRatio     float64 // 峰值QPS倍数
	GrowthRate       float64 // 年增长率
	SafetyFactor     float64 // 安全系数
	AvgRecordSize    int64   // 平均记录大小(字节)
	IndexOverhead    float64 // 索引开销比例
	ReplicationCount int     // 副本数量
}

// PlanningResult 规划结果
type PlanningResult struct {
	RecommendedShards int64   // 推荐分片数
	StoragePerShard   int64   // 每分片存储容量(GB)
	QPSPerShard       int64   // 每分片QPS
	TotalStorage      int64   // 总存储容量(GB)
	ExpansionPlan     []ExpansionPhase // 扩容计划
}

// ExpansionPhase 扩容阶段
type ExpansionPhase struct {
	Timeframe    string // 时间范围
	TargetShards int64  // 目标分片数
	TriggerQPS   int64  // 触发QPS
	Actions      []string // 扩容动作
}

// CalculateCapacity 计算容量需求
func (cp *CapacityPlanner) CalculateCapacity(timeHorizonYears int) *PlanningResult {
	// 1. 计算未来QPS需求
	futureQPS := cp.calculateFutureQPS(timeHorizonYears)
	peakQPS := int64(float64(futureQPS) * cp.PeakQPSRatio * cp.SafetyFactor)
	
	// 2. 单分片QPS容量（基于经验值）
	maxQPSPerShard := int64(5000) // MySQL单表建议最大QPS
	
	// 3. 计算分片数量
	recommendedShards := int64(math.Ceil(float64(peakQPS) / float64(maxQPSPerShard)))
	
	// 4. 计算存储需求
	totalRecords := cp.estimateRecordCount(timeHorizonYears)
	totalStorage := cp.calculateStorageRequirement(totalRecords)
	storagePerShard := totalStorage / recommendedShards
	
	// 5. 生成扩容计划
	expansionPlan := cp.generateExpansionPlan(timeHorizonYears)
	
	return &PlanningResult{
		RecommendedShards: recommendedShards,
		StoragePerShard:   storagePerShard,
		QPSPerShard:       peakQPS / recommendedShards,
		TotalStorage:      totalStorage,
		ExpansionPlan:     expansionPlan,
	}
}

func (cp *CapacityPlanner) calculateFutureQPS(years int) int64 {
	growthMultiplier := math.Pow(1+cp.GrowthRate, float64(years))
	return int64(float64(cp.CurrentQPS) * growthMultiplier)
}

func (cp *CapacityPlanner) estimateRecordCount(years int) int64 {
	// 基于QPS和业务特性估算记录数
	avgQPS := cp.calculateFutureQPS(years)
	secondsPerYear := int64(365 * 24 * 3600)
	return avgQPS * secondsPerYear * int64(years)
}

func (cp *CapacityPlanner) calculateStorageRequirement(recordCount int64) int64 {
	// 计算存储需求（GB）
	dataSize := recordCount * cp.AvgRecordSize
	indexSize := int64(float64(dataSize) * cp.IndexOverhead)
	totalSize := (dataSize + indexSize) * int64(cp.ReplicationCount)
	return totalSize / (1024 * 1024 * 1024) // 转换为GB
}

func (cp *CapacityPlanner) generateExpansionPlan(years int) []ExpansionPhase {
	var phases []ExpansionPhase
	
	// 生成分阶段扩容计划
	for year := 1; year <= years; year++ {
		targetQPS := cp.calculateFutureQPS(year)
		targetShards := int64(math.Ceil(float64(targetQPS) * cp.PeakQPSRatio * cp.SafetyFactor / 5000))
		
		phase := ExpansionPhase{
			Timeframe:    fmt.Sprintf("Year %d", year),
			TargetShards: targetShards,
			TriggerQPS:   int64(float64(targetQPS) * 0.8), // 80%阈值触发
			Actions: []string{
				fmt.Sprintf("Scale to %d shards", targetShards),
				"Update routing configuration",
				"Migrate data if necessary",
			},
		}
		phases = append(phases, phase)
	}
	
	return phases
}

// 使用示例
func main() {
	planner := &CapacityPlanner{
		CurrentQPS:       10000,
		PeakQPSRatio:     3.0,
		GrowthRate:       0.5, // 50%年增长率
		SafetyFactor:     1.5,
		AvgRecordSize:    1024, // 1KB
		IndexOverhead:    0.3,  // 30%索引开销
		ReplicationCount: 2,    // 主从复制
	}
	
	result := planner.CalculateCapacity(3) // 3年规划
	
	fmt.Printf("容量规划结果:\n")
	fmt.Printf("推荐分片数: %d\n", result.RecommendedShards)
	fmt.Printf("每分片存储: %d GB\n", result.StoragePerShard)
	fmt.Printf("每分片QPS: %d\n", result.QPSPerShard)
	fmt.Printf("总存储需求: %d GB\n", result.TotalStorage)
	
	fmt.Printf("\n扩容计划:\n")
	for _, phase := range result.ExpansionPlan {
		fmt.Printf("%s: 目标%d分片, 触发QPS %d\n", 
			phase.Timeframe, phase.TargetShards, phase.TriggerQPS)
	}
}
```

**2. 自动扩容实现**

```go
package main

import (
	"context"
	"fmt"
	"log"
	"sync"
	"time"

	v1 "k8s.io/api/apps/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
)

// AutoScaler 自动扩容器
type AutoScaler struct {
	k8sClient       kubernetes.Interface
	metrics         *MetricsCollector
	shardingManager *ShardingManager
	scaleRules      []ScaleRule
	mutex           sync.RWMutex
	isScaling       bool
}

// ScaleRule 扩容规则
type ScaleRule struct {
	MetricName    string  // 指标名称
	Threshold     float64 // 阈值
	Duration      time.Duration // 持续时间
	ScaleAction   string  // 扩容动作
	Cooldown      time.Duration // 冷却时间
}

// MetricsCollector 指标收集器
type MetricsCollector struct {
	qpsMetrics     map[string]float64
	cpuMetrics     map[string]float64
	memoryMetrics  map[string]float64
	connMetrics    map[string]float64
	mutex          sync.RWMutex
}

// NewAutoScaler 创建自动扩容器
func NewAutoScaler() *AutoScaler {
	// 初始化K8s客户端
	config, err := rest.InClusterConfig()
	if err != nil {
		log.Fatalf("Failed to create K8s config: %v", err)
	}
	
	clientset, err := kubernetes.NewForConfig(config)
	if err != nil {
		log.Fatalf("Failed to create K8s client: %v", err)
	}
	
	return &AutoScaler{
		k8sClient: clientset,
		metrics:   &MetricsCollector{
			qpsMetrics:    make(map[string]float64),
			cpuMetrics:    make(map[string]float64),
			memoryMetrics: make(map[string]float64),
			connMetrics:   make(map[string]float64),
		},
		scaleRules: []ScaleRule{
			{
				MetricName:  "qps",
				Threshold:   4000, // QPS超过4000
				Duration:    5 * time.Minute,
				ScaleAction: "scale_out",
				Cooldown:    10 * time.Minute,
			},
			{
				MetricName:  "cpu",
				Threshold:   80.0, // CPU使用率超过80%
				Duration:    3 * time.Minute,
				ScaleAction: "scale_out",
				Cooldown:    10 * time.Minute,
			},
			{
				MetricName:  "connections",
				Threshold:   800, // 连接数超过800
				Duration:    2 * time.Minute,
				ScaleAction: "scale_out",
				Cooldown:    15 * time.Minute,
			},
		},
	}
}

// StartMonitoring 开始监控
func (as *AutoScaler) StartMonitoring(ctx context.Context) {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			as.collectMetrics()
			as.evaluateScaleRules()
		}
	}
}

// collectMetrics 收集指标
func (as *AutoScaler) collectMetrics() {
	as.metrics.mutex.Lock()
	defer as.metrics.mutex.Unlock()
	
	// 收集各分片的QPS指标
	for shardName := range as.shardingManager.dataSources {
		// 模拟指标收集（实际应该从Prometheus等监控系统获取）
		qps := as.getShardQPS(shardName)
		cpu := as.getShardCPU(shardName)
		memory := as.getShardMemory(shardName)
		connections := as.getShardConnections(shardName)
		
		as.metrics.qpsMetrics[shardName] = qps
		as.metrics.cpuMetrics[shardName] = cpu
		as.metrics.memoryMetrics[shardName] = memory
		as.metrics.connMetrics[shardName] = connections
	}
}

// evaluateScaleRules 评估扩容规则
func (as *AutoScaler) evaluateScaleRules() {
	as.mutex.RLock()
	if as.isScaling {
		as.mutex.RUnlock()
		return
	}
	as.mutex.RUnlock()
	
	for _, rule := range as.scaleRules {
		if as.shouldTriggerScale(rule) {
			go as.executeScale(rule)
			break // 一次只执行一个扩容操作
		}
	}
}

// shouldTriggerScale 判断是否应该触发扩容
func (as *AutoScaler) shouldTriggerScale(rule ScaleRule) bool {
	as.metrics.mutex.RLock()
	defer as.metrics.mutex.RUnlock()
	
	var metrics map[string]float64
	switch rule.MetricName {
	case "qps":
		metrics = as.metrics.qpsMetrics
	case "cpu":
		metrics = as.metrics.cpuMetrics
	case "memory":
		metrics = as.metrics.memoryMetrics
	case "connections":
		metrics = as.metrics.connMetrics
	default:
		return false
	}
	
	// 检查是否有分片超过阈值
	for _, value := range metrics {
		if value > rule.Threshold {
			return true
		}
	}
	
	return false
}

// executeScale 执行扩容
func (as *AutoScaler) executeScale(rule ScaleRule) {
	as.mutex.Lock()
	as.isScaling = true
	as.mutex.Unlock()
	
	defer func() {
		as.mutex.Lock()
		as.isScaling = false
		as.mutex.Unlock()
		
		// 冷却期
		time.Sleep(rule.Cooldown)
	}()
	
	log.Printf("开始执行扩容: %s, 阈值: %.2f", rule.MetricName, rule.Threshold)
	
	switch rule.ScaleAction {
	case "scale_out":
		as.scaleOutDatabase()
	case "scale_up":
		as.scaleUpDatabase()
	}
}

// scaleOutDatabase 水平扩容数据库
func (as *AutoScaler) scaleOutDatabase() {
	// 1. 创建新的数据库实例
	newShardName := fmt.Sprintf("shard-%d", time.Now().Unix())
	
	// 2. 部署新的MySQL实例到K8s
	deployment := &v1.Deployment{
		ObjectMeta: metav1.ObjectMeta{
			Name:      newShardName,
			Namespace: "database",
		},
		Spec: v1.DeploymentSpec{
			Replicas: int32Ptr(1),
			Selector: &metav1.LabelSelector{
				MatchLabels: map[string]string{
					"app": newShardName,
				},
			},
			Template: as.createPodTemplate(newShardName),
		},
	}
	
	ctx := context.Background()
	_, err := as.k8sClient.AppsV1().Deployments("database").Create(ctx, deployment, metav1.CreateOptions{})
	if err != nil {
		log.Printf("Failed to create deployment: %v", err)
		return
	}
	
	// 3. 等待实例就绪
	time.Sleep(2 * time.Minute)
	
	// 4. 更新分片配置
	as.shardingManager.AddShard(newShardName, fmt.Sprintf("%s:3306", newShardName))
	
	// 5. 数据迁移（如果需要）
	go as.migrateDataToNewShard(newShardName)
	
	log.Printf("成功创建新分片: %s", newShardName)
}

// scaleUpDatabase 垂直扩容数据库
func (as *AutoScaler) scaleUpDatabase() {
	// 增加现有实例的资源配置
	log.Println("执行垂直扩容...")
	// 实现垂直扩容逻辑
}

// 辅助方法
func (as *AutoScaler) getShardQPS(shardName string) float64 {
	// 实际应该从监控系统获取
	return 3500.0 + float64(time.Now().Unix()%1000)
}

func (as *AutoScaler) getShardCPU(shardName string) float64 {
	return 75.0 + float64(time.Now().Unix()%20)
}

func (as *AutoScaler) getShardMemory(shardName string) float64 {
	return 60.0 + float64(time.Now().Unix()%30)
}

func (as *AutoScaler) getShardConnections(shardName string) float64 {
	return 700.0 + float64(time.Now().Unix()%200)
}

func (as *AutoScaler) createPodTemplate(shardName string) v1.PodTemplateSpec {
	// 创建Pod模板
	return v1.PodTemplateSpec{}
}

func (as *AutoScaler) migrateDataToNewShard(shardName string) {
	// 数据迁移逻辑
	log.Printf("开始迁移数据到新分片: %s", shardName)
}

func int32Ptr(i int32) *int32 {
	return &i
}
```

### 故障处理与恢复

**1. 熔断器实现**

```go
package main

import (
	"context"
	"errors"
	"fmt"
	"sync"
	"time"
)

// CircuitBreakerState 熔断器状态
type CircuitBreakerState int

const (
	StateClosed CircuitBreakerState = iota // 关闭状态
	StateOpen                              // 开启状态
	StateHalfOpen                          // 半开状态
)

// CircuitBreaker 熔断器
type CircuitBreaker struct {
	name            string
	maxRequests     uint32        // 半开状态最大请求数
	interval        time.Duration // 统计时间窗口
	timeout         time.Duration // 开启状态超时时间
	readyToTrip     func(counts Counts) bool // 触发熔断条件
	onStateChange   func(name string, from, to CircuitBreakerState) // 状态变更回调
	
	mutex      sync.Mutex
	state      CircuitBreakerState
	generation uint64
	counts     Counts
	expiry     time.Time
}

// Counts 统计计数
type Counts struct {
	Requests             uint32 // 总请求数
	TotalSuccesses       uint32 // 总成功数
	TotalFailures        uint32 // 总失败数
	ConsecutiveSuccesses uint32 // 连续成功数
	ConsecutiveFailures  uint32 // 连续失败数
}

// NewCircuitBreaker 创建熔断器
func NewCircuitBreaker(name string, settings Settings) *CircuitBreaker {
	cb := &CircuitBreaker{
		name:        name,
		maxRequests: settings.MaxRequests,
		interval:    settings.Interval,
		timeout:     settings.Timeout,
		readyToTrip: settings.ReadyToTrip,
		onStateChange: settings.OnStateChange,
		state:       StateClosed,
		expiry:      time.Now().Add(settings.Interval),
	}
	
	if cb.readyToTrip == nil {
		cb.readyToTrip = defaultReadyToTrip
	}
	
	return cb
}

// Settings 熔断器配置
type Settings struct {
	MaxRequests   uint32
	Interval      time.Duration
	Timeout       time.Duration
	ReadyToTrip   func(counts Counts) bool
	OnStateChange func(name string, from, to CircuitBreakerState)
}

// Execute 执行请求
func (cb *CircuitBreaker) Execute(req func() (interface{}, error)) (interface{}, error) {
	generation, err := cb.beforeRequest()
	if err != nil {
		return nil, err
	}
	
	defer func() {
		e := recover()
		if e != nil {
			cb.afterRequest(generation, false)
			panic(e)
		}
	}()
	
	result, err := req()
	cb.afterRequest(generation, err == nil)
	return result, err
}

// beforeRequest 请求前检查
func (cb *CircuitBreaker) beforeRequest() (uint64, error) {
	cb.mutex.Lock()
	defer cb.mutex.Unlock()
	
	now := time.Now()
	state, generation := cb.currentState(now)
	
	if state == StateOpen {
		return generation, errors.New("circuit breaker is open")
	} else if state == StateHalfOpen && cb.counts.Requests >= cb.maxRequests {
		return generation, errors.New("too many requests")
	}
	
	cb.counts.Requests++
	return generation, nil
}

// afterRequest 请求后处理
func (cb *CircuitBreaker) afterRequest(before uint64, success bool) {
	cb.mutex.Lock()
	defer cb.mutex.Unlock()
	
	now := time.Now()
	state, generation := cb.currentState(now)
	if generation != before {
		return
	}
	
	if success {
		cb.onSuccess(state, now)
	} else {
		cb.onFailure(state, now)
	}
}

// currentState 获取当前状态
func (cb *CircuitBreaker) currentState(now time.Time) (CircuitBreakerState, uint64) {
	switch cb.state {
	case StateClosed:
		if !cb.expiry.IsZero() && cb.expiry.Before(now) {
			cb.toNewGeneration(now)
		}
	case StateOpen:
		if cb.expiry.Before(now) {
			cb.setState(StateHalfOpen, now)
		}
	}
	return cb.state, cb.generation
}

// onSuccess 成功处理
func (cb *CircuitBreaker) onSuccess(state CircuitBreakerState, now time.Time) {
	cb.counts.TotalSuccesses++
	cb.counts.ConsecutiveSuccesses++
	cb.counts.ConsecutiveFailures = 0
	
	if state == StateHalfOpen {
		cb.setState(StateClosed, now)
	}
}

// onFailure 失败处理
func (cb *CircuitBreaker) onFailure(state CircuitBreakerState, now time.Time) {
	cb.counts.TotalFailures++
	cb.counts.ConsecutiveFailures++
	cb.counts.ConsecutiveSuccesses = 0
	
	if cb.readyToTrip(cb.counts) {
		cb.setState(StateOpen, now)
	}
}

// setState 设置状态
func (cb *CircuitBreaker) setState(state CircuitBreakerState, now time.Time) {
	if cb.state == state {
		return
	}
	
	prev := cb.state
	cb.state = state
	cb.toNewGeneration(now)
	
	if cb.onStateChange != nil {
		cb.onStateChange(cb.name, prev, state)
	}
}

// toNewGeneration 新一代
func (cb *CircuitBreaker) toNewGeneration(now time.Time) {
	cb.generation++
	cb.counts = Counts{}
	
	var zero time.Time
	switch cb.state {
	case StateClosed:
		if cb.interval == 0 {
			cb.expiry = zero
		} else {
			cb.expiry = now.Add(cb.interval)
		}
	case StateOpen:
		cb.expiry = now.Add(cb.timeout)
	default: // StateHalfOpen
		cb.expiry = zero
	}
}

// 默认熔断条件
func defaultReadyToTrip(counts Counts) bool {
	return counts.Requests >= 20 && counts.TotalFailures > counts.TotalSuccesses
}

// ShardingCircuitBreaker 分片熔断器管理
type ShardingCircuitBreaker struct {
	breakers map[string]*CircuitBreaker
	mutex    sync.RWMutex
}

// NewShardingCircuitBreaker 创建分片熔断器
func NewShardingCircuitBreaker() *ShardingCircuitBreaker {
	return &ShardingCircuitBreaker{
		breakers: make(map[string]*CircuitBreaker),
	}
}

// GetBreaker 获取分片熔断器
func (scb *ShardingCircuitBreaker) GetBreaker(shardName string) *CircuitBreaker {
	scb.mutex.RLock()
	breaker, exists := scb.breakers[shardName]
	scb.mutex.RUnlock()
	
	if exists {
		return breaker
	}
	
	scb.mutex.Lock()
	defer scb.mutex.Unlock()
	
	// 双重检查
	if breaker, exists := scb.breakers[shardName]; exists {
		return breaker
	}
	
	// 创建新的熔断器
	settings := Settings{
		MaxRequests: 10,
		Interval:    60 * time.Second,
		Timeout:     30 * time.Second,
		ReadyToTrip: func(counts Counts) bool {
			failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)
			return counts.Requests >= 10 && failureRatio >= 0.6
		},
		OnStateChange: func(name string, from, to CircuitBreakerState) {
			fmt.Printf("熔断器 %s 状态变更: %v -> %v\n", name, from, to)
		},
	}
	
	breaker = NewCircuitBreaker(shardName, settings)
	scb.breakers[shardName] = breaker
	return breaker
}

// ExecuteWithBreaker 使用熔断器执行数据库操作
func (scb *ShardingCircuitBreaker) ExecuteWithBreaker(shardName string, operation func() (interface{}, error)) (interface{}, error) {
	breaker := scb.GetBreaker(shardName)
	return breaker.Execute(operation)
}
```

**2. 数据备份与恢复**

```go
package main

import (
	"context"
	"fmt"
	"log"
	"os/exec"
	"path/filepath"
	"sync"
	"time"
)

// BackupManager 备份管理器
type BackupManager struct {
	shardingManager *ShardingManager
	backupConfig    BackupConfig
	mutex           sync.RWMutex
	backupStatus    map[string]*BackupStatus
}

// BackupConfig 备份配置
type BackupConfig struct {
	BackupPath      string        // 备份路径
	RetentionDays   int           // 保留天数
	BackupInterval  time.Duration // 备份间隔
	CompressionType string        // 压缩类型
	EncryptionKey   string        // 加密密钥
	S3Config        *S3Config     // S3配置
}

// S3Config S3配置
type S3Config struct {
	Bucket    string
	Region    string
	AccessKey string
	SecretKey string
}

// BackupStatus 备份状态
type BackupStatus struct {
	ShardName    string
	LastBackup   time.Time
	BackupSize   int64
	Status       string // running, completed, failed
	ErrorMessage string
}

// NewBackupManager 创建备份管理器
func NewBackupManager(sm *ShardingManager, config BackupConfig) *BackupManager {
	return &BackupManager{
		shardingManager: sm,
		backupConfig:    config,
		backupStatus:    make(map[string]*BackupStatus),
	}
}

// StartScheduledBackup 启动定时备份
func (bm *BackupManager) StartScheduledBackup(ctx context.Context) {
	ticker := time.NewTicker(bm.backupConfig.BackupInterval)
	defer ticker.Stop()
	
	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			bm.performFullBackup()
		}
	}
}

// performFullBackup 执行全量备份
func (bm *BackupManager) performFullBackup() {
	log.Println("开始执行全量备份...")
	
	var wg sync.WaitGroup
	for shardName := range bm.shardingManager.dataSources {
		wg.Add(1)
		go func(shard string) {
			defer wg.Done()
			bm.backupShard(shard)
		}(shardName)
	}
	
	wg.Wait()
	log.Println("全量备份完成")
	
	// 清理过期备份
	bm.cleanupOldBackups()
}

// backupShard 备份单个分片
func (bm *BackupManager) backupShard(shardName string) {
	bm.mutex.Lock()
	bm.backupStatus[shardName] = &BackupStatus{
		ShardName: shardName,
		Status:    "running",
	}
	bm.mutex.Unlock()
	
	startTime := time.Now()
	backupFile := bm.generateBackupFileName(shardName, startTime)
	
	// 执行mysqldump
	err := bm.executeMySQLDump(shardName, backupFile)
	if err != nil {
		bm.updateBackupStatus(shardName, "failed", 0, err.Error())
		return
	}
	
	// 压缩备份文件
	compressedFile, err := bm.compressBackup(backupFile)
	if err != nil {
		bm.updateBackupStatus(shardName, "failed", 0, err.Error())
		return
	}
	
	// 上传到S3（如果配置了）
	if bm.backupConfig.S3Config != nil {
		err = bm.uploadToS3(compressedFile)
		if err != nil {
			log.Printf("Failed to upload backup to S3: %v", err)
		}
	}
	
	// 获取文件大小
	fileInfo, _ := filepath.Glob(compressedFile)
	var fileSize int64
	if len(fileInfo) > 0 {
		// 获取文件大小逻辑
		fileSize = 1024 * 1024 // 示例大小
	}
	
	bm.updateBackupStatus(shardName, "completed", fileSize, "")
	log.Printf("分片 %s 备份完成: %s", shardName, compressedFile)
}

// executeMySQLDump 执行MySQL备份
func (bm *BackupManager) executeMySQLDump(shardName, backupFile string) error {
	// 获取分片连接信息
	ds := bm.shardingManager.dataSources[shardName]
	if ds == nil {
		return fmt.Errorf("shard %s not found", shardName)
	}
	
	// 构建mysqldump命令
	cmd := exec.Command("mysqldump",
		"--host=localhost",
		"--port=3306",
		"--user=backup_user",
		"--password=backup_password",
		"--single-transaction",
		"--routines",
		"--triggers",
		"--all-databases",
		"--result-file="+backupFile,
	)
	
	return cmd.Run()
}

// compressBackup 压缩备份文件
func (bm *BackupManager) compressBackup(backupFile string) (string, error) {
	compressedFile := backupFile + ".gz"
	
	cmd := exec.Command("gzip", backupFile)
	err := cmd.Run()
	if err != nil {
		return "", err
	}
	
	return compressedFile, nil
}

// uploadToS3 上传到S3
func (bm *BackupManager) uploadToS3(filePath string) error {
	// 实现S3上传逻辑
	log.Printf("上传备份文件到S3: %s", filePath)
	return nil
}

// generateBackupFileName 生成备份文件名
func (bm *BackupManager) generateBackupFileName(shardName string, timestamp time.Time) string {
	fileName := fmt.Sprintf("%s_%s.sql", shardName, timestamp.Format("20060102_150405"))
	return filepath.Join(bm.backupConfig.BackupPath, fileName)
}

// updateBackupStatus 更新备份状态
func (bm *BackupManager) updateBackupStatus(shardName, status string, size int64, errorMsg string) {
	bm.mutex.Lock()
	defer bm.mutex.Unlock()
	
	if bm.backupStatus[shardName] == nil {
		bm.backupStatus[shardName] = &BackupStatus{ShardName: shardName}
	}
	
	bm.backupStatus[shardName].Status = status
	bm.backupStatus[shardName].BackupSize = size
	bm.backupStatus[shardName].ErrorMessage = errorMsg
	bm.backupStatus[shardName].LastBackup = time.Now()
}

// cleanupOldBackups 清理过期备份
func (bm *BackupManager) cleanupOldBackups() {
	cutoffTime := time.Now().AddDate(0, 0, -bm.backupConfig.RetentionDays)
	
	// 清理本地备份文件
	pattern := filepath.Join(bm.backupConfig.BackupPath, "*.sql.gz")
	files, err := filepath.Glob(pattern)
	if err != nil {
		log.Printf("Failed to list backup files: %v", err)
		return
	}
	
	for _, file := range files {
		// 检查文件修改时间并删除过期文件
		// 实现文件时间检查和删除逻辑
		log.Printf("检查备份文件: %s", file)
	}
	
	log.Printf("清理 %s 之前的备份文件", cutoffTime.Format("2006-01-02"))
}

// RestoreFromBackup 从备份恢复
func (bm *BackupManager) RestoreFromBackup(shardName, backupFile string) error {
	log.Printf("开始恢复分片 %s 从备份文件 %s", shardName, backupFile)
	
	// 1. 解压备份文件
	if filepath.Ext(backupFile) == ".gz" {
		cmd := exec.Command("gunzip", backupFile)
		if err := cmd.Run(); err != nil {
			return fmt.Errorf("failed to decompress backup: %w", err)
		}
		backupFile = backupFile[:len(backupFile)-3] // 移除.gz扩展名
	}
	
	// 2. 执行MySQL恢复
	cmd := exec.Command("mysql",
		"--host=localhost",
		"--port=3306",
		"--user=restore_user",
		"--password=restore_password",
	)
	
	// 设置输入文件
	cmd.Stdin, _ = filepath.Glob(backupFile)
	
	err := cmd.Run()
	if err != nil {
		return fmt.Errorf("failed to restore from backup: %w", err)
	}
	
	log.Printf("分片 %s 恢复完成", shardName)
	return nil
}

// GetBackupStatus 获取备份状态
func (bm *BackupManager) GetBackupStatus() map[string]*BackupStatus {
	bm.mutex.RLock()
	defer bm.mutex.RUnlock()
	
	status := make(map[string]*BackupStatus)
	for k, v := range bm.backupStatus {
		status[k] = v
	}
	return status
}
```

## 面试要点总结

### 系统设计类问题

**1. 如何设计一个支持千万级用户的分库分表系统？**

**回答要点：**
- **容量规划**：基于用户增长预测，计算存储和QPS需求
- **分片策略**：用户ID哈希分片，保证数据均匀分布
- **路由设计**：一致性哈希算法，支持平滑扩容
- **数据迁移**：双写+渐进式迁移，保证服务不中断
- **监控告警**：QPS、延迟、错误率等关键指标监控

```go
// 示例：千万级用户分片设计
type UserShardingStrategy struct {
    ShardCount int // 初始分片数：64
    GrowthPlan []ExpansionPhase // 扩容计划
}

func (uss *UserShardingStrategy) CalculateShardID(userID int64) int {
    return int(userID % int64(uss.ShardCount))
}

// 扩容策略：64 -> 128 -> 256
func (uss *UserShardingStrategy) PlanExpansion() {
    // 当单分片用户数超过15万时触发扩容
    // 目标：每分片10万用户，峰值QPS不超过5000
}
```

**2. 分库分表后如何处理跨库事务？**

**回答要点：**
- **避免跨库事务**：业务设计时尽量避免
- **最终一致性**：使用消息队列实现最终一致性
- **分布式事务**：2PC、TCC、Saga等模式
- **补偿机制**：失败回滚和重试策略

```go
// 示例：基于消息的最终一致性
type EventualConsistencyManager struct {
    messageQueue MessageQueue
    retryPolicy  RetryPolicy
}

func (ecm *EventualConsistencyManager) ExecuteDistributedTransaction(
    operations []Operation) error {
    
    // 1. 执行本地事务
    localTx := ecm.beginLocalTransaction()
    
    // 2. 发送事务消息
    for _, op := range operations {
        message := TransactionMessage{
            OperationType: op.Type,
            Data:         op.Data,
            Timestamp:    time.Now(),
        }
        ecm.messageQueue.Send(message)
    }
    
    // 3. 提交本地事务
    return localTx.Commit()
}
```

### 高频技术问题

**1. 分片键选择的原则是什么？**

**回答要点：**
- **业务相关性**：选择业务查询最频繁的字段
- **数据分布均匀**：避免热点数据集中
- **扩展性**：支持未来的扩容需求
- **查询效率**：减少跨分片查询

**常见分片键选择：**
- 用户系统：user_id
- 订单系统：user_id 或 order_id
- 内容系统：content_id 或 user_id
- 日志系统：时间戳

**2. 如何解决分库分表后的热点问题？**

**解决方案：**
```go
// 热点数据处理策略
type HotDataHandler struct {
    cache       Cache
    readReplicas map[string][]DataSource
    loadBalancer LoadBalancer
}

func (hdh *HotDataHandler) HandleHotData(key string) {
    // 1. 缓存预热
    hdh.cache.Warmup(key)
    
    // 2. 读写分离
    readDS := hdh.readReplicas[hdh.getShardName(key)]
    hdh.loadBalancer.AddReadReplicas(readDS)
    
    // 3. 数据分散
    hdh.redistributeHotData(key)
}
```

**3. 分库分表的性能优化策略？**

**优化策略：**
- **索引优化**：合理设计分片内索引
- **连接池**：优化数据库连接管理
- **批量操作**：减少网络往返次数
- **异步处理**：非核心操作异步化
- **缓存策略**：多级缓存架构

### 深度技术问题

**1. 一致性哈希在分库分表中的应用？**

```go
// 一致性哈希实现
type ConsistentHash struct {
    ring     map[uint32]string // 哈希环
    nodes    map[string]bool   // 节点集合
    replicas int               // 虚拟节点数
}

func (ch *ConsistentHash) AddNode(node string) {
    for i := 0; i < ch.replicas; i++ {
        hash := ch.hashFunc(fmt.Sprintf("%s:%d", node, i))
        ch.ring[hash] = node
    }
    ch.nodes[node] = true
}

func (ch *ConsistentHash) GetNode(key string) string {
    hash := ch.hashFunc(key)
    
    // 顺时针查找第一个节点
    for h, node := range ch.ring {
        if h >= hash {
            return node
        }
    }
    
    // 环形结构，返回第一个节点
    return ch.getFirstNode()
}
```

**2. 数据迁移的技术细节？**

```go
// 数据迁移管理器
type DataMigrationManager struct {
    sourceDB      Database
    targetDB      Database
    migrationRate int // 每秒迁移记录数
    validator     DataValidator
}

func (dmm *DataMigrationManager) MigrateData(
    tableName string, startID, endID int64) error {
    
    batchSize := 1000
    for id := startID; id <= endID; id += int64(batchSize) {
        // 1. 读取源数据
        data, err := dmm.sourceDB.SelectRange(tableName, id, id+int64(batchSize))
        if err != nil {
            return err
        }
        
        // 2. 写入目标库
        err = dmm.targetDB.BatchInsert(tableName, data)
        if err != nil {
            return err
        }
        
        // 3. 数据校验
        if !dmm.validator.Validate(data) {
            return errors.New("data validation failed")
        }
        
        // 4. 限流控制
        time.Sleep(time.Second / time.Duration(dmm.migrationRate/batchSize))
    }
    
    return nil
}
```

**3. 分库分表监控体系设计？**

```go
// 监控指标收集器
type ShardingMonitor struct {
    metrics     map[string]*Metric
    alertRules  []AlertRule
    dashboard   Dashboard
}

type Metric struct {
    Name      string
    Value     float64
    Timestamp time.Time
    Labels    map[string]string
}

func (sm *ShardingMonitor) CollectMetrics() {
    // 1. QPS指标
    sm.collectQPSMetrics()
    
    // 2. 延迟指标
    sm.collectLatencyMetrics()
    
    // 3. 错误率指标
    sm.collectErrorRateMetrics()
    
    // 4. 连接数指标
    sm.collectConnectionMetrics()
    
    // 5. 存储容量指标
    sm.collectStorageMetrics()
}

func (sm *ShardingMonitor) CheckAlerts() {
    for _, rule := range sm.alertRules {
        if rule.ShouldAlert(sm.metrics) {
            sm.sendAlert(rule)
        }
    }
}
```

## 应用场景扩展

### 垂直行业应用

**1. 电商平台分库分表方案**

```go
// 电商多维度分片策略
type ECommerceSharding struct {
    UserShards    int // 用户维度分片
    ProductShards int // 商品维度分片
    OrderShards   int // 订单维度分片
}

// 用户表：按user_id分片
func (ecs *ECommerceSharding) GetUserShard(userID int64) string {
    return fmt.Sprintf("user_shard_%d", userID%int64(ecs.UserShards))
}

// 订单表：按user_id分片（保证用户订单在同一分片）
func (ecs *ECommerceSharding) GetOrderShard(userID int64) string {
    return fmt.Sprintf("order_shard_%d", userID%int64(ecs.OrderShards))
}

// 商品表：按category_id分片
func (ecs *ECommerceSharding) GetProductShard(categoryID int64) string {
    return fmt.Sprintf("product_shard_%d", categoryID%int64(ecs.ProductShards))
}
```

**2. 金融系统分库分表方案**

```go
// 金融系统分片策略
type FinancialSharding struct {
    AccountShards     int // 账户分片
    TransactionShards int // 交易分片
    RiskShards        int // 风控分片
}

// 账户表：按account_id分片
func (fs *FinancialSharding) GetAccountShard(accountID string) string {
    hash := crc32.ChecksumIEEE([]byte(accountID))
    return fmt.Sprintf("account_shard_%d", hash%uint32(fs.AccountShards))
}

// 交易表：按时间+账户双重分片
func (fs *FinancialSharding) GetTransactionShard(
    accountID string, timestamp time.Time) string {
    
    // 时间维度：按月分片
    monthShard := timestamp.Format("200601")
    
    // 账户维度：哈希分片
    hash := crc32.ChecksumIEEE([]byte(accountID))
    accountShard := hash % uint32(fs.TransactionShards)
    
    return fmt.Sprintf("transaction_%s_shard_%d", monthShard, accountShard)
}
```

**3. 社交平台分库分表方案**

```go
// 社交平台分片策略
type SocialSharding struct {
    UserShards         int // 用户分片
    RelationshipShards int // 关系分片
    ContentShards      int // 内容分片
    FeedShards         int // 动态分片
}

// 用户关系表：双向分片
func (ss *SocialSharding) GetRelationshipShard(userID1, userID2 int64) string {
    // 确保关系的双向查询在同一分片
    minID := userID1
    maxID := userID2
    if userID1 > userID2 {
        minID = userID2
        maxID = userID1
    }
    
    shardID := (minID + maxID) % int64(ss.RelationshipShards)
    return fmt.Sprintf("relationship_shard_%d", shardID)
}

// 内容表：按用户分片
func (ss *SocialSharding) GetContentShard(userID int64) string {
    return fmt.Sprintf("content_shard_%d", userID%int64(ss.ContentShards))
}

// 动态流表：按用户分片
func (ss *SocialSharding) GetFeedShard(userID int64) string {
    return fmt.Sprintf("feed_shard_%d", userID%int64(ss.FeedShards))
}
```

### 技术演进方向

**1. 云原生分库分表**

```go
// 云原生分片管理器
type CloudNativeSharding struct {
    k8sClient    kubernetes.Interface
    helmClient   helm.Interface
    prometheus   prometheus.Client
    autoScaler   *AutoScaler
}

// 基于K8s的动态分片部署
func (cns *CloudNativeSharding) DeployNewShard(shardName string) error {
    // 1. 创建MySQL StatefulSet
    statefulSet := cns.createMySQLStatefulSet(shardName)
    
    // 2. 创建Service
    service := cns.createMySQLService(shardName)
    
    // 3. 创建PVC
    pvc := cns.createPersistentVolumeClaim(shardName)
    
    // 4. 部署到K8s
    return cns.deployToK8s(statefulSet, service, pvc)
}

// 基于Prometheus的自动扩容
func (cns *CloudNativeSharding) AutoScale() {
    metrics := cns.prometheus.QueryMetrics()
    
    for shardName, metric := range metrics {
        if metric.QPS > 4000 || metric.CPUUsage > 80 {
            cns.autoScaler.ScaleOut(shardName)
        }
    }
}
```

**2. 智能化分片优化**

```go
// AI驱动的分片优化器
type AIShardingOptimizer struct {
    mlModel        MachineLearningModel
    dataCollector  *DataCollector
    optimizer      *ShardOptimizer
}

// 基于机器学习的分片策略优化
func (aiso *AIShardingOptimizer) OptimizeSharding() {
    // 1. 收集历史数据
    data := aiso.dataCollector.CollectHistoricalData()
    
    // 2. 训练模型
    aiso.mlModel.Train(data)
    
    // 3. 预测最优分片策略
    optimalStrategy := aiso.mlModel.Predict()
    
    // 4. 应用优化策略
    aiso.optimizer.ApplyStrategy(optimalStrategy)
}

// 智能负载预测
func (aiso *AIShardingOptimizer) PredictLoad() LoadPrediction {
    features := aiso.extractFeatures()
    return aiso.mlModel.PredictLoad(features)
}
```

**3. 多云分布式分片**

```go
// 多云分片管理器
type MultiCloudSharding struct {
    awsManager    *AWSManager
    gcpManager    *GCPManager
    azureManager  *AzureManager
    loadBalancer  *GlobalLoadBalancer
}

// 跨云分片部署
func (mcs *MultiCloudSharding) DeployAcrossClouds(
    shardConfig ShardConfig) error {
    
    // 1. AWS分片
    awsShards := shardConfig.AWSShards
    for _, shard := range awsShards {
        mcs.awsManager.DeployShard(shard)
    }
    
    // 2. GCP分片
    gcpShards := shardConfig.GCPShards
    for _, shard := range gcpShards {
        mcs.gcpManager.DeployShard(shard)
    }
    
    // 3. Azure分片
    azureShards := shardConfig.AzureShards
    for _, shard := range azureShards {
        mcs.azureManager.DeployShard(shard)
    }
    
    // 4. 配置全局负载均衡
    return mcs.loadBalancer.ConfigureGlobalRouting()
}
```

### 商业模式创新

**1. 分片即服务（Sharding as a Service）**

```go
// SaaS分片服务
type ShardingService struct {
    tenantManager   *TenantManager
    resourceManager *ResourceManager
    billingManager  *BillingManager
}

// 租户分片管理
func (ss *ShardingService) CreateTenantSharding(
    tenantID string, config ShardingConfig) error {
    
    // 1. 创建租户专属分片
    shards := ss.resourceManager.AllocateShards(tenantID, config)
    
    // 2. 配置租户路由
    ss.tenantManager.ConfigureRouting(tenantID, shards)
    
    // 3. 设置计费规则
    ss.billingManager.SetupBilling(tenantID, config)
    
    return nil
}

// 弹性计费模式
func (ss *ShardingService) CalculateBilling(tenantID string) BillingInfo {
    usage := ss.resourceManager.GetUsage(tenantID)
    
    return BillingInfo{
        StorageUsage: usage.Storage * 0.1,  // $0.1/GB
        QPSUsage:     usage.QPS * 0.001,   // $0.001/QPS
        BandwidthUsage: usage.Bandwidth * 0.05, // $0.05/GB
    }
}
```

**2. 边缘计算分片**

```go
// 边缘分片管理器
type EdgeSharding struct {
    edgeNodes     map[string]*EdgeNode
    syncManager   *DataSyncManager
    cacheManager  *EdgeCacheManager
}

// 边缘节点分片部署
func (es *EdgeSharding) DeployToEdge(
    region string, shardConfig EdgeShardConfig) error {
    
    edgeNode := es.edgeNodes[region]
    
    // 1. 部署轻量级分片
    lightShard := es.createLightweightShard(shardConfig)
    edgeNode.DeployShard(lightShard)
    
    // 2. 配置数据同步
    es.syncManager.SetupSync(region, shardConfig.SyncPolicy)
    
    // 3. 配置边缘缓存
    es.cacheManager.SetupEdgeCache(region, shardConfig.CachePolicy)
    
    return nil
}
```

## 解决的问题

### 1. 性能问题

**单表性能瓶颈：**
- **查询性能下降**：千万级数据查询缓慢
- **索引效率降低**：B+树层级增加，IO次数增多
- **锁竞争加剧**：表级锁、行级锁冲突频繁

**解决效果：**
```
分片前：
- 单表1000万数据
- 查询响应时间：500ms-2s
- QPS：1000

分片后（8个分片）：
- 单表125万数据
- 查询响应时间：50ms-200ms
- QPS：8000（理论值）
```

### 2. 存储问题

**存储容量限制：**
- MySQL单表建议不超过2000万行
- 单库容量建议不超过500GB
- 磁盘IO成为瓶颈

**解决方案：**
```
垂直分库：按业务模块分离
- 用户库：100GB
- 订单库：200GB
- 商品库：150GB
- 日志库：300GB

水平分库：按数据量分散
- 4个分库，每库负载降低75%
- 存储压力分散
- IO并发能力提升
```

### 3. 可用性问题

**单点故障风险：**
- 单库故障影响全业务
- 备份恢复时间长
- 维护窗口影响大

**高可用架构：**
```
┌─────────────┐ ┌─────────────┐ ┌─────────────┐
│   分片1     │ │   分片2     │ │   分片3     │
│ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
│ │ Master  │ │ │ │ Master  │ │ │ │ Master  │ │
│ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
│ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
│ │ Slave   │ │ │ │ Slave   │ │ │ │ Slave   │ │
│ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
└─────────────┘ └─────────────┘ └─────────────┘
```

## 方案设计

### 1. 垂直分库方案

**设计原则：**
- 按业务领域划分
- 减少跨库事务
- 便于团队协作

**实施步骤：**
```
1. 业务梳理
   ├── 用户域：用户信息、权限、认证
   ├── 商品域：商品信息、分类、库存
   ├── 订单域：订单、支付、物流
   └── 营销域：优惠券、活动、推荐

2. 数据库设计
   ├── user_db：用户相关表
   ├── product_db：商品相关表
   ├── order_db：订单相关表
   └── marketing_db：营销相关表

3. 应用改造
   ├── 数据源配置
   ├── DAO层改造
   ├── 事务处理
   └── 数据一致性
```

### 2. 水平分库分表方案

**分片键选择：**
```
常用分片键：
1. 用户ID：适合用户相关数据
2. 订单ID：适合订单相关数据
3. 时间：适合日志、流水数据
4. 地理位置：适合LBS应用
5. 业务ID：适合特定业务场景

选择原则：
- 数据分布均匀
- 查询路由简单
- 避免热点数据
- 支持范围查询
```

**分片数量规划：**
```go
// 分片数量计算
func CalculateShardCount(totalData, maxDataPerShard int64) int {
    shardCount := int(math.Ceil(float64(totalData) / float64(maxDataPerShard)))
    
    // 确保是2的幂次，便于扩容
    powerOf2 := 1
    for powerOf2 < shardCount {
        powerOf2 *= 2
    }
    
    return powerOf2
}

// 示例：
// 预计5年数据量：10亿
// 单表最大数据：1000万
// 计算结果：128个分片（2^7）
```

### 3. 混合分片方案

**垂直+水平分片：**
```
第一层：垂直分库（按业务）
├── user_cluster
│   ├── user_db_0 (user_id % 4 = 0)
│   ├── user_db_1 (user_id % 4 = 1)
│   ├── user_db_2 (user_id % 4 = 2)
│   └── user_db_3 (user_id % 4 = 3)
│
└── order_cluster
    ├── order_db_0 (user_id % 4 = 0)
    ├── order_db_1 (user_id % 4 = 1)
    ├── order_db_2 (user_id % 4 = 2)
    └── order_db_3 (user_id % 4 = 3)

第二层：水平分表（按数据量）
order_db_0:
├── orders_0 (order_id % 8 = 0)
├── orders_1 (order_id % 8 = 1)
├── ...
└── orders_7 (order_id % 8 = 7)
```

## MySQL实现细节

### 1. 分区表技术

**RANGE分区（按时间）：**
```sql
CREATE TABLE orders (
    id BIGINT AUTO_INCREMENT,
    user_id BIGINT NOT NULL,
    order_time DATETIME NOT NULL,
    amount DECIMAL(10,2),
    status TINYINT DEFAULT 0,
    PRIMARY KEY (id, order_time),
    INDEX idx_user_id (user_id),
    INDEX idx_status (status)
) PARTITION BY RANGE (YEAR(order_time)*100 + MONTH(order_time)) (
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    PARTITION p202403 VALUES LESS THAN (202404),
    PARTITION p202404 VALUES LESS THAN (202405),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- 自动分区管理
DELIMITER //
CREATE PROCEDURE CreateMonthlyPartition()
BEGIN
    DECLARE partition_name VARCHAR(20);
    DECLARE partition_value INT;
    
    SET partition_name = CONCAT('p', DATE_FORMAT(DATE_ADD(NOW(), INTERVAL 1 MONTH), '%Y%m'));
    SET partition_value = DATE_FORMAT(DATE_ADD(NOW(), INTERVAL 2 MONTH), '%Y%m');
    
    SET @sql = CONCAT('ALTER TABLE orders ADD PARTITION (PARTITION ', partition_name, 
                     ' VALUES LESS THAN (', partition_value, '))');
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
END //
DELIMITER ;

-- 定期执行
CREATE EVENT CreatePartitionEvent
ON SCHEDULE EVERY 1 MONTH
STARTS '2024-01-01 00:00:00'
DO CALL CreateMonthlyPartition();
```

**HASH分区（按ID）：**
```sql
CREATE TABLE user_logs (
    id BIGINT AUTO_INCREMENT,
    user_id BIGINT NOT NULL,
    action VARCHAR(50),
    log_time DATETIME DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (id, user_id),
    INDEX idx_user_time (user_id, log_time)
) PARTITION BY HASH(user_id)
PARTITIONS 16;

-- 查看分区信息
SELECT 
    PARTITION_NAME,
    TABLE_ROWS,
    DATA_LENGTH,
    INDEX_LENGTH
FROM INFORMATION_SCHEMA.PARTITIONS 
WHERE TABLE_NAME = 'user_logs';
```

**LIST分区（按地区）：**
```sql
CREATE TABLE regional_data (
    id BIGINT AUTO_INCREMENT,
    region_code VARCHAR(10),
    data_value TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (id, region_code)
) PARTITION BY LIST COLUMNS(region_code) (
    PARTITION p_north VALUES IN ('BJ', 'TJ', 'HE', 'SX', 'NM'),
    PARTITION p_east VALUES IN ('SH', 'JS', 'ZJ', 'AH', 'FJ', 'JX', 'SD'),
    PARTITION p_south VALUES IN ('GD', 'GX', 'HI'),
    PARTITION p_west VALUES IN ('CQ', 'SC', 'GZ', 'YN', 'XZ', 'SN', 'GS', 'QH', 'NX', 'XJ')
);
```

### 2. 分库分表实现

**数据库连接管理：**
```go
type ShardingDataSource struct {
    dataSources map[string]*sql.DB
    router      ShardingRouter
}

type ShardingRouter interface {
    RouteDatabase(shardingKey string) string
    RouteTable(shardingKey string) string
}

type HashRouter struct {
    dbCount    int
    tableCount int
}

func (r *HashRouter) RouteDatabase(shardingKey string) string {
    hash := crc32.ChecksumIEEE([]byte(shardingKey))
    dbIndex := int(hash) % r.dbCount
    return fmt.Sprintf("db_%d", dbIndex)
}

func (r *HashRouter) RouteTable(shardingKey string) string {
    hash := crc32.ChecksumIEEE([]byte(shardingKey))
    tableIndex := int(hash) % r.tableCount
    return fmt.Sprintf("orders_%d", tableIndex)
}

// 使用示例
func (ds *ShardingDataSource) Insert(userID string, order *Order) error {
    dbName := ds.router.RouteDatabase(userID)
    tableName := ds.router.RouteTable(userID)
    
    db := ds.dataSources[dbName]
    query := fmt.Sprintf("INSERT INTO %s (user_id, amount, status) VALUES (?, ?, ?)", tableName)
    
    _, err := db.Exec(query, order.UserID, order.Amount, order.Status)
    return err
}
```

### 3. 跨分片查询

**分片聚合查询：**
```go
type AggregateResult struct {
    TotalCount int64
    TotalAmount float64
    AvgAmount   float64
}

func (ds *ShardingDataSource) AggregateQuery(startTime, endTime time.Time) (*AggregateResult, error) {
    var wg sync.WaitGroup
    resultChan := make(chan *AggregateResult, len(ds.dataSources))
    errorChan := make(chan error, len(ds.dataSources))
    
    // 并行查询所有分片
    for dbName, db := range ds.dataSources {
        wg.Add(1)
        go func(dbName string, db *sql.DB) {
            defer wg.Done()
            
            result, err := ds.queryShardAggregate(db, startTime, endTime)
            if err != nil {
                errorChan <- err
                return
            }
            resultChan <- result
        }(dbName, db)
    }
    
    wg.Wait()
    close(resultChan)
    close(errorChan)
    
    // 检查错误
    select {
    case err := <-errorChan:
        return nil, err
    default:
    }
    
    // 聚合结果
    finalResult := &AggregateResult{}
    for result := range resultChan {
        finalResult.TotalCount += result.TotalCount
        finalResult.TotalAmount += result.TotalAmount
    }
    
    if finalResult.TotalCount > 0 {
        finalResult.AvgAmount = finalResult.TotalAmount / float64(finalResult.TotalCount)
    }
    
    return finalResult, nil
}

func (ds *ShardingDataSource) queryShardAggregate(db *sql.DB, startTime, endTime time.Time) (*AggregateResult, error) {
    // 查询所有分表
    tables := []string{"orders_0", "orders_1", "orders_2", "orders_3"}
    
    var totalCount int64
    var totalAmount float64
    
    for _, table := range tables {
        query := fmt.Sprintf(`
            SELECT COUNT(*), COALESCE(SUM(amount), 0)
            FROM %s 
            WHERE order_time >= ? AND order_time < ?
        `, table)
        
        var count int64
        var amount float64
        err := db.QueryRow(query, startTime, endTime).Scan(&count, &amount)
        if err != nil {
            return nil, err
        }
        
        totalCount += count
        totalAmount += amount
    }
    
    return &AggregateResult{
        TotalCount:  totalCount,
        TotalAmount: totalAmount,
    }, nil
}
```

### 4. 分布式事务

**两阶段提交（2PC）：**
```go
type DistributedTransaction struct {
    dataSources map[string]*sql.DB
    transactions map[string]*sql.Tx
}

func (dt *DistributedTransaction) Begin() error {
    dt.transactions = make(map[string]*sql.Tx)
    
    for name, db := range dt.dataSources {
        tx, err := db.Begin()
        if err != nil {
            // 回滚已开始的事务
            dt.Rollback()
            return err
        }
        dt.transactions[name] = tx
    }
    
    return nil
}

func (dt *DistributedTransaction) Commit() error {
    // 第一阶段：准备提交
    for name, tx := range dt.transactions {
        _, err := tx.Exec("XA PREPARE ?;", name)
        if err != nil {
            dt.Rollback()
            return err
        }
    }
    
    // 第二阶段：提交
    for name, tx := range dt.transactions {
        err := tx.Commit()
        if err != nil {
            // 记录日志，需要人工干预
            log.Printf("Failed to commit transaction %s: %v", name, err)
            return err
        }
    }
    
    return nil
}

func (dt *DistributedTransaction) Rollback() {
    for _, tx := range dt.transactions {
        tx.Rollback()
    }
}
```

**TCC模式（Try-Confirm-Cancel）：**
```go
type TCCTransaction struct {
    operations []TCCOperation
}

type TCCOperation interface {
    Try() error
    Confirm() error
    Cancel() error
}

type TransferOperation struct {
    fromAccount string
    toAccount   string
    amount      float64
    ds          *ShardingDataSource
}

func (op *TransferOperation) Try() error {
    // 冻结转出账户金额
    return op.ds.FreezeAmount(op.fromAccount, op.amount)
}

func (op *TransferOperation) Confirm() error {
    // 确认转账
    err := op.ds.DeductAmount(op.fromAccount, op.amount)
    if err != nil {
        return err
    }
    return op.ds.AddAmount(op.toAccount, op.amount)
}

func (op *TransferOperation) Cancel() error {
    // 解冻金额
    return op.ds.UnfreezeAmount(op.fromAccount, op.amount)
}

func (tcc *TCCTransaction) Execute() error {
    // Try阶段
    for _, op := range tcc.operations {
        if err := op.Try(); err != nil {
            tcc.cancelAll()
            return err
        }
    }
    
    // Confirm阶段
    for _, op := range tcc.operations {
        if err := op.Confirm(); err != nil {
            tcc.cancelAll()
            return err
        }
    }
    
    return nil
}
```

## 注意事项与坑点

### 1. 分片键选择陷阱

**热点数据问题：**
```go
// 错误示例：按时间分片导致热点
func BadTimeSharding(timestamp time.Time) string {
    // 所有当前数据都写入同一分片
    return fmt.Sprintf("shard_%s", timestamp.Format("2006-01-02"))
}

// 正确示例：组合分片键
func GoodSharding(userID int64, timestamp time.Time) string {
    // 用户ID保证分散，时间用于查询优化
    shardIndex := userID % 16
    return fmt.Sprintf("shard_%d", shardIndex)
}
```

**数据倾斜问题：**
```sql
-- 监控分片数据分布
SELECT 
    table_name,
    table_rows,
    ROUND(data_length/1024/1024, 2) as data_mb,
    ROUND(index_length/1024/1024, 2) as index_mb
FROM information_schema.tables 
WHERE table_schema = 'order_db' 
AND table_name LIKE 'orders_%'
ORDER BY table_rows DESC;

-- 结果分析
-- orders_0: 1,500,000 rows (数据倾斜)
-- orders_1: 800,000 rows
-- orders_2: 900,000 rows
-- orders_3: 750,000 rows
```

### 2. 跨分片查询复杂性

**JOIN查询限制：**
```sql
-- 无法执行的跨分片JOIN
SELECT u.username, o.amount
FROM users u
JOIN orders o ON u.id = o.user_id
WHERE o.order_time >= '2024-01-01';

-- 解决方案1：应用层JOIN
-- 1. 先查询orders表获取user_id列表
-- 2. 再查询users表获取用户信息
-- 3. 应用层组装数据

-- 解决方案2：数据冗余
CREATE TABLE orders (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    username VARCHAR(50), -- 冗余用户名
    amount DECIMAL(10,2),
    order_time DATETIME
);
```

**分页查询问题：**
```go
// 错误的分页实现
func BadPagination(page, size int) ([]Order, error) {
    var allOrders []Order
    
    // 从每个分片查询
    for _, db := range dataSources {
        orders, err := queryFromShard(db, page, size)
        if err != nil {
            return nil, err
        }
        allOrders = append(allOrders, orders...)
    }
    
    // 问题：数据量不准确，排序错误
    return allOrders, nil
}

// 正确的分页实现
func GoodPagination(page, size int) ([]Order, error) {
    // 1. 从每个分片查询更多数据
    shardSize := size * len(dataSources)
    
    var allOrders []Order
    for _, db := range dataSources {
        orders, err := queryFromShard(db, 1, shardSize)
        if err != nil {
            return nil, err
        }
        allOrders = append(allOrders, orders...)
    }
    
    // 2. 全局排序
    sort.Slice(allOrders, func(i, j int) bool {
        return allOrders[i].OrderTime.After(allOrders[j].OrderTime)
    })
    
    // 3. 分页截取
    start := (page - 1) * size
    end := start + size
    if start >= len(allOrders) {
        return []Order{}, nil
    }
    if end > len(allOrders) {
        end = len(allOrders)
    }
    
    return allOrders[start:end], nil
}
```

### 3. 数据一致性问题

**分布式事务性能问题：**
```go
// 避免分布式事务的设计
type OrderService struct {
    orderDB   *sql.DB
    accountDB *sql.DB
    stockDB   *sql.DB
}

// 错误：强一致性要求导致分布式事务
func (s *OrderService) CreateOrderWithTransaction(order *Order) error {
    // 需要跨3个数据库的事务
    // 1. 扣减库存 (stockDB)
    // 2. 扣减余额 (accountDB)
    // 3. 创建订单 (orderDB)
    
    // 分布式事务复杂且性能差
    return s.distributedTransaction(order)
}

// 正确：最终一致性设计
func (s *OrderService) CreateOrderEventually(order *Order) error {
    // 1. 先创建订单（状态：待支付）
    err := s.createPendingOrder(order)
    if err != nil {
        return err
    }
    
    // 2. 异步处理库存和支付
    s.publishOrderCreatedEvent(order)
    
    return nil
}

func (s *OrderService) handleOrderCreatedEvent(order *Order) {
    // 异步处理，允许重试
    if err := s.reserveStock(order); err != nil {
        s.retryLater(order, "reserve_stock")
        return
    }
    
    if err := s.processPayment(order); err != nil {
        s.releaseStock(order)
        s.retryLater(order, "process_payment")
        return
    }
    
    // 更新订单状态为已支付
    s.updateOrderStatus(order.ID, "paid")
}
```

### 4. 扩容缩容问题

**数据迁移复杂性：**
```go
// 扩容方案：双写+迁移
type MigrationManager struct {
    oldShards []ShardInfo
    newShards []ShardInfo
    migrating bool
}

func (m *MigrationManager) Write(key string, data interface{}) error {
    if !m.migrating {
        // 正常写入
        return m.writeToShard(m.getOldShard(key), data)
    }
    
    // 迁移期间双写
    oldShard := m.getOldShard(key)
    newShard := m.getNewShard(key)
    
    // 先写新分片
    if err := m.writeToShard(newShard, data); err != nil {
        return err
    }
    
    // 再写旧分片（允许失败）
    m.writeToShard(oldShard, data)
    
    return nil
}

func (m *MigrationManager) Read(key string) (interface{}, error) {
    if !m.migrating {
        return m.readFromShard(m.getOldShard(key), key)
    }
    
    // 迁移期间优先读新分片
    newShard := m.getNewShard(key)
    data, err := m.readFromShard(newShard, key)
    if err == nil {
        return data, nil
    }
    
    // 新分片没有则读旧分片
    oldShard := m.getOldShard(key)
    return m.readFromShard(oldShard, key)
}
```

### 5. 运维复杂性

**监控指标：**
```sql
-- 分片性能监控
SELECT 
    table_schema,
    table_name,
    table_rows,
    ROUND(data_length/1024/1024, 2) as data_mb,
    ROUND(index_length/1024/1024, 2) as index_mb,
    ROUND((data_length + index_length)/1024/1024, 2) as total_mb
FROM information_schema.tables 
WHERE table_schema LIKE '%_db_%'
ORDER BY total_mb DESC;

-- 慢查询监控
SELECT 
    db,
    sql_text,
    exec_count,
    avg_timer_wait/1000000000 as avg_time_sec,
    max_timer_wait/1000000000 as max_time_sec
FROM performance_schema.events_statements_summary_by_digest
WHERE avg_timer_wait > 1000000000 -- 超过1秒
ORDER BY avg_timer_wait DESC
LIMIT 10;
```

**备份策略：**
```bash
#!/bin/bash
# 分片备份脚本

SHARD_COUNT=4
BACKUP_DIR="/backup/$(date +%Y%m%d)"

mkdir -p $BACKUP_DIR

# 并行备份所有分片
for i in $(seq 0 $((SHARD_COUNT-1))); do
    {
        echo "Backing up shard $i..."
        mysqldump -h db_$i.example.com -u backup_user -p$BACKUP_PASSWORD \
            --single-transaction \
            --routines \
            --triggers \
            order_db_$i > $BACKUP_DIR/order_db_$i.sql
        
        if [ $? -eq 0 ]; then
            echo "Shard $i backup completed"
            gzip $BACKUP_DIR/order_db_$i.sql
        else
            echo "Shard $i backup failed"
        fi
    } &
done

wait
echo "All shards backup completed"
```

## 高频面试题

### 1. 基础概念题

**Q1: 什么是分库分表？为什么需要分库分表？**

**答案：**
分库分表是一种数据库水平扩展技术，通过将数据分散到多个数据库实例或表中来解决单库单表的性能瓶颈。

**需要分库分表的原因：**
1. **数据量过大**：单表超过千万级别，查询性能下降
2. **并发压力**：单库QPS/TPS达到瓶颈
3. **存储限制**：单库容量接近上限
4. **可用性要求**：避免单点故障

**Q2: 垂直分库和水平分库的区别是什么？**

**答案：**

| 维度 | 垂直分库 | 水平分库 |
|------|----------|----------|
| **分割方式** | 按业务模块分割 | 按数据特征分割 |
| **数据分布** | 不同业务在不同库 | 同业务数据分散到多库 |
| **查询特点** | 减少跨库查询 | 需要路由到正确分片 |
| **扩展性** | 受业务模块限制 | 可无限水平扩展 |
| **复杂度** | 相对简单 | 路由和聚合复杂 |

**Q3: 常见的分片算法有哪些？各有什么优缺点？**

**答案：**

1. **哈希取模算法**
   - 优点：分布均匀、实现简单
   - 缺点：扩容困难、无法范围查询

2. **范围分片算法**
   - 优点：支持范围查询、扩容相对容易
   - 缺点：可能数据倾斜、热点问题

3. **一致性哈希算法**
   - 优点：扩容影响小、负载均衡好
   - 缺点：实现复杂、可能数据倾斜

4. **目录映射算法**
   - 优点：灵活性高、支持复杂路由
   - 缺点：需要额外存储、单点风险

### 2. 技术实现题

**Q4: 如何解决分库分表后的跨分片查询问题？**

**答案：**

1. **应用层聚合**
   ```go
   func CrossShardQuery(condition QueryCondition) ([]Result, error) {
       var results []Result
       var wg sync.WaitGroup
       resultChan := make(chan []Result, len(shards))
       
       // 并行查询所有分片
       for _, shard := range shards {
           wg.Add(1)
           go func(s Shard) {
               defer wg.Done()
               result := s.Query(condition)
               resultChan <- result
           }(shard)
       }
       
       wg.Wait()
       close(resultChan)
       
       // 聚合结果
       for result := range resultChan {
           results = append(results, result...)
       }
       
       return results, nil
   }
   ```

2. **数据冗余**
   - 在需要JOIN的表中冗余关联字段
   - 使用宽表设计减少关联查询

3. **搜索引擎**
   - 使用Elasticsearch等搜索引擎
   - 异步同步数据到搜索引擎

**Q5: 分库分表后如何保证数据一致性？**

**答案：**

1. **避免分布式事务**
   - 通过业务设计避免跨分片事务
   - 使用最终一致性代替强一致性

2. **分布式事务方案**
   ```go
   // TCC模式示例
   type TCCManager struct {
       operations []TCCOperation
   }
   
   func (m *TCCManager) Execute() error {
       // Try阶段
       for _, op := range m.operations {
           if err := op.Try(); err != nil {
               m.cancelAll()
               return err
           }
       }
       
       // Confirm阶段
       for _, op := range m.operations {
           if err := op.Confirm(); err != nil {
               // 需要补偿机制
               return err
           }
       }
       
       return nil
   }
   ```

3. **消息队列保证最终一致性**
   - 使用可靠消息投递
   - 实现幂等性处理
   - 补偿机制处理异常

### 3. 架构设计题

**Q6: 设计一个电商订单系统的分库分表方案**

**答案：**

**业务分析：**
- 订单数据量大，增长快
- 查询模式：按用户查询、按时间范围查询
- 写入频繁，读取也频繁

**分片策略：**
```
1. 垂直分库：
   - user_db：用户相关数据
   - order_db：订单相关数据
   - product_db：商品相关数据
   - payment_db：支付相关数据

2. 水平分库分表：
   - 分片键：user_id（保证用户相关数据在同一分片）
   - 分库：4个order_db实例
   - 分表：每库8张orders表
   - 路由算法：user_id % 4 确定库，order_id % 8 确定表

3. 数据分布：
   order_db_0:
   ├── orders_0, orders_1, ..., orders_7
   ├── order_items_0, order_items_1, ..., order_items_7
   └── order_logs_0, order_logs_1, ..., order_logs_7
```

**技术架构：**
```go
type OrderShardingService struct {
    dataSources map[string]*sql.DB
    router      *OrderRouter
}

type OrderRouter struct {
    dbCount    int
    tableCount int
}

func (r *OrderRouter) Route(userID int64) (string, string) {
    dbIndex := userID % int64(r.dbCount)
    tableIndex := userID % int64(r.tableCount)
    
    dbName := fmt.Sprintf("order_db_%d", dbIndex)
    tableName := fmt.Sprintf("orders_%d", tableIndex)
    
    return dbName, tableName
}
```

**Q7: 如何设计分库分表的扩容方案？**

**答案：**

**扩容策略：**
1. **倍数扩容**：从N个分片扩容到2N个分片
2. **数据迁移**：只需迁移50%的数据
3. **双写方案**：保证服务不中断

**实施步骤：**
```
1. 准备阶段：
   - 部署新的数据库实例
   - 创建新的分片表结构
   - 配置数据同步工具

2. 迁移阶段：
   - 开启双写模式（新旧分片同时写入）
   - 历史数据迁移（按分片键重新路由）
   - 数据一致性校验

3. 切换阶段：
   - 更新路由配置
   - 停止旧分片写入
   - 验证新分片数据完整性

4. 清理阶段：
   - 删除旧分片冗余数据
   - 回收旧分片资源
```

**双写实现：**
```go
type MigrationWriter struct {
    oldRouter ShardingRouter
    newRouter ShardingRouter
    migrating bool
}

func (w *MigrationWriter) Write(key string, data interface{}) error {
    if !w.migrating {
        return w.writeToOldShard(key, data)
    }
    
    // 双写模式
    newShard := w.newRouter.Route(key)
    if err := w.writeToShard(newShard, data); err != nil {
        return err
    }
    
    oldShard := w.oldRouter.Route(key)
    w.writeToShard(oldShard, data) // 允许失败
    
    return nil
}
```

### 4. 性能优化题

**Q8: 分库分表后如何优化查询性能？**

**答案：**

1. **索引优化**
   ```sql
   -- 分片键必须是索引的前缀
   CREATE INDEX idx_user_time ON orders(user_id, order_time);
   
   -- 覆盖索引减少回表
   CREATE INDEX idx_user_status_amount ON orders(user_id, status, amount);
   ```

2. **查询路由优化**
   ```go
   func OptimizedQuery(userID int64, status string) ([]Order, error) {
       // 精确路由到单个分片
       dbName, tableName := router.Route(userID)
       db := dataSources[dbName]
       
       query := fmt.Sprintf(
           "SELECT * FROM %s WHERE user_id = ? AND status = ?",
           tableName)
       
       return queryOrders(db, query, userID, status)
   }
   ```

3. **缓存策略**
   ```go
   func QueryWithCache(userID int64) ([]Order, error) {
       // 先查缓存
       cacheKey := fmt.Sprintf("user_orders:%d", userID)
       if orders := cache.Get(cacheKey); orders != nil {
           return orders.([]Order), nil
       }
       
       // 缓存未命中，查询数据库
       orders, err := queryFromDB(userID)
       if err != nil {
           return nil, err
       }
       
       // 写入缓存
       cache.Set(cacheKey, orders, 5*time.Minute)
       return orders, nil
   }
   ```

4. **读写分离**
   ```go
   type ShardCluster struct {
       master *sql.DB
       slaves []*sql.DB
   }
   
   func (c *ShardCluster) Read(query string, args ...interface{}) (*sql.Rows, error) {
       // 负载均衡选择从库
       slave := c.slaves[rand.Intn(len(c.slaves))]
       return slave.Query(query, args...)
   }
   
   func (c *ShardCluster) Write(query string, args ...interface{}) (sql.Result, error) {
       // 写入主库
       return c.master.Exec(query, args...)
   }
   ```

### 5. 故障处理题

**Q9: 分库分表环境下如何处理单个分片故障？**

**答案：**

1. **故障检测**
   ```go
   type HealthChecker struct {
       shards map[string]*ShardInfo
       mutex  sync.RWMutex
   }
   
   func (hc *HealthChecker) CheckHealth() {
       for name, shard := range hc.shards {
           go func(name string, shard *ShardInfo) {
               if err := shard.Ping(); err != nil {
                   hc.markUnhealthy(name)
                   log.Printf("Shard %s is unhealthy: %v", name, err)
               } else {
                   hc.markHealthy(name)
               }
           }(name, shard)
       }
   }
   ```

2. **故障转移**
   ```go
   func (ds *ShardingDataSource) Query(shardKey string, query string) (*sql.Rows, error) {
       primaryShard := ds.router.Route(shardKey)
       
       // 尝试主分片
       if ds.isHealthy(primaryShard) {
           return ds.queryFromShard(primaryShard, query)
       }
       
       // 主分片故障，尝试备份分片
       backupShard := ds.getBackupShard(primaryShard)
       if backupShard != "" && ds.isHealthy(backupShard) {
           return ds.queryFromShard(backupShard, query)
       }
       
       return nil, errors.New("all shards for this key are unavailable")
   }
   ```

3. **数据恢复**
   ```bash
   # 从备份恢复分片
   #!/bin/bash
   FAILED_SHARD="order_db_2"
   BACKUP_FILE="/backup/order_db_2_20240115.sql.gz"
   
   # 1. 停止应用对该分片的写入
   echo "Marking shard $FAILED_SHARD as readonly"
   
   # 2. 恢复数据
   echo "Restoring shard $FAILED_SHARD from backup"
   gunzip < $BACKUP_FILE | mysql -h new_db_host -u admin -p $FAILED_SHARD
   
   # 3. 同步增量数据（从binlog）
   echo "Syncing incremental data"
   mysqlbinlog --start-datetime="2024-01-15 10:00:00" \
               --database=$FAILED_SHARD \
               /var/log/mysql/mysql-bin.000001 | \
               mysql -h new_db_host -u admin -p $FAILED_SHARD
   
   # 4. 验证数据一致性
   echo "Verifying data consistency"
   
   # 5. 恢复服务
   echo "Marking shard $FAILED_SHARD as healthy"
   ```

## 实战场景分析

### 场景1：电商订单系统

**业务特点：**
- 订单数据量：日增100万+
- 查询模式：用户查询自己订单、商家查询店铺订单
- 峰值特点：促销期间写入量暴增

**分片方案：**
```yaml
# 分片配置
sharding:
  databases:
    order_db:
      shardingColumn: user_id
      algorithmExpression: order_db_${user_id % 8}
      
  tables:
    orders:
      actualDataNodes: order_db_${0..7}.orders_${0..15}
      databaseStrategy:
        shardingColumn: user_id
        algorithmExpression: order_db_${user_id % 8}
      tableStrategy:
        shardingColumn: order_id
        algorithmExpression: orders_${order_id % 16}
        
    order_items:
      actualDataNodes: order_db_${0..7}.order_items_${0..15}
      databaseStrategy:
        shardingColumn: user_id
        algorithmExpression: order_db_${user_id % 8}
      tableStrategy:
        shardingColumn: order_id
        algorithmExpression: order_items_${order_id % 16}
```

**Go实现：**
```go
type OrderService struct {
    shardingDS *ShardingDataSource
    cache      *redis.Client
}

// 创建订单
func (s *OrderService) CreateOrder(ctx context.Context, order *Order) error {
    // 生成分布式ID
    order.ID = s.generateOrderID()
    
    // 开启事务
    tx, err := s.shardingDS.BeginTx(ctx, order.UserID)
    if err != nil {
        return err
    }
    defer tx.Rollback()
    
    // 插入订单主表
    err = s.insertOrder(ctx, tx, order)
    if err != nil {
        return err
    }
    
    // 插入订单明细
    for _, item := range order.Items {
        err = s.insertOrderItem(ctx, tx, item)
        if err != nil {
            return err
        }
    }
    
    // 提交事务
    if err = tx.Commit(); err != nil {
        return err
    }
    
    // 异步更新缓存
    go s.updateOrderCache(order)
    
    return nil
}

// 查询用户订单
func (s *OrderService) GetUserOrders(ctx context.Context, userID int64, page, size int) ([]Order, error) {
    // 先查缓存
    cacheKey := fmt.Sprintf("user_orders:%d:%d:%d", userID, page, size)
    if cached := s.cache.Get(ctx, cacheKey).Val(); cached != "" {
        var orders []Order
        json.Unmarshal([]byte(cached), &orders)
        return orders, nil
    }
    
    // 缓存未命中，查询数据库
    orders, err := s.queryUserOrders(ctx, userID, page, size)
    if err != nil {
        return nil, err
    }
    
    // 写入缓存
    data, _ := json.Marshal(orders)
    s.cache.Set(ctx, cacheKey, data, 5*time.Minute)
    
    return orders, nil
}
```

### 场景2：金融交易系统

**业务特点：**
- 交易数据：强一致性要求
- 查询模式：按用户、按时间、按交易类型
- 监管要求：数据不可篡改、完整审计

**分片策略：**
```go
// 按用户+时间双维度分片
type FinanceRouter struct {
    userShardCount int
    timeShardCount int
}

func (r *FinanceRouter) Route(userID int64, timestamp time.Time) (string, string) {
    // 用户维度分库
    userShard := userID % int64(r.userShardCount)
    
    // 时间维度分表
    timeKey := timestamp.Format("200601") // 按月分表
    
    dbName := fmt.Sprintf("finance_db_%d", userShard)
    tableName := fmt.Sprintf("transactions_%s", timeKey)
    
    return dbName, tableName
}
```

**事务处理：**
```go
type TransactionService struct {
    shardingDS *ShardingDataSource
    sagaManager *SagaManager
}

// 转账操作（Saga模式）
func (s *TransactionService) Transfer(ctx context.Context, req *TransferRequest) error {
    saga := s.sagaManager.NewSaga()
    
    // 步骤1：冻结转出账户金额
    saga.AddStep(
        func() error { return s.freezeAmount(req.FromAccount, req.Amount) },
        func() error { return s.unfreezeAmount(req.FromAccount, req.Amount) },
    )
    
    // 步骤2：扣减转出账户
    saga.AddStep(
        func() error { return s.deductAmount(req.FromAccount, req.Amount) },
        func() error { return s.addAmount(req.FromAccount, req.Amount) },
    )
    
    // 步骤3：增加转入账户
    saga.AddStep(
        func() error { return s.addAmount(req.ToAccount, req.Amount) },
        func() error { return s.deductAmount(req.ToAccount, req.Amount) },
    )
    
    // 步骤4：记录交易流水
    saga.AddStep(
        func() error { return s.recordTransaction(req) },
        func() error { return s.cancelTransaction(req.TransactionID) },
    )
    
    return saga.Execute(ctx)
}
```

### 场景3：日志分析系统

**业务特点：**
- 数据量：TB级别日志数据
- 写入：高并发写入，批量导入
- 查询：按时间范围、按关键字搜索

**分片策略：**
```sql
-- 按时间分表（天级别）
CREATE TABLE access_logs_20240115 (
    id BIGINT AUTO_INCREMENT,
    user_id BIGINT,
    ip_address VARCHAR(45),
    user_agent TEXT,
    request_url VARCHAR(500),
    response_code INT,
    response_time INT,
    log_time DATETIME,
    PRIMARY KEY (id, log_time),
    INDEX idx_user_time (user_id, log_time),
    INDEX idx_ip_time (ip_address, log_time),
    INDEX idx_response_code (response_code)
) PARTITION BY RANGE (TO_DAYS(log_time)) (
    PARTITION p20240115 VALUES LESS THAN (TO_DAYS('2024-01-16')),
    PARTITION p20240116 VALUES LESS THAN (TO_DAYS('2024-01-17')),
    PARTITION p20240117 VALUES LESS THAN (TO_DAYS('2024-01-18'))
);
```

**批量写入优化：**
```go
type LogService struct {
    shardingDS *ShardingDataSource
    buffer     *LogBuffer
}

type LogBuffer struct {
    logs   []AccessLog
    mutex  sync.Mutex
    ticker *time.Ticker
}

// 异步批量写入
func (s *LogService) WriteLog(log *AccessLog) {
    s.buffer.Add(log)
}

func (b *LogBuffer) Add(log *AccessLog) {
    b.mutex.Lock()
    defer b.mutex.Unlock()
    
    b.logs = append(b.logs, *log)
    
    // 达到批量大小或时间间隔，触发写入
    if len(b.logs) >= 1000 {
        go b.flush()
    }
}

func (b *LogBuffer) flush() {
    b.mutex.Lock()
    logs := make([]AccessLog, len(b.logs))
    copy(logs, b.logs)
    b.logs = b.logs[:0] // 清空缓冲区
    b.mutex.Unlock()
    
    // 按分片分组
    shardGroups := make(map[string][]AccessLog)
    for _, log := range logs {
        shard := b.getShardKey(log.LogTime)
        shardGroups[shard] = append(shardGroups[shard], log)
    }
    
    // 并行写入各分片
    var wg sync.WaitGroup
    for shard, shardLogs := range shardGroups {
        wg.Add(1)
        go func(shard string, logs []AccessLog) {
            defer wg.Done()
            b.batchInsert(shard, logs)
        }(shard, shardLogs)
    }
    wg.Wait()
}
```

## Go语言实现

### 1. Go语言分库分表中间件选型

在Go语言生态中，目前没有像Java生态中ShardingSphere那样成熟且功能全面的分库分表中间件。通常需要结合以下方式实现：

- **自研路由层：** 在应用层实现分片键解析、路由计算、SQL改写和结果归并。
- **数据库连接池管理：** 使用Go原生的 `database/sql` 结合连接池管理，或者使用第三方库如 `gorm`、`xorm` 等ORM框架。
- **ShardingSphere-Proxy：** 如果团队有Java背景或希望使用成熟的代理层方案，可以部署 ShardingSphere-Proxy，Go应用通过标准MySQL协议连接。
- **Vitess：** 对于超大规模的MySQL集群，Vitess 是一个云原生的分库分表解决方案，但引入成本较高。

本节主要关注自研路由层的实现思路。

### 2. 连接管理与路由实现

```go
package sharding

import (
	"database/sql"
	"fmt"
	"hash/crc32"
	"log"
	"sync"

	_ "github.com/go-sql-driver/mysql" // MySQL driver
)

// ShardConfig defines the sharding topology
type ShardConfig struct {
	DBCount    int                 // Total number of sharded databases
	TableCount int                 // Total number of sharded tables per database
	DSNMap     map[string]string // Map of logical DB name to DSN
	DBPools    map[string]*sql.DB // Connection pools for each database
	mu         sync.RWMutex      // Mutex for protecting DBPools
}

// NewShardConfig initializes a new ShardConfig
func NewShardConfig(dbCount, tableCount int, dsnMap map[string]string) (*ShardConfig, error) {
	sc := &ShardConfig{
		DBCount:    dbCount,
		TableCount: tableCount,
		DSNMap:     dsnMap,
		DBPools:    make(map[string]*sql.DB),
	}

	// Initialize connection pools for all databases
	for dbName, dsn := range dsnMap {
		db, err := sql.Open("mysql", dsn)
		if err != nil {
			return nil, fmt.Errorf("failed to open DB %s: %w", dbName, err)
		}
		db.SetMaxOpenConns(100) // Max open connections
		db.SetMaxIdleConns(10)  // Max idle connections
		db.SetConnMaxLifetime(5 * time.Minute) // Connection max lifetime
		sc.DBPools[dbName] = db
		log.Printf("Initialized DB pool for %s\n", dbName)
	}
	return sc, nil
}

// Close closes all database connections
func (sc *ShardConfig) Close() {
	sc.mu.Lock()
	defer sc.mu.Unlock()
	for dbName, db := range sc.DBPools {
		if err := db.Close(); err != nil {
			log.Printf("Error closing DB %s: %v\n", dbName, err)
		}
	}
}

// RouteDB calculates the database index based on sharding key
func (sc *ShardConfig) RouteDB(shardingKey string) string {
	hash := crc32.ChecksumIEEE([]byte(shardingKey))
	dbIndex := int(hash) % sc.DBCount
	return fmt.Sprintf("db_%d", dbIndex) // Assuming logical DB names are db_0, db_1, ...
}

// RouteTable calculates the table index based on sharding key
func (sc *ShardConfig) RouteTable(shardingKey string) string {
	hash := crc32.ChecksumIEEE([]byte(shardingKey))
	tableIndex := int(hash) % sc.TableCount
	return fmt.Sprintf("orders_%d", tableIndex) // Assuming table names are orders_0, orders_1, ...
}

// GetDB retrieves a database connection pool for a given sharding key
func (sc *ShardConfig) GetDB(shardingKey string) (*sql.DB, error) {
	dbName := sc.RouteDB(shardingKey)
	sc.mu.RLock()
	defer sc.mu.RUnlock()
	db, ok := sc.DBPools[dbName]
	if !ok {
		return nil, fmt.Errorf("database pool not found for %s", dbName)
	}
	return db, nil
}

// GetActualTableName retrieves the actual table name for a given sharding key
func (sc *ShardConfig) GetActualTableName(baseTableName, shardingKey string) string {
	return fmt.Sprintf("%s_%s", baseTableName, sc.RouteTable(shardingKey))
}
```

### 3. CRUD操作示例

```go
package sharding

import (
	"database/sql"
	"fmt"
	"time"
)

// Order represents a simplified order struct
type Order struct {
	ID        int64
	UserID    int64
	Amount    float64
	Status    int
	CreatedAt time.Time
}

// OrderRepository handles CRUD operations for Order
type OrderRepository struct {
	sc *ShardConfig
}

// NewOrderRepository creates a new OrderRepository
func NewOrderRepository(sc *ShardConfig) *OrderRepository {
	return &OrderRepository{sc: sc}
}

// CreateOrder inserts a new order into the sharded database
func (r *OrderRepository) CreateOrder(order *Order) error {
	db, err := r.sc.GetDB(fmt.Sprintf("%d", order.UserID))
	if err != nil {
		return err
	}
	tableName := r.sc.GetActualTableName("orders", fmt.Sprintf("%d", order.ID))

	query := fmt.Sprintf("INSERT INTO %s (id, user_id, amount, status, created_at) VALUES (?, ?, ?, ?, ?)", tableName)
	stmt, err := db.Prepare(query)
	if err != nil {
		return fmt.Errorf("failed to prepare statement: %w", err)
	}
	defer stmt.Close()

	_, err = stmt.Exec(order.ID, order.UserID, order.Amount, order.Status, order.CreatedAt)
	if err != nil {
		return fmt.Errorf("failed to execute insert: %w", err)
	}
	return nil
}

// GetOrderByID retrieves an order by its ID and UserID (requires sharding key for routing)
func (r *OrderRepository) GetOrderByID(orderID, userID int64) (*Order, error) {
	db, err := r.sc.GetDB(fmt.Sprintf("%d", userID))
	if err != nil {
		return nil, err
	}
	tableName := r.sc.GetActualTableName("orders", fmt.Sprintf("%d", orderID))

	query := fmt.Sprintf("SELECT id, user_id, amount, status, created_at FROM %s WHERE id = ? AND user_id = ?", tableName)
	row := db.QueryRow(query, orderID, userID)

	order := &Order{}
	err = row.Scan(&order.ID, &order.UserID, &order.Amount, &order.Status, &order.CreatedAt)
	if err != nil {
		if err == sql.ErrNoRows {
			return nil, fmt.Errorf("order not found")
		}
		return nil, fmt.Errorf("failed to scan order: %w", err)
	}
	return order, nil
}

// UpdateOrderStatus updates the status of an order
func (r *OrderRepository) UpdateOrderStatus(orderID, userID int64, status int) error {
	db, err := r.sc.GetDB(fmt.Sprintf("%d", userID))
	if err != nil {
		return err
	}
	tableName := r.sc.GetActualTableName("orders", fmt.Sprintf("%d", orderID))

	query := fmt.Sprintf("UPDATE %s SET status = ? WHERE id = ? AND user_id = ?", tableName)
	stmt, err := db.Prepare(query)
	if err != nil {
		return fmt.Errorf("failed to prepare statement: %w", err)
	}
	defer stmt.Close()

	res, err := stmt.Exec(status, orderID, userID)
	if err != nil {
		return fmt.Errorf("failed to execute update: %w", err)
	}
	rowsAffected, _ := res.RowsAffected()
	if rowsAffected == 0 {
		return fmt.Errorf("order %d not found or no changes made", orderID)
	}
	return nil
}

// DeleteOrder deletes an order
func (r *OrderRepository) DeleteOrder(orderID, userID int64) error {
	db, err := r.sc.GetDB(fmt.Sprintf("%d", userID))
	if err != nil {
		return err
	}
	tableName := r.sc.GetActualTableName("orders", fmt.Sprintf("%d", orderID))

	query := fmt.Sprintf("DELETE FROM %s WHERE id = ? AND user_id = ?", tableName)
	stmt, err := db.Prepare(query)
	if err != nil {
		return fmt.Errorf("failed to prepare statement: %w", err)
	}
	defer stmt.Close()

	res, err := stmt.Exec(orderID, userID)
	if err != nil {
		return fmt.Errorf("failed to execute delete: %w", err)
	}
	rowsAffected, _ := res.RowsAffected()
	if rowsAffected == 0 {
		return fmt.Errorf("order %d not found", orderID)
	}
	return nil
}

// AggregateOrdersByUserID aggregates orders for a specific user across all their tables
func (r *OrderRepository) AggregateOrdersByUserID(userID int64) (int64, float64, error) {
	db, err := r.sc.GetDB(fmt.Sprintf("%d", userID))
	if err != nil {
		return 0, 0, err
	}

	var totalCount int64
	var totalAmount float64

	// Iterate through all possible tables for this user's DB shard
	for i := 0; i < r.sc.TableCount; i++ {
		tableName := fmt.Sprintf("orders_%d", i)
		query := fmt.Sprintf("SELECT COUNT(*), COALESCE(SUM(amount), 0) FROM %s WHERE user_id = ?", tableName)
		
		var count int64
		var amount float64
		err = db.QueryRow(query, userID).Scan(&count, &amount)
		if err != nil {
			return 0, 0, fmt.Errorf("failed to query aggregate for table %s: %w", tableName, err)
		}
		totalCount += count
		totalAmount += amount
	}
	return totalCount, totalAmount, nil
}

// GlobalAggregateOrders aggregates orders across all databases and tables (expensive operation)
func (r *OrderRepository) GlobalAggregateOrders() (int64, float64, error) {
	var totalCount int64
	var totalAmount float64
	var wg sync.WaitGroup
	resultChan := make(chan struct{ Count int64; Amount float64 }, r.sc.DBCount)
	errorChan := make(chan error, r.sc.DBCount)

	for dbName, db := range r.sc.DBPools {
		wg.Add(1)
		go func(db *sql.DB, dbName string) {
			defer wg.Done()
			var dbCount int64
			var dbAmount float64
			for i := 0; i < r.sc.TableCount; i++ {
				tableName := fmt.Sprintf("orders_%d", i)
				query := fmt.Sprintf("SELECT COUNT(*), COALESCE(SUM(amount), 0) FROM %s", tableName)
				var count int64
				var amount float64
				err := db.QueryRow(query).Scan(&count, &amount)
				if err != nil {
					errorChan <- fmt.Errorf("failed to query global aggregate for %s.%s: %w", dbName, tableName, err)
					return
				}
				dbCount += count
				dbAmount += amount
			}
			resultChan <- struct{ Count int64; Amount float64 }{Count: dbCount, Amount: dbAmount}
		}(db, dbName)
	}

	wg.Wait()
	close(resultChan)
	close(errorChan)

	select {
	case err := <-errorChan:
		return 0, 0, err
	default:
		// No error
	}

	for res := range resultChan {
		totalCount += res.Count
		totalAmount += res.Amount
	}
	return totalCount, totalAmount, nil
}
```

### 4. 分布式事务与全局唯一ID

**分布式事务：**

在分库分表场景下，跨库事务是复杂且难以避免的问题。Go语言中通常采用以下策略：

- **最终一致性：** 大多数互联网应用会选择最终一致性，通过消息队列（如Kafka, RabbitMQ）或事务消息（如RocketMQ事务消息）来实现。例如，订单创建后发送消息，库存服务消费消息扣减库存。
  - **优点：** 性能高，系统吞吐量大。
  - **缺点：** 业务需要处理数据不一致的中间状态，对业务侵入性强。
- **TCC (Try-Confirm-Cancel) 模式：** 适用于对一致性要求较高，但又不能接受2PC性能损耗的场景。业务层面需要实现Try、Confirm、Cancel三个操作。
  - **优点：** 解决了2PC的阻塞问题，业务侵入性适中。
  - **缺点：** 业务开发复杂，需要为每个操作实现TCC接口。
- **SAGA 模式：** 一系列本地事务的组合，每个本地事务都有一个对应的补偿操作。当某个本地事务失败时，通过执行之前已成功事务的补偿操作来回滚。
  - **优点：** 解决了长事务问题，无需锁定资源。
  - **缺点：** 补偿逻辑复杂，需要保证补偿操作的幂等性。

**全局唯一ID生成：**

分库分表后，数据库的自增ID无法保证全局唯一性。常见的解决方案有：

- **UUID：** 简单易用，但无序，作为主键会影响数据库索引性能。
- **Snowflake算法：** Twitter开源的分布式ID生成算法，生成的是趋势递增的64位整数ID，包含时间戳、机器ID、序列号等信息，适用于分布式环境。
  - **Go语言实现示例：**
    ```go
    package main
    
    import (
    	"fmt"
    	"sync"
    	"time"
    )
    
    const (
    	workerIDBits     = uint(5)  // 机器ID占5位
    	dataCenterIDBits = uint(5)  // 数据中心ID占5位
    	sequenceBits     = uint(12) // 序列号占12位
    
    	maxWorkerID     = int64(-1) ^ (int64(-1) << workerIDBits)
    	maxDataCenterID = int64(-1) ^ (int64(-1) << dataCenterIDBits)
    	maxSequence     = int64(-1) ^ (int64(-1) << sequenceBits)
    
    	workerIDShift     = sequenceBits
    	dataCenterIDShift = sequenceBits + workerIDBits
    	timestampShift    = sequenceBits + workerIDBits + dataCenterIDBits
    
    	epoch = int64(1672531200000) // 2023-01-01 00:00:00 UTC in milliseconds
    )
    
    // Snowflake struct
    type Snowflake struct {
    	mu           sync.Mutex
    	lastTimestamp int64
    	workerID      int64
    	dataCenterID  int64
    	sequence      int64
    }
    
    // NewSnowflake creates a new Snowflake instance
    func NewSnowflake(workerID, dataCenterID int64) (*Snowflake, error) {
    	if workerID < 0 || workerID > maxWorkerID {
    		return nil, fmt.Errorf("worker ID must be between 0 and %d", maxWorkerID)
    	}
    	if dataCenterID < 0 || dataCenterID > maxDataCenterID {
    		return nil, fmt.Errorf("data center ID must be between 0 and %d", maxDataCenterID)
    	}
    	return &Snowflake{
    		workerID:     workerID,
    		dataCenterID: dataCenterID,
    		lastTimestamp: -1,
    		sequence:     0,
    	}, nil
    }
    
    // Generate generates a unique ID
    func (sf *Snowflake) Generate() (int64, error) {
    	sf.mu.Lock()
    	defer sf.mu.Unlock()
    
    	now := time.Now().UnixNano() / 1e6 // Milliseconds
    
    	if now < sf.lastTimestamp {
    		return 0, fmt.Errorf("clock moved backwards. Refusing to generate id for %d milliseconds", sf.lastTimestamp-now)
    	}
    
    	if now == sf.lastTimestamp {
    		sf.sequence = (sf.sequence + 1) & maxSequence
    		if sf.sequence == 0 {
    			// Sequence exhausted, wait until next millisecond
    			for now <= sf.lastTimestamp {
    				now = time.Now().UnixNano() / 1e6
    			}
    		}
    	} else {
    		sf.sequence = 0
    	}
    
    	sf.lastTimestamp = now
    
    	id := ((now - epoch) << timestampShift) |
    		(sf.dataCenterID << dataCenterIDShift) |
    		(sf.workerID << workerIDShift) |
    		sf.sequence
    
    	return id, nil
    }
    
    // Example usage:
    // func main() {
    // 	sf, err := NewSnowflake(1, 1) // workerID=1, dataCenterID=1
    // 	if err != nil {
    // 		log.Fatalf("Error creating snowflake: %v", err)
    // 	}
    // 	for i := 0; i < 10; i++ {
    // 		id, err := sf.Generate()
    // 		if err != nil {
    // 			log.Printf("Error generating ID: %v", err)
    // 			continue
    // 		}
    // 		fmt.Printf("Generated ID: %d\n", id)
    // 	}
    // }
    ```
- **Redis自增ID：** 利用Redis的 `INCR` 命令生成，简单高效，但需要考虑Redis的持久化和高可用。
- **数据库序列/号段模式：** 预先在数据库中生成一段ID，服务每次取一段ID使用，用完再取。减少数据库压力，但需要保证号段服务的可用性。

### 5. 性能优化要点

- **合理选择分片键：** 确保数据分布均匀，避免热点。尽量选择查询条件中经常出现的字段作为分片键。
- **SQL优化：** 避免全表扫描，确保SQL语句能命中索引。针对跨库查询，尽量通过业务层聚合或引入数据仓库解决。
- **连接池优化：** 合理配置数据库连接池大小，避免频繁创建和销毁连接。
- **缓存策略：** 引入Redis等缓存，减少对数据库的直接访问，特别是针对热点数据和读多写少的场景。
- **批量操作：** 批量插入、批量更新，减少网络IO和数据库交互次数。
- **读写分离：** 在分库分表的基础上，每个分库再进行读写分离，进一步提升读并发能力。
- **异步化：** 将非核心、耗时的操作异步化，如日志记录、消息通知等，减少主流程的响应时间。
- **监控与告警：** 建立完善的监控体系，实时监控数据库性能指标（CPU、内存、IO、连接数、慢查询等），及时发现并解决问题。

### 6. 生产实践经验

- **灰度发布与A/B测试：** 对于分库分表这种核心架构变更，务必进行灰度发布，逐步验证新架构的稳定性和性能。
- **数据迁移方案：** 制定详细的数据迁移计划，包括全量迁移、增量同步、数据校验、回滚方案等。推荐双写模式平滑过渡。
- **容量规划与压测：** 提前进行容量规划，并进行充分的压力测试，验证系统在高并发下的表现，发现潜在瓶颈。
- **应急预案：** 针对可能出现的故障（如分片故障、中间件故障、数据不一致等）制定详细的应急预案和恢复流程。
- **自动化运维：** 结合Kubernetes、Ansible等工具，实现分库分表集群的自动化部署、扩容、缩容、备份和恢复。
- **团队协作与知识共享：** 分库分表涉及多个团队和模块，需要加强团队间的沟通协作，沉淀和共享知识。

### 7. 面试要点总结

- **分库分表解决了什么问题？** (性能、存储、可用性)
- **分库分表的策略有哪些？** (垂直分库、水平分库、水平分表、混合分片)
- **分片键如何选择？** (均匀分布、避免热点、查询路由)
- **全局唯一ID如何生成？** (UUID、Snowflake、Redis、号段模式)
- **分布式事务如何处理？** (2PC、TCC、SAGA、最终一致性)
- **分库分表带来的挑战及解决方案？** (跨库Join、跨库事务、扩容、热点数据、复杂查询)
- **常用的分库分表中间件有哪些？** (ShardingSphere、MyCat、Vitess等)
- **如何进行数据迁移和扩容？** (双写、渐进式迁移)
- **Go语言中如何实现分库分表？** (自研路由、ShardingSphere-Proxy、Vitess)

通过以上内容的补充，<mcfile name="分库分表方案.md" path="d:\ownCode\leetcode\system_design\分库分表方案.md"></mcfile>文档将更加全面和深入，不仅包含理论知识，还提供了Go语言的实战代码和生产实践经验，更符合资深架构师的视角。

### 1. 分片路由器

```go
package sharding

import (
    "crypto/md5"
    "fmt"
    "hash/crc32"
    "sort"
    "strconv"
    "time"
)

// ShardingRouter 分片路由接口
type ShardingRouter interface {
    RouteDatabase(shardingKey string) string
    RouteTable(shardingKey string) string
    GetAllShards() []string
}

// HashRouter 哈希路由器
type HashRouter struct {
    DatabaseCount int
    TableCount    int
    DatabasePrefix string
    TablePrefix   string
}

func NewHashRouter(dbCount, tableCount int, dbPrefix, tablePrefix string) *HashRouter {
    return &HashRouter{
        DatabaseCount:  dbCount,
        TableCount:     tableCount,
        DatabasePrefix: dbPrefix,
        TablePrefix:    tablePrefix,
    }
}

func (r *HashRouter) RouteDatabase(shardingKey string) string {
    hash := crc32.ChecksumIEEE([]byte(shardingKey))
    index := int(hash) % r.DatabaseCount
    return fmt.Sprintf("%s_%d", r.DatabasePrefix, index)
}

func (r *HashRouter) RouteTable(shardingKey string) string {
    hash := crc32.ChecksumIEEE([]byte(shardingKey))
    index := int(hash) % r.TableCount
    return fmt.Sprintf("%s_%d", r.TablePrefix, index)
}

func (r *HashRouter) GetAllShards() []string {
    var shards []string
    for i := 0; i < r.DatabaseCount; i++ {
        shards = append(shards, fmt.Sprintf("%s_%d", r.DatabasePrefix, i))
    }
    return shards
}

// RangeRouter 范围路由器
type RangeRouter struct {
    Ranges []ShardRange
}

type ShardRange struct {
    Start    int64
    End      int64
    Database string
    Table    string
}

func (r *RangeRouter) RouteDatabase(shardingKey string) string {
    value, _ := strconv.ParseInt(shardingKey, 10, 64)
    for _, rang := range r.Ranges {
        if value >= rang.Start && value < rang.End {
            return rang.Database
        }
    }
    return r.Ranges[len(r.Ranges)-1].Database // 默认最后一个
}

func (r *RangeRouter) RouteTable(shardingKey string) string {
    value, _ := strconv.ParseInt(shardingKey, 10, 64)
    for _, rang := range r.Ranges {
        if value >= rang.Start && value < rang.End {
            return rang.Table
        }
    }
    return r.Ranges[len(r.Ranges)-1].Table
}

// TimeRouter 时间路由器
type TimeRouter struct {
    DatabasePrefix string
    TablePrefix    string
    TimeFormat     string // "200601" 按月, "20060102" 按天
}

func (r *TimeRouter) RouteDatabase(shardingKey string) string {
    // 时间路由通常不分库，只分表
    return r.DatabasePrefix
}

func (r *TimeRouter) RouteTable(shardingKey string) string {
    timestamp, _ := strconv.ParseInt(shardingKey, 10, 64)
    t := time.Unix(timestamp, 0)
    suffix := t.Format(r.TimeFormat)
    return fmt.Sprintf("%s_%s", r.TablePrefix, suffix)
}
```

### 2. 分片数据源

```go
package sharding

import (
    "context"
    "database/sql"
    "fmt"
    "sync"
    "time"
)

// ShardingDataSource 分片数据源
type ShardingDataSource struct {
    dataSources map[string]*sql.DB
    router      ShardingRouter
    healthCheck *HealthChecker
    mutex       sync.RWMutex
}

func NewShardingDataSource(router ShardingRouter) *ShardingDataSource {
    return &ShardingDataSource{
        dataSources: make(map[string]*sql.DB),
        router:      router,
        healthCheck: NewHealthChecker(),
    }
}

// AddDataSource 添加数据源
func (ds *ShardingDataSource) AddDataSource(name string, db *sql.DB) {
    ds.mutex.Lock()
    defer ds.mutex.Unlock()
    
    ds.dataSources[name] = db
    ds.healthCheck.AddShard(name, db)
}

// GetDataSource 获取数据源
func (ds *ShardingDataSource) GetDataSource(shardingKey string) (*sql.DB, string, error) {
    dbName := ds.router.RouteDatabase(shardingKey)
    tableName := ds.router.RouteTable(shardingKey)
    
    ds.mutex.RLock()
    db, exists := ds.dataSources[dbName]
    ds.mutex.RUnlock()
    
    if !exists {
        return nil, "", fmt.Errorf("database %s not found", dbName)
    }
    
    if !ds.healthCheck.IsHealthy(dbName) {
        return nil, "", fmt.Errorf("database %s is unhealthy", dbName)
    }
    
    return db, tableName, nil
}

// Execute 执行SQL
func (ds *ShardingDataSource) Execute(ctx context.Context, shardingKey, query string, args ...interface{}) (sql.Result, error) {
    db, tableName, err := ds.GetDataSource(shardingKey)
    if err != nil {
        return nil, err
    }
    
    // 替换表名占位符
    finalQuery := fmt.Sprintf(query, tableName)
    
    return db.ExecContext(ctx, finalQuery, args...)
}

// Query 查询数据
func (ds *ShardingDataSource) Query(ctx context.Context, shardingKey, query string, args ...interface{}) (*sql.Rows, error) {
    db, tableName, err := ds.GetDataSource(shardingKey)
    if err != nil {
        return nil, err
    }
    
    finalQuery := fmt.Sprintf(query, tableName)
    
    return db.QueryContext(ctx, finalQuery, args...)
}

// CrossShardQuery 跨分片查询
func (ds *ShardingDataSource) CrossShardQuery(ctx context.Context, query string, args ...interface{}) ([]map[string]interface{}, error) {
    shards := ds.router.GetAllShards()
    
    type shardResult struct {
        data []map[string]interface{}
        err  error
    }
    
    resultChan := make(chan shardResult, len(shards))
    
    // 并行查询所有分片
    for _, shard := range shards {
        go func(shardName string) {
            db := ds.dataSources[shardName]
            if db == nil || !ds.healthCheck.IsHealthy(shardName) {
                resultChan <- shardResult{nil, fmt.Errorf("shard %s unavailable", shardName)}
                return
            }
            
            rows, err := db.QueryContext(ctx, query, args...)
            if err != nil {
                resultChan <- shardResult{nil, err}
                return
            }
            defer rows.Close()
            
            data, err := scanRowsToMap(rows)
            resultChan <- shardResult{data, err}
        }(shard)
    }
    
    // 收集结果
    var allResults []map[string]interface{}
    for i := 0; i < len(shards); i++ {
        result := <-resultChan
        if result.err != nil {
            return nil, result.err
        }
        allResults = append(allResults, result.data...)
    }
    
    return allResults, nil
}

// 健康检查器
type HealthChecker struct {
    shards map[string]*ShardHealth
    mutex  sync.RWMutex
}

type ShardHealth struct {
    DB        *sql.DB
    Healthy   bool
    LastCheck time.Time
}

func NewHealthChecker() *HealthChecker {
    hc := &HealthChecker{
        shards: make(map[string]*ShardHealth),
    }
    
    // 启动健康检查
    go hc.startHealthCheck()
    
    return hc
}

func (hc *HealthChecker) AddShard(name string, db *sql.DB) {
    hc.mutex.Lock()
    defer hc.mutex.Unlock()
    
    hc.shards[name] = &ShardHealth{
        DB:        db,
        Healthy:   true,
        LastCheck: time.Now(),
    }
}

func (hc *HealthChecker) IsHealthy(name string) bool {
    hc.mutex.RLock()
    defer hc.mutex.RUnlock()
    
    shard, exists := hc.shards[name]
    return exists && shard.Healthy
}

func (hc *HealthChecker) startHealthCheck() {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for range ticker.C {
        hc.checkAllShards()
    }
}

func (hc *HealthChecker) checkAllShards() {
    hc.mutex.Lock()
    defer hc.mutex.Unlock()
    
    for name, shard := range hc.shards {
        healthy := hc.pingShard(shard.DB)
        shard.Healthy = healthy
        shard.LastCheck = time.Now()
        
        if !healthy {
            fmt.Printf("Shard %s is unhealthy\n", name)
        }
    }
}

func (hc *HealthChecker) pingShard(db *sql.DB) bool {
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    
    return db.PingContext(ctx) == nil
}

// 辅助函数：将查询结果转换为map
func scanRowsToMap(rows *sql.Rows) ([]map[string]interface{}, error) {
    columns, err := rows.Columns()
    if err != nil {
        return nil, err
    }
    
    var results []map[string]interface{}
    
    for rows.Next() {
        values := make([]interface{}, len(columns))
        valuePtrs := make([]interface{}, len(columns))
        
        for i := range values {
            valuePtrs[i] = &values[i]
        }
        
        if err := rows.Scan(valuePtrs...); err != nil {
            return nil, err
        }
        
        row := make(map[string]interface{})
        for i, col := range columns {
            row[col] = values[i]
        }
        
        results = append(results, row)
    }
    
    return results, rows.Err()
}
```

### 3. 分布式事务管理

```go
package transaction

import (
    "context"
    "database/sql"
    "fmt"
    "sync"
    "time"
)

// TCC事务管理器
type TCCManager struct {
    transactions map[string]*TCCTransaction
    mutex        sync.RWMutex
}

type TCCTransaction struct {
    ID         string
    Operations []TCCOperation
    Status     TransactionStatus
    CreatedAt  time.Time
    UpdatedAt  time.Time
}

type TransactionStatus int

const (
    StatusPending TransactionStatus = iota
    StatusTrying
    StatusConfirming
    StatusCanceling
    StatusConfirmed
    StatusCanceled
    StatusFailed
)

type TCCOperation interface {
    Try(ctx context.Context) error
    Confirm(ctx context.Context) error
    Cancel(ctx context.Context) error
    GetResourceID() string
}

func NewTCCManager() *TCCManager {
    return &TCCManager{
        transactions: make(map[string]*TCCTransaction),
    }
}

func (tm *TCCManager) BeginTransaction(id string) *TCCTransaction {
    tm.mutex.Lock()
    defer tm.mutex.Unlock()
    
    tx := &TCCTransaction{
        ID:        id,
        Status:    StatusPending,
        CreatedAt: time.Now(),
        UpdatedAt: time.Now(),
    }
    
    tm.transactions[id] = tx
    return tx
}

func (tx *TCCTransaction) AddOperation(op TCCOperation) {
    tx.Operations = append(tx.Operations, op)
}

func (tm *TCCManager) ExecuteTransaction(ctx context.Context, txID string) error {
    tm.mutex.Lock()
    tx, exists := tm.transactions[txID]
    if !exists {
        tm.mutex.Unlock()
        return fmt.Errorf("transaction %s not found", txID)
    }
    tx.Status = StatusTrying
    tx.UpdatedAt = time.Now()
    tm.mutex.Unlock()
    
    // Try阶段
    for i, op := range tx.Operations {
        if err := op.Try(ctx); err != nil {
            // Try失败，执行Cancel
            tm.cancelTransaction(ctx, tx, i)
            return err
        }
    }
    
    // Confirm阶段
    tx.Status = StatusConfirming
    for i, op := range tx.Operations {
        if err := op.Confirm(ctx); err != nil {
            // Confirm失败，需要人工干预
            tx.Status = StatusFailed
            return fmt.Errorf("confirm failed at operation %d: %v", i, err)
        }
    }
    
    tx.Status = StatusConfirmed
    tx.UpdatedAt = time.Now()
    
    return nil
}

func (tm *TCCManager) cancelTransaction(ctx context.Context, tx *TCCTransaction, failedIndex int) {
    tx.Status = StatusCanceling
    
    // 逆序取消已执行的操作
    for i := failedIndex - 1; i >= 0; i-- {
        if err := tx.Operations[i].Cancel(ctx); err != nil {
            // 记录取消失败的操作，需要人工干预
            fmt.Printf("Cancel operation %d failed: %v\n", i, err)
        }
    }
    
    tx.Status = StatusCanceled
    tx.UpdatedAt = time.Now()
}

// 账户转账TCC操作示例
type TransferOperation struct {
    FromAccount string
    ToAccount   string
    Amount      float64
    DS          *ShardingDataSource
    TxID        string
}

func (op *TransferOperation) GetResourceID() string {
    return fmt.Sprintf("%s-%s", op.FromAccount, op.ToAccount)
}

func (op *TransferOperation) Try(ctx context.Context) error {
    // 冻结转出账户金额
    query := "UPDATE accounts SET frozen_amount = frozen_amount + ?, version = version + 1 WHERE account_id = ? AND balance >= ?"
    
    result, err := op.DS.Execute(ctx, op.FromAccount, query, op.Amount, op.FromAccount, op.Amount)
    if err != nil {
        return err
    }
    
    affected, _ := result.RowsAffected()
    if affected == 0 {
        return fmt.Errorf("insufficient balance or account not found")
    }
    
    // 记录TCC事务日志
    return op.recordTCCLog(ctx, "TRY", "SUCCESS")
}

func (op *TransferOperation) Confirm(ctx context.Context) error {
    // 扣减转出账户，增加转入账户
    
    // 1. 扣减转出账户
    deductQuery := "UPDATE accounts SET balance = balance - ?, frozen_amount = frozen_amount - ?, version = version + 1 WHERE account_id = ?"
    _, err := op.DS.Execute(ctx, op.FromAccount, deductQuery, op.Amount, op.Amount, op.FromAccount)
    if err != nil {
        return err
    }
    
    // 2. 增加转入账户
    addQuery := "UPDATE accounts SET balance = balance + ?, version = version + 1 WHERE account_id = ?"
    _, err = op.DS.Execute(ctx, op.ToAccount, addQuery, op.Amount, op.ToAccount)
    if err != nil {
        return err
    }
    
    // 3. 记录转账流水
    err = op.recordTransferLog(ctx)
    if err != nil {
        return err
    }
    
    return op.recordTCCLog(ctx, "CONFIRM", "SUCCESS")
}

func (op *TransferOperation) Cancel(ctx context.Context) error {
    // 解冻转出账户金额
    query := "UPDATE accounts SET frozen_amount = frozen_amount - ?, version = version + 1 WHERE account_id = ?"
    
    _, err := op.DS.Execute(ctx, op.FromAccount, query, op.Amount, op.FromAccount)
    if err != nil {
        return err
    }
    
    return op.recordTCCLog(ctx, "CANCEL", "SUCCESS")
}

func (op *TransferOperation) recordTCCLog(ctx context.Context, action, status string) error {
    query := "INSERT INTO tcc_logs (tx_id, resource_id, action, status, created_at) VALUES (?, ?, ?, ?, ?)"
    
    _, err := op.DS.Execute(ctx, op.TxID, query, op.TxID, op.GetResourceID(), action, status, time.Now())
    return err
}

func (op *TransferOperation) recordTransferLog(ctx context.Context) error {
    query := "INSERT INTO transfer_logs (tx_id, from_account, to_account, amount, created_at) VALUES (?, ?, ?, ?, ?)"
    
    _, err := op.DS.Execute(ctx, op.TxID, query, op.TxID, op.FromAccount, op.ToAccount, op.Amount, time.Now())
    return err
}
```

## 总结

分库分表是解决大规模数据存储和高并发访问的重要技术手段，但也带来了系统复杂性的显著增加。在实施分库分表方案时，需要综合考虑业务特点、技术架构、运维成本等多个因素。

**关键要点：**

1. **合理的分片策略**：选择合适的分片键和算法
2. **数据一致性**：在性能和一致性之间找到平衡
3. **查询优化**：避免跨分片查询，合理使用缓存
4. **运维监控**：完善的监控和故障处理机制
5. **扩容规划**：提前规划扩容方案，避免后期重构

**最佳实践：**

- 优先考虑垂直分库，再考虑水平分库
- 分片数量选择2的幂次，便于扩容
- 避免分布式事务，采用最终一致性
- 建立完善的监控和告警机制
- 制定详细的数据迁移和扩容方案

通过合理的架构设计和技术选型，分库分表能够有效解决大规模系统的数据存储和访问问题，为业务的快速发展提供强有力的技术支撑。